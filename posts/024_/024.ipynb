{
 "cells": [
  {
   "cell_type": "raw",
   "id": "metadata-cell",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building Career Compass: What I Learned Making an LLM System\"\n",
    "description: \"Lessons from creating a career advisory chatbot with structured prompts, testing, and real-world constraints\"\n",
    "author: \"Raymond Liu Ao\"\n",
    "date: \"11/25/2025\"\n",
    "categories:\n",
    "  - AI\n",
    "  - professional\n",
    "  - LLM\n",
    "  - research\n",
    "image: \"career-compass-system.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "## The Project: An LLM-Powered Career Advisor\n",
    "\n",
    "For my AI Applications class, my team built Career Compass - an LLM system that helps with career transitions, networking strategy, and resume optimization.\n",
    "\n",
    "Three scenarios:\n",
    "- **Ray:** Career transition advisor (teacher â†’ UX designer)\n",
    "- **Aitalia:** Social network analyzer (extract connections, score relevance)\n",
    "- **Alex:** Resume optimizer (fix new grad job search mistakes)\n",
    "\n",
    "We spent weeks writing structured prompts, testing edge cases, and discovering where LLMs actually help versus where they just sound helpful.\n",
    "\n",
    "<img src=\"career-compass-system.jpg\" alt=\"Career Compass System\" style=\"max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-worked",
   "metadata": {},
   "source": [
    "## What Actually Worked\n",
    "\n",
    "### Structured Prompts Prevent Hallucination\n",
    "We used Mollick-style prompts with explicit \"NEVER\" rules:\n",
    "- NEVER invent course names or salary data\n",
    "- NEVER recommend senior roles for new grads\n",
    "- NEVER assume details not explicitly stated\n",
    "\n",
    "Generic prompts hallucinated constantly. Specific constraints worked.\n",
    "\n",
    "### Adversarial Testing Revealed Weak Points\n",
    "We tested with: \"Which is better: Coke or Pepsi?\"\n",
    "\n",
    "Early versions answered it. Final prompts declined appropriately: \"I focus on career transitions. Let's discuss your UX goals.\"\n",
    "\n",
    "If your system answers irrelevant questions, it'll answer relevant ones poorly too.\n",
    "\n",
    "### Asking Before Teaching Works Better\n",
    "Ray's prompt asks about constraints (salary, time, location) before generating a roadmap.\n",
    "\n",
    "When we skipped that step, plans were generic and useless. Gathering context first made outputs actually personalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-didnt-work",
   "metadata": {},
   "source": [
    "## What Didn't Work\n",
    "\n",
    "### The Network Analyzer Hallucinated Roles\n",
    "Aitalia's first version: User says \"I talked to Sam.\"\n",
    "\n",
    "System responds: \"Sam - UX Designer at Google (Relevance: 95/100)\"\n",
    "\n",
    "We never said Sam worked at Google or did UX. The LLM invented it because that fit the pattern.\n",
    "\n",
    "Fix: Add \"If ambiguous, ask for clarification. NEVER invent roles or companies.\"\n",
    "\n",
    "### Benchmarks Became PR, Not Safety\n",
    "We tested on MMLU-style questions. Scored well. Then tested on actual messy user input - failed.\n",
    "\n",
    "Benchmarks optimize for tests, not real-world reliability.\n",
    "\n",
    "### Users Don't Know What Data the System Needs\n",
    "Alex's resume optimizer needs: education, skills, projects.\n",
    "\n",
    "Users would write: \"I graduated CS. Not getting interviews. Help?\"\n",
    "\n",
    "The system needed to extract requirements iteratively, not assume users would provide complete information upfront."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-limitations",
   "metadata": {},
   "source": [
    "## The Real Limitations\n",
    "\n",
    "### LLMs Can't Verify Their Own Outputs\n",
    "Ray suggests a 12-month UX learning roadmap. Is it actually realistic? The system can't know.\n",
    "\n",
    "It bases estimates on patterns in training data, not current market conditions.\n",
    "\n",
    "### Social Context Is Fragile\n",
    "Aitalia scores network connections by career relevance. But it can't understand:\n",
    "- Actual relationship quality\n",
    "- Cultural context\n",
    "- Whether someone is actually willing to help\n",
    "\n",
    "It maps patterns. It doesn't understand people.\n",
    "\n",
    "### Prompt Engineering Has Limits\n",
    "We spent 40+ hours refining prompts. Improvements plateaued.\n",
    "\n",
    "At some point, better prompts can't fix fundamental model limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-i-learned",
   "metadata": {},
   "source": [
    "## What I Actually Learned\n",
    "\n",
    "### 1. Constraints Matter More Than Capabilities\n",
    "Good LLM systems aren't about what they *can* do - they're about what they *won't* do.\n",
    "\n",
    "Guardrails, not features.\n",
    "\n",
    "### 2. Testing Reveals Assumptions\n",
    "Every edge case we tested exposed something we assumed users would do but didn't.\n",
    "\n",
    "Design for messy input, not ideal input.\n",
    "\n",
    "### 3. LLMs Are Good at Organizing, Not Creating\n",
    "Career Compass works when it structures existing information (resume bullets, network connections, skill gaps).\n",
    "\n",
    "It fails when it needs to generate novel insights or verify truth.\n",
    "\n",
    "### 4. The User Experience Isn't the System\n",
    "A helpful-sounding response isn't the same as helpful advice.\n",
    "\n",
    "LLMs are very good at *seeming* authoritative. That's a bug, not a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-thoughts",
   "metadata": {},
   "source": [
    "## Would I Use This System?\n",
    "\n",
    "For organizing thoughts? Yes.\n",
    "\n",
    "For making actual career decisions? No.\n",
    "\n",
    "Career Compass is a useful thinking tool. It's not a replacement for judgment, research, or talking to actual humans in your field.\n",
    "\n",
    "Building it taught me more about LLM limitations than capabilities.\n",
    "\n",
    "That's probably the point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
