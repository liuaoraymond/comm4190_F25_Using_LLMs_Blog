<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>What Can You Do With AI?</title>
<link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/</link>
<atom:link href="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Let&#39;s Find Out</description>
<generator>quarto-1.7.34</generator>
<lastBuildDate>Wed, 03 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>How AI Really Works: Beyond the Magic</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/</link>
  <description><![CDATA[ 





<section id="the-great-ai-illusion" class="level2">
<h2 class="anchored" data-anchor-id="the-great-ai-illusion">The Great AI Illusion</h2>
<p>Artificial Intelligence feels like magic. You ask ChatGPT a question, and it responds with human-like intelligence. You show DALL-E a text prompt, and it creates art. But here’s the uncomfortable truth: <strong>there’s no actual intelligence happening here</strong>.</p>
<p>What we call “AI” is really an incredibly sophisticated pattern matching system—one that’s so good at finding and reproducing patterns that it can fool us into thinking it understands. Today, we’re going to pull back the curtain and examine exactly how these systems work.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" class="img-fluid" alt="Neural Network vs Brain"> <em>AI neural networks mimic brain structure but work very differently under the hood</em></p>
<blockquote class="blockquote">
<p><strong>Key Question:</strong> If AI doesn’t truly “think,” why does it seem so smart?</p>
</blockquote>
</section>
<section id="the-mathematical-foundation-its-all-about-prediction" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematical-foundation-its-all-about-prediction">The Mathematical Foundation: It’s All About Prediction</h2>
<p>At its core, every AI system is trying to solve one fundamental problem: <strong>given some input, predict the most likely output</strong>. Whether it’s predicting the next word in a sentence, the next pixel in an image, or the next move in a game, AI is fundamentally a prediction engine.</p>
<section id="the-architecture-of-modern-ai" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-of-modern-ai">The Architecture of Modern AI</h3>
<p>Modern AI systems, particularly large language models like GPT-4 or Claude, are built on an architecture called <strong>transformers</strong>. Here’s how they work:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified transformer logic</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> transformer_prediction(input_text):</span>
<span id="cb1-3">    tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize(input_text)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Break text into pieces</span></span>
<span id="cb1-4">    embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_to_vectors(tokens)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert to numbers</span></span>
<span id="cb1-5">    </span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> neural_layers:</span>
<span id="cb1-7">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention_mechanism(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Find relationships</span></span>
<span id="cb1-8">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Process patterns</span></span>
<span id="cb1-9">    </span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predict_next_token(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Guess what comes next</span></span></code></pre></div>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/transformer-architecture.jpg" class="img-fluid" alt="Transformer Architecture"> <em>The transformer architecture that powers modern AI - layers of attention and processing</em></p>
<p>The scale of modern AI is staggering. GPT-3 has 175 billion parameters and was trained on 45TB of text data at a cost of around $4.6 million. GPT-4 is estimated to have over 1 trillion parameters with training costs exceeding $60 million.</p>
</section>
<section id="the-attention-mechanism-how-ai-focuses" class="level3">
<h3 class="anchored" data-anchor-id="the-attention-mechanism-how-ai-focuses">The Attention Mechanism: How AI “Focuses”</h3>
<p>The breakthrough that made modern AI possible was the <strong>attention mechanism</strong>. This allows models to selectively focus on different parts of their input when making predictions.</p>
<p>Think of attention like a spotlight that can illuminate different parts of a sentence based on context:</p>
<p><strong>Example:</strong> “The cat sat on the mat because it was comfortable.”</p>
<p>When predicting what “it” refers to, the attention mechanism looks at: - “cat” (high attention - 0.7) - “mat” (medium attention - 0.2) - “sat” (low attention - 0.1)</p>
<p>This attention mechanism allows AI to understand relationships between words that are far apart in a sentence, enabling more sophisticated language understanding.</p>
</section>
</section>
<section id="the-training-process-teaching-patterns-at-scale" class="level2">
<h2 class="anchored" data-anchor-id="the-training-process-teaching-patterns-at-scale">The Training Process: Teaching Patterns at Scale</h2>
<p>Understanding how AI systems learn is crucial to understanding their capabilities and limitations. The training process happens in distinct phases:</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-training-process.jpg" class="img-fluid" alt="AI Training Process"> <em>The complete pipeline from raw data to deployed AI system</em></p>
<section id="phase-1-pre-training-learning-the-world" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-pre-training-learning-the-world">Phase 1: Pre-training (Learning the World)</h3>
<p>Imagine you had to read the entire internet and predict what comes next in every sentence. That’s essentially what happens during pre-training:</p>
<ol type="1">
<li><strong>Data Collection</strong>: Scrape massive amounts of text from websites, books, articles</li>
<li><strong>Tokenization</strong>: Break everything into small chunks (words, parts of words, punctuation)</li>
<li><strong>Prediction Training</strong>: For every sequence, hide the last word and train the model to predict it</li>
<li><strong>Repeat Trillions of Times</strong>: Adjust internal parameters based on prediction accuracy</li>
</ol>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified training loop</span></span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(many_epochs):</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> text_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> massive_dataset:</span>
<span id="cb2-4">        input_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># All but last token</span></span>
<span id="cb2-5">        target_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The token to predict</span></span>
<span id="cb2-6">        </span>
<span id="cb2-7">        prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(input_tokens)</span>
<span id="cb2-8">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calculate_error(prediction, target_token)</span>
<span id="cb2-9">        </span>
<span id="cb2-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Adjust model weights to reduce error</span></span>
<span id="cb2-11">        model.update_weights(loss.gradient())</span></code></pre></div>
</section>
<section id="phase-2-fine-tuning-learning-to-be-helpful" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-fine-tuning-learning-to-be-helpful">Phase 2: Fine-tuning (Learning to be Helpful)</h3>
<p>Raw pre-training creates a model that can continue any text, but it doesn’t necessarily produce helpful responses. Fine-tuning shapes the model’s behavior:</p>
<ul>
<li><strong>Supervised Fine-tuning</strong>: Train on high-quality question-answer pairs</li>
<li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: Use human preferences to reward good responses</li>
<li><strong>Constitutional AI</strong>: Train the model to follow principles and avoid harmful outputs</li>
</ul>
<p>This is why ChatGPT feels helpful and conversational, while a raw language model might just continue your sentence in unexpected ways.</p>
</section>
</section>
<section id="emergent-abilities-when-simple-rules-create-complex-behavior" class="level2">
<h2 class="anchored" data-anchor-id="emergent-abilities-when-simple-rules-create-complex-behavior">Emergent Abilities: When Simple Rules Create Complex Behavior</h2>
<p>One of the most fascinating aspects of large AI models is <strong>emergence</strong>—complex capabilities that weren’t explicitly programmed but arise from the interaction of simple components at scale.</p>
<section id="examples-of-emergent-abilities" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-emergent-abilities">Examples of Emergent Abilities</h3>
<p>As models get larger, they suddenly develop new capabilities:</p>
<p><strong>Chain-of-Thought Reasoning</strong>: Models learn to “think step by step” without being explicitly taught this strategy.</p>
<p><strong>In-Context Learning</strong>: Models can learn new tasks from just a few examples in the prompt, without any additional training.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example of in-context learning</span></span>
<span id="cb3-2">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Translate English to French:</span></span>
<span id="cb3-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Hello → Bonjour</span></span>
<span id="cb3-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Goodbye → Au revoir</span></span>
<span id="cb3-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Thank you → Merci</span></span>
<span id="cb3-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Good morning → </span></span>
<span id="cb3-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Model predicts "Bonjour" despite never being</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># explicitly trained on this translation task</span></span></code></pre></div>
<p><strong>Cross-Lingual Transfer</strong>: Models trained primarily on English can perform well in other languages.</p>
<p>Scientists still don’t fully understand why emergence happens. Leading theories include phase transitions (like water becoming ice), increased representational capacity, and “grokking”—when models suddenly understand patterns after seeing enough examples.</p>
</section>
</section>
<section id="the-fundamental-limitations-what-ai-cant-do" class="level2">
<h2 class="anchored" data-anchor-id="the-fundamental-limitations-what-ai-cant-do">The Fundamental Limitations: What AI Can’t Do</h2>
<p>Despite their impressive capabilities, current AI systems have fundamental limitations that stem from their architecture:</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-limitations-chart.jpg" class="img-fluid" alt="AI Limitations Chart"> <em>Understanding what AI can and cannot reliably do</em></p>
<section id="no-true-understanding" class="level3">
<h3 class="anchored" data-anchor-id="no-true-understanding">1. No True Understanding</h3>
<p>AI systems manipulate symbols without understanding their meaning. They’re like a person who can perfectly reproduce Chinese characters without speaking Chinese.</p>
<p><strong>Example</strong>: An AI can tell you that “water boils at 100°C” but doesn’t understand what boiling actually <em>is</em>—the molecular motion, the phase change, the physical reality behind the words.</p>
</section>
<section id="hallucination-and-confabulation" class="level3">
<h3 class="anchored" data-anchor-id="hallucination-and-confabulation">2. Hallucination and Confabulation</h3>
<p>Because AI systems are trained to always produce output, they’ll generate plausible-sounding but false information when they don’t know something.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># What happens inside when AI doesn't know</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_response(query):</span>
<span id="cb4-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> confidence_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> threshold:</span>
<span id="cb4-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> known_information(query)</span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-6">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Problem: AI still tries to respond!</span></span>
<span id="cb4-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> generate_plausible_sounding_answer(query)</span></code></pre></div>
</section>
<section id="training-data-cutoff" class="level3">
<h3 class="anchored" data-anchor-id="training-data-cutoff">3. Training Data Cutoff</h3>
<p>AI models are frozen in time at their training cutoff. They can’t learn new information or update their knowledge without retraining.</p>
</section>
<section id="context-window-limitations" class="level3">
<h3 class="anchored" data-anchor-id="context-window-limitations">4. Context Window Limitations</h3>
<p>Current models can only “remember” a limited amount of recent conversation (typically 4,000-128,000 tokens). They have no long-term episodic memory.</p>
</section>
<section id="lack-of-causal-understanding" class="level3">
<h3 class="anchored" data-anchor-id="lack-of-causal-understanding">5. Lack of Causal Understanding</h3>
<p>AI systems excel at finding correlations but struggle with causation. They might notice that umbrella sales correlate with rain but don’t understand that rain <em>causes</em> people to buy umbrellas.</p>
</section>
</section>
<section id="what-this-means-for-society" class="level2">
<h2 class="anchored" data-anchor-id="what-this-means-for-society">What This Means for Society</h2>
<p>Understanding how AI really works has profound implications for how we should think about and use these systems:</p>
<section id="the-good-news" class="level3">
<h3 class="anchored" data-anchor-id="the-good-news">The Good News</h3>
<ul>
<li><strong>Powerful Tools</strong>: AI systems are incredibly useful for pattern recognition, content generation, and automation</li>
<li><strong>Democratization</strong>: Complex capabilities are becoming accessible to everyone</li>
<li><strong>Augmentation</strong>: AI can enhance human capabilities rather than replace human judgment</li>
</ul>
</section>
<section id="the-concerning-news" class="level3">
<h3 class="anchored" data-anchor-id="the-concerning-news">The Concerning News</h3>
<ul>
<li><strong>Overconfidence</strong>: AI systems present uncertain information with false confidence</li>
<li><strong>Bias Amplification</strong>: Training on internet data amplifies existing societal biases</li>
<li><strong>Misinformation</strong>: Sophisticated text generation can create convincing but false content</li>
</ul>
</section>
<section id="best-practices-for-ai-use" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-ai-use">Best Practices for AI Use</h3>
<ol type="1">
<li><strong>Verify Important Information</strong>: Always fact-check AI outputs for critical decisions</li>
<li><strong>Understand Limitations</strong>: Know what AI can and cannot do reliably</li>
<li><strong>Maintain Human Oversight</strong>: Keep humans in the loop for important judgments</li>
<li><strong>Recognize Bias</strong>: Be aware that AI systems reflect training data biases</li>
<li><strong>Use as a Tool</strong>: Treat AI as a sophisticated calculator, not an oracle</li>
</ol>
</section>
</section>
<section id="the-future-whats-next" class="level2">
<h2 class="anchored" data-anchor-id="the-future-whats-next">The Future: What’s Next?</h2>
<p>Current AI represents just the beginning. Several developments on the horizon could dramatically change the landscape:</p>
<section id="near-term-developments-1-3-years" class="level3">
<h3 class="anchored" data-anchor-id="near-term-developments-1-3-years">Near-Term Developments (1-3 years)</h3>
<ul>
<li><strong>Multimodal Integration</strong>: AI that seamlessly combines text, images, audio, and video</li>
<li><strong>Longer Context Windows</strong>: Models that can “remember” entire books or conversations</li>
<li><strong>Better Reasoning</strong>: Improved step-by-step problem-solving capabilities</li>
<li><strong>Specialized Models</strong>: AI systems optimized for specific domains (medicine, law, science)</li>
</ul>
</section>
<section id="long-term-questions-10-years" class="level3">
<h3 class="anchored" data-anchor-id="long-term-questions-10-years">Long-Term Questions (10+ years)</h3>
<ul>
<li><strong>Artificial General Intelligence (AGI)</strong>: AI that matches human cognitive abilities across all domains</li>
<li><strong>Consciousness</strong>: Whether AI systems can develop subjective experiences</li>
<li><strong>Alignment</strong>: Ensuring advanced AI systems remain beneficial to humanity</li>
</ul>
</section>
<section id="the-research-frontiers" class="level3">
<h3 class="anchored" data-anchor-id="the-research-frontiers">The Research Frontiers</h3>
<p>Scientists are actively working on fundamental questions:</p>
<ul>
<li><strong>Scaling Laws</strong>: How do capabilities change with model size, data, and compute?</li>
<li><strong>Emergence</strong>: Why do new abilities suddenly appear at certain scales?</li>
<li><strong>Alignment</strong>: How do we ensure AI systems do what we want them to do?</li>
<li><strong>Interpretability</strong>: Can we understand what’s happening inside these black boxes?</li>
</ul>
</section>
</section>
<section id="conclusion-intelligence-vs.-sophistication" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-intelligence-vs.-sophistication">Conclusion: Intelligence vs.&nbsp;Sophistication</h2>
<p>After peeling back the layers, we can see that current AI systems are incredibly sophisticated pattern-matching engines rather than truly intelligent entities. They don’t understand the world in the way humans do—they manipulate symbols based on statistical relationships learned from vast amounts of data.</p>
<p>But here’s the remarkable thing: <strong>this statistical approach is powerful enough to seem intelligent</strong>. By processing patterns at unprecedented scale, these systems can engage in conversations, solve problems, and create content that feels genuinely intelligent.</p>
<section id="the-paradox-of-ai" class="level3">
<h3 class="anchored" data-anchor-id="the-paradox-of-ai">The Paradox of AI</h3>
<p>The central paradox of modern AI is that it achieves intelligence-like behavior without intelligence-like understanding. This has profound implications:</p>
<ul>
<li><strong>For Users</strong>: We must learn to work with these systems’ strengths while compensating for their weaknesses</li>
<li><strong>For Society</strong>: We need new frameworks for thinking about machine capabilities and limitations</li>
<li><strong>For the Future</strong>: We must grapple with questions about consciousness, agency, and what it means to be intelligent</li>
</ul>
<p>Understanding how AI really works doesn’t diminish its impressiveness—if anything, it makes the achievement more remarkable. The fact that statistical pattern matching at scale can produce such sophisticated behavior suggests that intelligence itself might be more about information processing patterns than we previously thought.</p>
<p>As these systems become more powerful and ubiquitous, our understanding of their true nature becomes increasingly important. The more we know about how AI works, the better we can harness its benefits while avoiding its pitfalls.</p>
<p>The AI revolution is just beginning, and we’re all learning to navigate this new landscape together. By understanding the reality behind the magic, we can make better decisions about how to integrate these powerful tools into our lives and society.</p>
<hr>
<p><strong>What do you think? Does understanding the mechanics behind AI change how you view these systems? Let me know your thoughts.</strong></p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>deep-dive</category>
  <category>technology</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
