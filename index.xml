<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>What Can You Do With AI?</title>
<link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/</link>
<atom:link href="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Let&#39;s Find Out</description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Fri, 03 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Running a Virtual Focus Group with ChatGPT</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/011.html</link>
  <description><![CDATA[ 





<section id="testing-products-with-ai-generated-focus-groups" class="level2">
<h2 class="anchored" data-anchor-id="testing-products-with-ai-generated-focus-groups">Testing Products with AI-Generated Focus Groups</h2>
<p>For the Cuzco Crunch project, Eury and I needed to test two product concepts before committing to one. Instead of recruiting actual participants for a focus group, we decided to try something experimental: using ChatGPT to simulate a focus group with 10 diverse personas.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/cuzco-crunch-package.jpg" alt="Cuzco Crunch Product" style="max-width: 450px; max-height: 300px; object-fit: cover; object-position: center; display: block; margin: 20px auto;"></p>
<p><em>Cuzco Crunch: Golden plantain slices with Peruvian sal de Maras</em></p>
</section>
<section id="the-experimental-design" class="level2">
<h2 class="anchored" data-anchor-id="the-experimental-design">The Experimental Design</h2>
<section id="two-products-to-test" class="level3">
<h3 class="anchored" data-anchor-id="two-products-to-test">Two Products to Test</h3>
<p><strong>Cuzco Crunch (Product A):</strong> Positioned as premium - golden, ultra-crispy plantain slices with Peruvian sal de Maras for a mineral-salt finish. Natural sweetness from plantain, meant to be versatile. Price: $7 for 1.5 oz.</p>
<p><strong>Plantain Lite (Product B):</strong> Positioned as everyday - lighter snack with delicate crunch and simple seasoning. Emphasizes convenience and portability. Price: $5 for 1.5 oz.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/plantain-lite-package.jpg" alt="Plantain Lite Product" style="max-width: 500px; max-height: 400px; object-fit: cover; object-position: center; display: block; margin: 20px auto;"></p>
<p><em>Plantain Lite: A lighter, everyday plantain chip option</em></p>
</section>
<section id="creating-10-diverse-personas" class="level3">
<h3 class="anchored" data-anchor-id="creating-10-diverse-personas">Creating 10 Diverse Personas</h3>
<p>We built personas across multiple demographic dimensions: - <strong>Gender:</strong> Male/Female - <strong>Age ranges:</strong> 18-25, 26-33, 34-41, 42-50 - <strong>Health conditions:</strong> Hypertension, obesity, heart disease, asthma, food allergies, digestive issues, or none - <strong>Ethnicity:</strong> Latino or Not Latino - <strong>Location:</strong> Urban, suburban, or rural</p>
<p>Examples included: - Young urban health-conscious Latina (18-25, no health conditions) - Middle-aged suburban Latino managing hypertension (42-50) - Young urban non-Latino with food allergies (26-33, gluten/dairy/nuts) - Middle-aged rural non-Latino with obesity (34-41) - Middle-aged rural Latina with multiple conditions (42-50, hypertension + obesity)</p>
<p>The goal was to represent our potential target segments and see how different audiences responded to each product.</p>
</section>
</section>
<section id="the-calibrated-survey" class="level2">
<h2 class="anchored" data-anchor-id="the-calibrated-survey">The Calibrated Survey</h2>
<section id="starting-with-a-benchmark" class="level3">
<h3 class="anchored" data-anchor-id="starting-with-a-benchmark">Starting with a Benchmark</h3>
<p>We calibrated responses by having participants rate Lay’s Classic Potato Chips first as a reference point. This gave us a common baseline to compare against: - 1 = Unacceptable, wouldn’t eat even if free - 3 = Acceptable/Average, meets basic expectations - 5 = Excellent, exceeds expectations</p>
</section>
<section id="comprehensive-rating-categories" class="level3">
<h3 class="anchored" data-anchor-id="comprehensive-rating-categories">Comprehensive Rating Categories</h3>
<p>The survey covered:</p>
<p><strong>A. Overall Satisfaction</strong> (quality, purchase intent, recommendation likelihood)</p>
<p><strong>B. Taste &amp; Flavor</strong> (overall taste, flavor intensity, saltiness, naturalness, aftertaste)</p>
<p><strong>C. Texture &amp; Physical Quality</strong> (crunchiness, thickness, consistency, oiliness, freshness)</p>
<p><strong>D. Visual Appeal</strong> (appearance, color, uniformity, packaging appeal, information clarity)</p>
<p><strong>E. Value &amp; Competitive Positioning</strong> (value for money, price sensitivity, preference vs potato chips and competitors)</p>
<p><strong>F. Product Attributes</strong> (uniqueness, healthiness, suitability for guests, meeting expectations)</p>
<p><strong>G. Usage Context</strong> (purchase frequency, consumption occasions)</p>
<p>Each persona rated 33 quantitative questions plus provided qualitative feedback on likes, improvements, and how they’d describe the product.</p>
</section>
</section>
<section id="what-we-learned-from-the-exercise" class="level2">
<h2 class="anchored" data-anchor-id="what-we-learned-from-the-exercise">What We Learned from the Exercise</h2>
<section id="ai-can-generate-plausible-responses" class="level3">
<h3 class="anchored" data-anchor-id="ai-can-generate-plausible-responses">AI Can Generate Plausible Responses</h3>
<p>ChatGPT was surprisingly good at maintaining consistent personas. The middle-aged rural Latina with hypertension and obesity consistently flagged sodium concerns and price sensitivity across multiple questions. The young urban health-conscious Latina responded positively to premium positioning and cultural connection.</p>
<p>The personas felt internally coherent - their ratings for saltiness, healthiness, and value aligned with their demographic profiles and stated health concerns.</p>
</section>
<section id="but-its-still-simulated-data" class="level3">
<h3 class="anchored" data-anchor-id="but-its-still-simulated-data">But It’s Still Simulated Data</h3>
<p>The fundamental limitation: these aren’t real taste preferences. The AI is generating statistically plausible responses based on stereotypical associations between demographics and preferences.</p>
<p>For example, it “knows” that someone with hypertension should care about sodium, so it rates accordingly. But it can’t actually tell us if our specific salt level tastes good or if the Peruvian sal de Maras provides a noticeably different experience.</p>
</section>
<section id="useful-for-initial-direction" class="level3">
<h3 class="anchored" data-anchor-id="useful-for-initial-direction">Useful for Initial Direction</h3>
<p>Where this exercise helped: - Identifying which demographic segments might prefer premium vs everyday positioning - Spotting potential concerns (price sensitivity in rural markets, sodium levels for health-conscious segments) - Practicing survey design before using it with real participants - Understanding how different personas might prioritize different product attributes</p>
</section>
</section>
<section id="serious-limitations" class="level2">
<h2 class="anchored" data-anchor-id="serious-limitations">Serious Limitations</h2>
<section id="no-actual-sensory-experience" class="level3">
<h3 class="anchored" data-anchor-id="no-actual-sensory-experience">No Actual Sensory Experience</h3>
<p>The biggest problem: ChatGPT hasn’t tasted anything. It can’t tell us if our plantain chips are actually crunchy, if the salt level is genuinely balanced, or if the flavor profile works.</p>
<p>All taste-related responses are based on generic associations (“premium plantain chips should be crunchier,” “simple seasoning means less salty”). These might not match reality.</p>
</section>
<section id="reinforces-stereotypes" class="level3">
<h3 class="anchored" data-anchor-id="reinforces-stereotypes">Reinforces Stereotypes</h3>
<p>The AI generates responses based on demographic patterns it learned from training data. This means it might reproduce stereotypical assumptions rather than capturing actual individual preferences.</p>
<p>A real 42-year-old Latino with hypertension might not care about sodium as much as the persona suggests, or might have completely different taste preferences than “typical” for that demographic.</p>
</section>
<section id="cant-capture-real-market-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="cant-capture-real-market-dynamics">Can’t Capture Real Market Dynamics</h3>
<p>Things the simulation can’t tell us: - Whether people would actually notice our product on shelves - If the packaging design triggers emotional responses - Whether word-of-mouth would happen organically - If there are unexpected use cases we haven’t considered - How brand perception builds over time</p>
</section>
<section id="the-validation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-validation-problem">The Validation Problem</h3>
<p>We built in validation checks (questions 31-33) to catch inconsistent rating patterns. But when the AI is generating all responses, it’s just validating its own internal consistency, not actual human behavior.</p>
<p>According to <a href="https://arxiv.org/abs/2305.17564">research on AI-generated synthetic data</a>, using LLM outputs as substitutes for human research data can introduce systematic biases that aren’t immediately obvious.</p>
</section>
</section>
<section id="how-were-actually-using-this" class="level2">
<h2 class="anchored" data-anchor-id="how-were-actually-using-this">How We’re Actually Using This</h2>
<section id="initial-hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="initial-hypothesis-testing">Initial Hypothesis Testing</h3>
<p>The AI focus group helped us form hypotheses about product positioning: - Cuzco Crunch might appeal more to urban health-conscious consumers willing to pay premium - Plantain Lite could work better for price-sensitive families and convenience-focused shoppers - Both products might face sodium concerns from health-conscious segments</p>
<p>But these are just hypotheses that need real validation.</p>
</section>
<section id="survey-design-practice" class="level3">
<h3 class="anchored" data-anchor-id="survey-design-practice">Survey Design Practice</h3>
<p>Building the comprehensive survey instrument was valuable. We learned: - Which questions provide useful differentiation - How to structure rating scales with proper calibration - What validation checks to include - Which demographic factors might matter most</p>
<p>The survey itself is now ready to use with actual participants.</p>
</section>
<section id="next-steps-with-real-people" class="level3">
<h3 class="anchored" data-anchor-id="next-steps-with-real-people">Next Steps with Real People</h3>
<p>We’re not making product decisions based on AI responses. The plan is: 1. Use the survey with real taste testers 2. Compare actual results to AI predictions 3. Identify where AI assumptions were wrong 4. Make product decisions based on real preferences</p>
<p>The AI exercise was a dress rehearsal, not the actual performance.</p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Running a virtual focus group with AI personas was an interesting experiment in using LLMs for product development. It’s useful for: - Rapid hypothesis generation - Testing survey instruments - Exploring how different demographic segments might respond - Practicing market research methodologies</p>
<p>But it’s dangerous if you: - Trust the responses as actual market data - Skip real human testing because “we already did AI testing” - Make product decisions based on simulated preferences - Assume the AI understands nuanced taste experiences</p>
<p>For Cuzco Crunch, this exercise helped us structure our research approach and form initial hypotheses. But we’re clear that actual product validation requires real people tasting real chips. The AI can simulate responses, but it can’t simulate whether our plantain chips actually taste good.</p>


</section>

 ]]></description>
  <category>branding</category>
  <category>LLM</category>
  <category>entrepreneurship</category>
  <category>creative</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/011.html</guid>
  <pubDate>Fri, 03 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/cuzco-crunch-package.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using LLMs for Interview Prep</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/010.html</link>
  <description><![CDATA[ 





<section id="building-an-mbb-interview-assistant" class="level2">
<h2 class="anchored" data-anchor-id="building-an-mbb-interview-assistant">Building an MBB Interview Assistant</h2>
<p>With consulting recruiting coming up, I built a custom project in Claude to help with case interview prep. The idea was to have a dedicated space where I could practice cases, track my progress, and get feedback—all in one place.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/interview-preparation.jpg" class="img-fluid" alt="Interview Preparation"> <em>Generic stock photo of a professional interview setting</em></p>
</section>
<section id="how-the-assistant-works" class="level2">
<h2 class="anchored" data-anchor-id="how-the-assistant-works">How the Assistant Works</h2>
<section id="the-setup" class="level3">
<h3 class="anchored" data-anchor-id="the-setup">The Setup</h3>
<p>I created a Claude project called “MBB Interview Assistant” with custom instructions:</p>
<blockquote class="blockquote">
<p>“You will be hearing inputs from MBB interviews. Your role as my highly-paid MBB tutor is to come up with frameworks. Answer questions quickly and brainstorm potential questions, and then… develop a MECE, MBB approach (case structure).”</p>
</blockquote>
<p>The project has become a repository of my case practice—I can see all my past cases listed: UK leisure club market analysis, UK media company revenue challenges, Science magazine revenue decline, Residential cable company alarm services, Kids Place Daycare Capacity Strategy.</p>
</section>
<section id="using-the-voice-recording-feature" class="level3">
<h3 class="anchored" data-anchor-id="using-the-voice-recording-feature">Using the Voice Recording Feature</h3>
<p>The most useful feature has been Claude’s voice recording capability. I can speak through a case out loud, and it transcribes everything accurately and then provides feedback. This simulates the verbal nature of actual case interviews better than typing.</p>
<p>For example, when I was working through the UK leisure club case, I could talk through my framework structure verbally, and Claude would follow along and point out gaps in my logic or areas I hadn’t considered.</p>
</section>
</section>
<section id="what-actually-works" class="level2">
<h2 class="anchored" data-anchor-id="what-actually-works">What Actually Works</h2>
<section id="case-tracking" class="level3">
<h3 class="anchored" data-anchor-id="case-tracking">Case Tracking</h3>
<p>Having all my practice cases in one place is genuinely helpful. I can see which types of cases I’ve worked on and which I need more practice with. The project view shows each case with timestamps, so I can track my prep progress over time.</p>
</section>
<section id="framework-development" class="level3">
<h3 class="anchored" data-anchor-id="framework-development">Framework Development</h3>
<p>Claude is good at helping me structure MECE (Mutually Exclusive, Collectively Exhaustive) frameworks. When I’m stuck on how to break down a problem, it can suggest logical buckets to organize my thinking.</p>
<p>For the daycare capacity strategy case, it helped me think through supply-side factors (staffing, facilities, regulations) and demand-side factors (demographics, competition, pricing) in a structured way.</p>
</section>
<section id="accurate-transcription" class="level3">
<h3 class="anchored" data-anchor-id="accurate-transcription">Accurate Transcription</h3>
<p>The voice recording feature transcribes my verbal case practice accurately. I can review what I actually said rather than what I thought I said, which helps identify verbal tics or unclear explanations.</p>
</section>
<section id="quick-feedback" class="level3">
<h3 class="anchored" data-anchor-id="quick-feedback">Quick Feedback</h3>
<p>It provides immediate feedback on structural issues—missing elements in my framework, calculations I didn’t account for, or assumptions I should have stated explicitly.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/claude-interview-project.jpg" class="img-fluid" alt="Claude Project Interface"> <em>My MBB Interview Assistant project showing case history and files</em></p>
</section>
</section>
<section id="major-limitations-ive-found" class="level2">
<h2 class="anchored" data-anchor-id="major-limitations-ive-found">Major Limitations I’ve Found</h2>
<section id="hallucinations-and-made-up-information" class="level3">
<h3 class="anchored" data-anchor-id="hallucinations-and-made-up-information">Hallucinations and Made-Up Information</h3>
<p>This is the biggest problem. Claude sometimes makes up data points or industry facts that sound plausible but aren’t real. In the media company revenue case, it cited specific market statistics that I later couldn’t verify anywhere.</p>
<p>For interview prep, this is dangerous because you might memorize false information and confidently state it in a real interview. I’ve learned to verify anything that sounds like a specific fact or statistic.</p>
</section>
<section id="going-off-topic" class="level3">
<h3 class="anchored" data-anchor-id="going-off-topic">Going Off Topic</h3>
<p>Sometimes Claude diverges from the specific case question to discuss tangentially related concepts. When working on the cable company alarm services case, it started explaining general IoT trends instead of focusing on the specific competitive positioning question I was asking about.</p>
</section>
<section id="scope-creep" class="level3">
<h3 class="anchored" data-anchor-id="scope-creep">Scope Creep</h3>
<p>The AI occasionally suggests frameworks or analyses that are way too broad for a 20-minute case interview. It might recommend conducting customer surveys or building complex financial models—things that wouldn’t be feasible in an actual interview setting.</p>
</section>
<section id="weird-or-broken-links" class="level3">
<h3 class="anchored" data-anchor-id="weird-or-broken-links">Weird or Broken Links</h3>
<p>When Claude tries to reference sources or suggest additional reading, the links are often fabricated or don’t lead anywhere useful. I’ve clicked on several suggested resources only to find they don’t exist.</p>
</section>
<section id="cant-simulate-real-pressure" class="level3">
<h3 class="anchored" data-anchor-id="cant-simulate-real-pressure">Can’t Simulate Real Pressure</h3>
<p>The fundamental limitation remains: practicing with an AI in my room doesn’t replicate the pressure of a real interviewer. There’s no one judging my confidence, no awkward silences to manage, no reading of facial expressions.</p>
<p><a href="https://www.mckinsey.com/careers/interviewing">Research on case interview preparation</a> emphasizes that the interpersonal dynamics and time pressure are critical elements that can only be practiced with real people.</p>
</section>
</section>
<section id="my-actual-approach" class="level2">
<h2 class="anchored" data-anchor-id="my-actual-approach">My Actual Approach</h2>
<section id="early-stage-framework-practice" class="level3">
<h3 class="anchored" data-anchor-id="early-stage-framework-practice">Early-Stage Framework Practice</h3>
<p>I use the Claude project at the beginning of my prep for each case type. It helps me understand what a good framework looks like and gives me immediate feedback on structural issues.</p>
</section>
<section id="verbal-practice-tool" class="level3">
<h3 class="anchored" data-anchor-id="verbal-practice-tool">Verbal Practice Tool</h3>
<p>The voice recording feature is useful for getting comfortable speaking through cases out loud. I can practice articulating my thinking without the pressure of another person listening.</p>
</section>
<section id="progress-tracking" class="level3">
<h3 class="anchored" data-anchor-id="progress-tracking">Progress Tracking</h3>
<p>The project acts as a log of all the cases I’ve practiced. I can see patterns in which types of cases I struggle with and where I need more work.</p>
</section>
<section id="always-verify-facts" class="level3">
<h3 class="anchored" data-anchor-id="always-verify-facts">Always Verify Facts</h3>
<p>I never trust specific data points or statistics from Claude without verification. If it mentions a market size or industry trend, I look it up independently.</p>
</section>
<section id="real-humans-are-essential" class="level3">
<h3 class="anchored" data-anchor-id="real-humans-are-essential">Real Humans Are Essential</h3>
<p>After using Claude for initial structure practice, I do the same cases with actual people—friends doing consulting recruiting, career services, or case partners. The AI gets me to baseline competence faster, but real practice is what actually prepares you for the interview dynamic.</p>
</section>
</section>
<section id="final-take" class="level2">
<h2 class="anchored" data-anchor-id="final-take">Final Take</h2>
<p>The MBB Interview Assistant project is a useful supplementary tool but comes with significant limitations. It’s good for: - Organizing and tracking case practice - Getting quick structural feedback - Practicing verbal articulation with voice recordings - Understanding framework basics</p>
<p>But it’s actively harmful if you: - Trust its specific facts without verification - Rely on it as your primary practice method - Follow its suggestions when they’re too broad or impractical - Think it can replace practicing with real people</p>
<p>The hallucination issue is serious enough that I treat everything Claude says as a suggestion to verify rather than information to memorize. For actual interview prep, the AI is a starting point, not a replacement for traditional practice methods.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>entrepreneurship</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/010.html</guid>
  <pubDate>Wed, 01 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/interview-preparation.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Experimenting with Vibe Creation Using LLMs</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/009.html</link>
  <description><![CDATA[ 





<section id="playing-with-atmospheric-prompting" class="level2">
<h2 class="anchored" data-anchor-id="playing-with-atmospheric-prompting">Playing with Atmospheric Prompting</h2>
<p>I’ve been experimenting with what I’m calling “vibe creation” in Claude—essentially asking the LLM to generate or interpret atmospheric descriptions for different contexts. The goal was to see how well these models understand and reproduce subjective, mood-based content rather than factual information.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-prompting.jpg" class="img-fluid" alt="Vibe Prompting Interface"> <em>Testing different prompts to generate atmospheric descriptions</em></p>
</section>
<section id="the-prompts-i-tested" class="level2">
<h2 class="anchored" data-anchor-id="the-prompts-i-tested">The Prompts I Tested</h2>
<section id="starting-simple" class="level3">
<h3 class="anchored" data-anchor-id="starting-simple">Starting Simple</h3>
<p>My first prompt was straightforward:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of a rainy coffee shop at 3pm on a Tuesday”</p>
</blockquote>
<p>Claude generated a description that hit the expected notes—muted conversations, steam from coffee cups, the rhythmic sound of rain. It was accurate but generic, the kind of description you’d find in any piece of atmospheric writing.</p>
</section>
<section id="adding-specificity" class="level3">
<h3 class="anchored" data-anchor-id="adding-specificity">Adding Specificity</h3>
<p>I tried making the prompt more specific:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of a college library during finals week at 2am. Include sounds, lighting, and the feeling of collective stress”</p>
</blockquote>
<p>This produced better results. The model captured details like the fluorescent lighting, the sound of highlighters on paper, and the “quiet panic” of students. It understood the assignment but still felt somewhat detached—like someone describing a scene they’d read about rather than experienced.</p>
</section>
<section id="testing-negative-space" class="level3">
<h3 class="anchored" data-anchor-id="testing-negative-space">Testing Negative Space</h3>
<p>Here’s where it got interesting. I asked:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of an empty office building at night. Focus on what’s NOT there rather than what is”</p>
</blockquote>
<p>Claude struggled more with this. It could describe absence—no people, no noise—but couldn’t quite capture the uncanny feeling of a space designed for activity sitting dormant. The descriptions remained surface-level. The output focused on the lack of meetings, chatter, talk, noise in the office. But it did not capture the liminal-space-like, vast, expansive (almost scary) vibe of an empty office in the way the show Severance, for instance, can.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-comparison.jpg" class="img-fluid" alt="Vibe Comparison Results"> <em>Comparing outputs from different prompting approaches</em></p>
</section>
</section>
<section id="what-worked-and-what-didnt" class="level2">
<h2 class="anchored" data-anchor-id="what-worked-and-what-didnt">What Worked and What Didn’t</h2>
<section id="strengths" class="level3">
<h3 class="anchored" data-anchor-id="strengths">Strengths</h3>
<p>The LLM is good at: - Assembling sensory details that conventionally fit a scene - Understanding cultural contexts (“finals week” immediately triggered appropriate stress markers) - Maintaining consistency within a single atmospheric description - Responding to structural prompts (“focus on sounds” vs “focus on lighting”)</p>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Where it falls short: - Generating truly novel atmospheric combinations - Capturing subtle emotional undertones that aren’t explicitly named - Moving beyond stereotypical associations (coffee shop = cozy, library = studious) - Understanding the relationship between physical space and psychological state in non-obvious ways</p>
<p>The model seems to work from a database of common associations rather than synthesizing new atmospheric understanding.</p>
</section>
</section>
<section id="why-this-matters-from-a-skeptical-angle" class="level2">
<h2 class="anchored" data-anchor-id="why-this-matters-from-a-skeptical-angle">Why This Matters (From a Skeptical Angle)</h2>
<section id="the-homogenization-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-homogenization-problem">The Homogenization Problem</h3>
<p>If everyone starts using LLMs to generate atmospheric descriptions for creative projects, writing, or marketing, we risk creating a feedback loop of averaged-out vibes. The model can only reproduce what it’s seen in training data, which means we get the most statistically likely description of any given mood or setting.</p>
<p>After all, LLMs tend to regress toward mean responses when generating creative content, potentially flattening the diversity of human expression.</p>
</section>
<section id="lost-nuance" class="level3">
<h3 class="anchored" data-anchor-id="lost-nuance">Lost Nuance</h3>
<p>Real atmospheric writing comes from personal observation and specific experience. When I asked Claude to describe my college library during finals, it gave me a generic college library experience. It couldn’t know about the specific smell of old heating systems mixed with energy drinks, or the way light reflects off that one weird pillar in the corner.</p>
</section>
<section id="useful-but-limited-tool" class="level3">
<h3 class="anchored" data-anchor-id="useful-but-limited-tool">Useful But Limited Tool</h3>
<p>This isn’t to say vibe creation with LLMs is useless. It’s good for: - Quick brainstorming when you’re stuck - Getting a baseline description to then personalize - Understanding how certain settings are conventionally described</p>
<p>But treating it as a replacement for actual observation and personal experience would result in flatter, less distinctive creative work.</p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Playing with vibe creation revealed both the capabilities and constraints of current LLMs. They’re sophisticated pattern-matching systems that can assemble convincing atmospheric descriptions from training data, but they lack the experiential knowledge that makes truly evocative writing memorable.</p>
<p>The exercise reinforced my skepticism about over-relying on these tools for creative work. They’re useful for scaffolding and inspiration, but the most interesting atmospheric details still need to come from human observation and experience.</p>
<p>For anyone trying similar experiments: push the model toward specificity and unconventional combinations. The more generic your prompt, the more generic your output.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>creative</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/009.html</guid>
  <pubDate>Mon, 29 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-prompting.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Photo Organization: Finding Pictures I Forgot I Had</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /008.html</link>
  <description><![CDATA[ 





<section id="searching-through-10000-photos" class="level2">
<h2 class="anchored" data-anchor-id="searching-through-10000-photos">Searching Through 10,000 Photos</h2>
<p>I realized I had over 10,000 photos in Google Photos and no way to find specific ones. Today I tried using the AI search features to locate pictures from last year’s vacation, and it actually worked better than expected.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-search.jpg" class="img-fluid" alt="AI Photo Search"> <em>Google Photos AI recognizing objects, people, and locations</em></p>
</section>
<section id="what-the-ai-can-find" class="level2">
<h2 class="anchored" data-anchor-id="what-the-ai-can-find">What the AI Can Find</h2>
<section id="object-recognition" class="level3">
<h3 class="anchored" data-anchor-id="object-recognition">Object Recognition</h3>
<p>I searched for “pizza” and it found every photo with pizza in it, even ones where pizza was just sitting on a table in the background. Same thing worked for “dog,” “beach,” and “birthday cake.” The AI recognizes way more objects than I expected.</p>
</section>
<section id="face-grouping" class="level3">
<h3 class="anchored" data-anchor-id="face-grouping">Face Grouping</h3>
<p>Google Photos automatically groups photos by faces, so I can find all pictures of specific people without having to manually tag them. It even recognizes people as they age or in different lighting.</p>
</section>
<section id="location-and-time" class="level3">
<h3 class="anchored" data-anchor-id="location-and-time">Location and Time</h3>
<p>If you have location data turned on, you can search by place names. I found all my photos from “San Francisco” or “coffee shop” pretty easily. Time-based searches work too - “photos from summer 2024” pulled up the right time period.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-albums.jpg" class="img-fluid" alt="Auto-Generated Albums"> <em>Automatically created albums based on events and trips</em></p>
</section>
</section>
<section id="automatic-albums" class="level2">
<h2 class="anchored" data-anchor-id="automatic-albums">Automatic Albums</h2>
<section id="trip-detection" class="level3">
<h3 class="anchored" data-anchor-id="trip-detection">Trip Detection</h3>
<p>The AI creates albums for trips automatically based on location and date patterns. It figured out my weekend in Portland and grouped all those photos together without me doing anything.</p>
</section>
<section id="event-recognition" class="level3">
<h3 class="anchored" data-anchor-id="event-recognition">Event Recognition</h3>
<p>It also makes albums for things like “Birthday party” or “Graduation” based on the types of photos and timing. Sometimes it gets it wrong - labeled a regular dinner as a “celebration” - but it’s right more often than not.</p>
</section>
<section id="people-albums" class="level3">
<h3 class="anchored" data-anchor-id="people-albums">People Albums</h3>
<p>Creates collections of photos featuring specific people, which is useful for family pictures or when you want to share photos of someone with them.</p>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="privacy-concerns" class="level3">
<h3 class="anchored" data-anchor-id="privacy-concerns">Privacy Concerns</h3>
<p>All this convenience requires uploading your photos to Google’s servers where they analyze everything. If that bothers you, you can’t really use these features.</p>
</section>
<section id="sometimes-too-smart" class="level3">
<h3 class="anchored" data-anchor-id="sometimes-too-smart">Sometimes Too Smart</h3>
<p>The AI occasionally creates albums for things that weren’t actually events, or groups random photos together based on superficial similarities.</p>
</section>
<section id="search-isnt-perfect" class="level3">
<h3 class="anchored" data-anchor-id="search-isnt-perfect">Search Isn’t Perfect</h3>
<p>Complex searches don’t always work. “Photos of John at the beach” might miss some or include wrong results. Simple, single-concept searches work much better.</p>
<p>It’s genuinely useful for finding old photos you’d never locate otherwise. The search functionality makes having thousands of photos actually manageable instead of just overwhelming.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>photos</category>
  <category>organization</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /008.html</guid>
  <pubDate>Sat, 27 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-search.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Email Management That Actually Works</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/007.html</link>
  <description><![CDATA[ 





<section id="letting-ai-handle-my-inbox" class="level2">
<h2 class="anchored" data-anchor-id="letting-ai-handle-my-inbox">Letting AI Handle My Inbox</h2>
<p>I set up SaneBox to manage my email and it’s been running for about a week now. Instead of manually sorting through everything, the AI decides what’s important and what can wait.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-inbox-sorting.jpg" class="img-fluid" alt="Smart Inbox Sorting"> <em>AI automatically categorizing emails by priority level</em></p>
</section>
<section id="how-it-sorts-things-out" class="level2">
<h2 class="anchored" data-anchor-id="how-it-sorts-things-out">How It Sorts Things Out</h2>
<section id="priority-filtering" class="level3">
<h3 class="anchored" data-anchor-id="priority-filtering">Priority Filtering</h3>
<p>SaneBox learns from which emails I actually open and respond to. Work emails from my team get flagged as important, while newsletters and promotional stuff gets moved to a separate folder. It’s not perfect but catches most of the obvious stuff.</p>
</section>
<section id="smart-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="smart-scheduling">Smart Scheduling</h3>
<p>Gmail’s AI also suggests response times and can schedule emails to send later. I’ve been using it to send emails during business hours even when I’m working late, so I don’t look like I’m always online.</p>
</section>
<section id="quick-replies" class="level3">
<h3 class="anchored" data-anchor-id="quick-replies">Quick Replies</h3>
<p>The suggested responses are actually useful for simple emails. When someone sends a meeting request, it’ll suggest “Looks good” or “Let me check my calendar” which saves typing the same responses over and over.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-email-responses.jpg" class="img-fluid" alt="Email Response Suggestions"> <em>AI-generated reply suggestions for common email types</em></p>
</section>
</section>
<section id="what-ive-noticed" class="level2">
<h2 class="anchored" data-anchor-id="what-ive-noticed">What I’ve Noticed</h2>
<section id="less-time-sorting" class="level3">
<h3 class="anchored" data-anchor-id="less-time-sorting">Less Time Sorting</h3>
<p>The main benefit is spending less time deciding what to read first. Important emails show up in the main inbox, everything else gets organized automatically.</p>
</section>
<section id="fewer-missed-messages" class="level3">
<h3 class="anchored" data-anchor-id="fewer-missed-messages">Fewer Missed Messages</h3>
<p>Before, urgent emails would get buried under promotional stuff. Now they’re separated, so I’m less likely to miss something important.</p>
</section>
<section id="still-need-to-check" class="level3">
<h3 class="anchored" data-anchor-id="still-need-to-check">Still Need to Check</h3>
<p>The AI sometimes gets it wrong - puts important emails in the low-priority folder or flags spam as urgent. You still need to glance through everything, but it’s more organized.</p>
<p>It’s a decent time-saver for people who get a lot of email. Not life-changing, but removes some of the daily friction of inbox management. The filtering works better than I expected, though you need to train it for a few days before it gets your priorities right.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>productivity</category>
  <category>email</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/007.html</guid>
  <pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-inbox-sorting.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Building a Brand with AI: Cuzco Crunch</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/006.html</link>
  <description><![CDATA[ 





<section id="creating-a-brand-with-ai" class="level2">
<h2 class="anchored" data-anchor-id="creating-a-brand-with-ai">Creating a Brand with AI</h2>
<p>Eury and I decided to try building a snack brand called Cuzco Crunch, mostly to see how much of the process we could handle using AI tools. We are just experimenting with what’s possible when you use GPT for everything from naming to packaging concepts.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch.jpg" class="img-fluid" alt="Cuzco Crunch Logo"> <em>Initial logo concepts generated through AI prompting</em></p>
</section>
<section id="what-were-using-ai-for" class="level2">
<h2 class="anchored" data-anchor-id="what-were-using-ai-for">What We’re Using AI For</h2>
<section id="brand-development" class="level3">
<h3 class="anchored" data-anchor-id="brand-development">Brand Development</h3>
<p>The name “Cuzco Crunch” came from asking GPT to suggest names that sounded distinctive but not too weird. We wanted something that hinted at South American flavors without being too on-the-nose. It generated about 30 options and this one felt right.</p>
</section>
<section id="packaging-design-direction" class="level3">
<h3 class="anchored" data-anchor-id="packaging-design-direction">Packaging Design Direction</h3>
<p>We’re using AI to create mood boards and design concepts. Instead of hiring a designer upfront, we’re generating different packaging styles to see what resonates. The AI helps us think through color schemes, typography, and overall brand personality.</p>
</section>
<section id="marketing-copy" class="level3">
<h3 class="anchored" data-anchor-id="marketing-copy">Marketing Copy</h3>
<p>Product descriptions, social media posts, and even this blog post started with AI-generated drafts that we then edited. It’s faster than staring at a blank page, and sometimes the AI suggests angles we wouldn’t have thought of.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch-packaging.jpg" class="img-fluid" alt="Packaging Concepts"> <em>Various packaging design concepts and color schemes</em></p>
</section>
</section>
<section id="whats-working-and-what-isnt" class="level2">
<h2 class="anchored" data-anchor-id="whats-working-and-what-isnt">What’s Working and What Isn’t</h2>
<section id="the-good-stuff" class="level3">
<h3 class="anchored" data-anchor-id="the-good-stuff">The Good Stuff</h3>
<p>Speed is the biggest advantage. We can iterate on ideas quickly without waiting for external feedback or spending money on design consultations. The AI is also good at generating variations - give it one concept and it’ll produce 10 different takes on it.</p>
</section>
<section id="the-limitations" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations">The Limitations</h3>
<p>Everything needs human editing. The AI-generated copy often sounds too generic or tries too hard to be clever. Design concepts look decent but lack the subtle details that make professional packaging stand out on shelves.</p>
<p>Also, the AI doesn’t understand practical constraints. It’ll suggest elaborate packaging designs that would be expensive to produce or flavors that might not actually taste good together.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch-mockups.jpg" class="img-fluid" alt="Package Mockups"> <em>3D mockups and shelf visualization concepts</em></p>
</section>
</section>
<section id="the-reality-check" class="level2">
<h2 class="anchored" data-anchor-id="the-reality-check">The Reality Check</h2>
<section id="still-need-real-skills" class="level3">
<h3 class="anchored" data-anchor-id="still-need-real-skills">Still Need Real Skills</h3>
<p>AI handles the brainstorming and initial concepts, but you still need to understand branding, know your target market, and make business decisions. It’s a tool, not a replacement for actual knowledge about building a brand.</p>
</section>
<section id="cost-vs.-quality-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="cost-vs.-quality-trade-offs">Cost vs.&nbsp;Quality Trade-offs</h3>
<p>We’re probably saving money in the early stages, but there’s a point where you need professional designers and marketers. The question is finding that sweet spot where AI gets you far enough to make informed decisions about what’s worth investing in.</p>
</section>
<section id="learning-experience" class="level3">
<h3 class="anchored" data-anchor-id="learning-experience">Learning Experience</h3>
<p>The most valuable part has been understanding how much work goes into brand development. Using AI to handle the repetitive parts lets us focus on strategy and decision-making instead of getting stuck on execution details.</p>
<p>We’re treating this as an experiment rather than a serious business venture. If Cuzco Crunch turns into something real, great. If not, we’ve learned a lot about using AI for creative projects and brand development.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>branding</category>
  <category>entrepreneurship</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/006.html</guid>
  <pubDate>Tue, 23 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Fitness Coaching at Home</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/005.html</link>
  <description><![CDATA[ 





<section id="working-out-with-ai-guidance" class="level2">
<h2 class="anchored" data-anchor-id="working-out-with-ai-guidance">Working Out with AI Guidance</h2>
<p>I tried the Freeletics app today instead of going to the gym. It uses your phone’s camera to watch your form and give feedback in real time, which felt like having a trainer watching you.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/ai-form-analysis.jpg" class="img-fluid" alt="AI Form Analysis"> <em>Smartphone analyzing exercise form with real-time feedback</em></p>
</section>
<section id="how-it-works" class="level2">
<h2 class="anchored" data-anchor-id="how-it-works">How It Works</h2>
<section id="real-time-form-correction" class="level3">
<h3 class="anchored" data-anchor-id="real-time-form-correction">Real-Time Form Correction</h3>
<p>You prop your phone up and the camera tracks your movements. When I was doing squats, it noticed I wasn’t going low enough and told me to “squat deeper” right as I was doing the exercise. It also caught that I was leaning too far forward.</p>
</section>
<section id="counts-reps-automatically" class="level3">
<h3 class="anchored" data-anchor-id="counts-reps-automatically">Counts Reps Automatically</h3>
<p>The app counts your repetitions so you don’t have to keep track. It only counts reps with proper form, so if your push-up doesn’t go low enough, it won’t count it.</p>
</section>
<section id="adjusts-based-on-performance" class="level3">
<h3 class="anchored" data-anchor-id="adjusts-based-on-performance">Adjusts Based on Performance</h3>
<p>When I started struggling with burpees halfway through, the app suggested switching to a modified version. It seemed to pick up on the fact that my form was getting sloppy and offered an easier variation.</p>
</section>
</section>
<section id="what-works-well" class="level2">
<h2 class="anchored" data-anchor-id="what-works-well">What Works Well</h2>
<section id="form-feedback" class="level3">
<h3 class="anchored" data-anchor-id="form-feedback">Form Feedback</h3>
<p>The corrections are actually helpful. It’s like having someone watch you who knows what proper form looks like. Caught several things I didn’t realize I was doing wrong.</p>
</section>
<section id="no-equipment-needed" class="level3">
<h3 class="anchored" data-anchor-id="no-equipment-needed">No Equipment Needed</h3>
<p>All bodyweight exercises, so you can do it anywhere. The app designs workouts based on what space and time you have available.</p>
</section>
<section id="adapts-to-your-level" class="level3">
<h3 class="anchored" data-anchor-id="adapts-to-your-level">Adapts to Your Level</h3>
<p>Starts with easier exercises and gradually increases difficulty based on how you perform. If you’re struggling, it scales back. If you’re crushing it, it adds more challenging moves.</p>
</section>
<section id="tracks-progress-over-time" class="level3">
<h3 class="anchored" data-anchor-id="tracks-progress-over-time">Tracks Progress Over Time</h3>
<p>Keeps track of how many reps you can do and how your form improves. You can see whether you’re getting stronger or if certain exercises are still difficult.</p>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="camera-angle-matters" class="level3">
<h3 class="anchored" data-anchor-id="camera-angle-matters">Camera Angle Matters</h3>
<p>You need to position your phone correctly for it to see your whole body. Took a few tries to get the angle right.</p>
</section>
<section id="lighting-requirements" class="level3">
<h3 class="anchored" data-anchor-id="lighting-requirements">Lighting Requirements</h3>
<p>Works better in good lighting. Had trouble tracking movements when the room was too dim.</p>
</section>
<section id="limited-exercise-types" class="level3">
<h3 class="anchored" data-anchor-id="limited-exercise-types">Limited Exercise Types</h3>
<p>Focuses mainly on bodyweight movements. If you want to lift weights or use equipment, you’ll need a different approach.</p>
</section>
<section id="not-always-perfect" class="level3">
<h3 class="anchored" data-anchor-id="not-always-perfect">Not Always Perfect</h3>
<p>Sometimes misses form issues or gives feedback that doesn’t quite match what you’re doing. Still better than working out with no guidance, but not as precise as a human trainer.</p>
<p>It’s a solid option for home workouts when you want some structure and feedback. The form correction feature is genuinely useful, especially for exercises you’re not familiar with.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>fitness</category>
  <category>health</category>
  <category>daily-life</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/005.html</guid>
  <pubDate>Fri, 19 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/ai-form-analysis.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI-Assisted Grocery Shopping and Meal Prep</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/004.html</link>
  <description><![CDATA[ 





<section id="smarter-grocery-shopping" class="level2">
<h2 class="anchored" data-anchor-id="smarter-grocery-shopping">Smarter Grocery Shopping</h2>
<p>I tried out the AnyList grocery app today and it’s different from just writing things down on paper. Instead of my usual scattered notes and forgotten items, the app organizes everything and suggests what I might need.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/smart-grocery-list.jpg" class="img-fluid" alt="Smart Grocery List"> <em>AI-generated grocery list organized by store layout</em></p>
</section>
<section id="what-it-actually-does" class="level2">
<h2 class="anchored" data-anchor-id="what-it-actually-does">What It Actually Does</h2>
<section id="tracks-what-you-need" class="level3">
<h3 class="anchored" data-anchor-id="tracks-what-you-need">Tracks What You Need</h3>
<p>The app learns from your purchase history and can predict when you’re running low on regular items. It suggested I needed milk and bread this week before I even thought about it. Not perfect, but surprisingly accurate for staples I buy regularly.</p>
</section>
<section id="organizes-by-store-layout" class="level3">
<h3 class="anchored" data-anchor-id="organizes-by-store-layout">Organizes by Store Layout</h3>
<p>Instead of jumping around the store, the list is organized by sections - produce, dairy, meat, etc. Some apps even know the specific layout of stores you frequent, so your list follows the actual aisles.</p>
</section>
<section id="meal-planning-integration" class="level3">
<h3 class="anchored" data-anchor-id="meal-planning-integration">Meal Planning Integration</h3>
<p>You can add meals for the week and it automatically generates ingredient lists. When I planned tacos for Tuesday, it added ground beef, tortillas, and lettuce to my list. It also checked what I already had at home based on recent purchases.</p>
</section>
</section>
<section id="practical-benefits" class="level2">
<h2 class="anchored" data-anchor-id="practical-benefits">Practical Benefits</h2>
<section id="less-time-in-store" class="level3">
<h3 class="anchored" data-anchor-id="less-time-in-store">Less Time in Store</h3>
<p>Following an organized list means less wandering around looking for items. I’ve noticed my shopping trips are about 15-20 minutes shorter.</p>
</section>
<section id="fewer-forgotten-items" class="level3">
<h3 class="anchored" data-anchor-id="fewer-forgotten-items">Fewer Forgotten Items</h3>
<p>The app remembers things I consistently buy but often forget to write down, like batteries or cleaning supplies.</p>
</section>
<section id="better-meal-planning" class="level3">
<h3 class="anchored" data-anchor-id="better-meal-planning">Better Meal Planning</h3>
<p>When you can see ingredient costs upfront, it’s easier to plan meals within budget. The app sometimes suggests cheaper alternatives for expensive items.</p>
</section>
<section id="automatic-coupons" class="level3">
<h3 class="anchored" data-anchor-id="automatic-coupons">Automatic Coupons</h3>
<p>Many apps integrate with store loyalty programs and apply relevant coupons automatically. I’ve saved money without having to hunt for deals.</p>
</section>
</section>
<section id="what-doesnt-work-perfectly" class="level2">
<h2 class="anchored" data-anchor-id="what-doesnt-work-perfectly">What Doesn’t Work Perfectly</h2>
<section id="learning-period" class="level3">
<h3 class="anchored" data-anchor-id="learning-period">Learning Period</h3>
<p>It takes a few weeks for the app to understand your shopping patterns. Early suggestions were often wrong.</p>
</section>
<section id="store-specific-features" class="level3">
<h3 class="anchored" data-anchor-id="store-specific-features">Store-Specific Features</h3>
<p>Advanced features like aisle mapping only work with certain grocery chains. Smaller or independent stores usually don’t have this integration.</p>
</section>
<section id="fresh-produce-timing" class="level3">
<h3 class="anchored" data-anchor-id="fresh-produce-timing">Fresh Produce Timing</h3>
<p>The AI isn’t great at predicting when you need fresh items since it depends on how quickly you use them.</p>
<p>Overall, it’s a useful tool that makes grocery shopping more organized. The meal planning aspect is particularly helpful for busy weeks when you need to think ahead. Not essential, but definitely convenient once you get used to it.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>daily-life</category>
  <category>productivity</category>
  <category>health</category>
  <category>daily-life</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/004.html</guid>
  <pubDate>Thu, 18 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/smart-fridge-inventory.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Research with Perplexity AI</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/003.html</link>
  <description><![CDATA[ 





<section id="using-ai-for-research" class="level2">
<h2 class="anchored" data-anchor-id="using-ai-for-research">Using AI for Research</h2>
<p>I’ve been trying out Perplexity AI for research tasks lately. It’s different from regular Google searches because it synthesizes information from multiple sources and gives you one coherent answer with citations.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-research.jpg" class="img-fluid" alt="Perplexity AI Interface"> <em>Perplexity AI interface showing search results with sources</em></p>
</section>
<section id="how-it-works" class="level2">
<h2 class="anchored" data-anchor-id="how-it-works">How It Works</h2>
<section id="getting-information-from-multiple-sources" class="level3">
<h3 class="anchored" data-anchor-id="getting-information-from-multiple-sources">Getting Information from Multiple Sources</h3>
<p>Instead of opening multiple tabs and reading through different articles, Perplexity pulls information from various sources and combines it into a single response. When I was researching market trends for a work project, I got a summary that included key points from several industry reports, all with proper citations.</p>
</section>
<section id="follow-up-questions" class="level3">
<h3 class="anchored" data-anchor-id="follow-up-questions">Follow-up Questions</h3>
<p>You can ask follow-up questions that build on your previous search. After asking about “sustainable packaging trends 2025,” I could continue with: - “What are the cost implications?” - “Which companies are leading this transition?” - “How does this compare to 2024 predictions?”</p>
<p>The AI remembers the context, so you don’t need to re-explain what you’re looking for.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-citations.jpg" class="img-fluid" alt="Perplexity Search Results"> <em>Live citations and source integration in action</em></p>
</section>
</section>
<section id="when-its-useful" class="level2">
<h2 class="anchored" data-anchor-id="when-its-useful">When It’s Useful</h2>
<section id="academic-work" class="level3">
<h3 class="anchored" data-anchor-id="academic-work">Academic Work</h3>
<p>Good for getting an overview of research topics and finding relevant studies. Helps identify what’s already been studied and where there might be gaps.</p>
</section>
<section id="business-research" class="level3">
<h3 class="anchored" data-anchor-id="business-research">Business Research</h3>
<p>Useful for market analysis and industry trends. You can ask specific questions and get answers that draw from recent reports and news sources.</p>
</section>
<section id="learning-new-topics" class="level3">
<h3 class="anchored" data-anchor-id="learning-new-topics">Learning New Topics</h3>
<p>When you’re unfamiliar with a subject, it can explain concepts and provide background without having to piece together information from multiple websites.</p>
</section>
<section id="fact-checking" class="level3">
<h3 class="anchored" data-anchor-id="fact-checking">Fact-Checking</h3>
<p>Since it shows sources for each piece of information, it’s easier to verify claims and see where data comes from.</p>
</section>
</section>
<section id="what-ive-found" class="level2">
<h2 class="anchored" data-anchor-id="what-ive-found">What I’ve Found</h2>
<section id="saves-time" class="level3">
<h3 class="anchored" data-anchor-id="saves-time">Saves Time</h3>
<p>The main benefit is speed. You get the key information faster since it’s already compiled from multiple sources. You still need to read the original sources for details, but the initial overview is much quicker.</p>
</section>
<section id="clear-sources" class="level3">
<h3 class="anchored" data-anchor-id="clear-sources">Clear Sources</h3>
<p>Every piece of information comes with citations, so you can see exactly where it came from and verify it if needed.</p>
</section>
<section id="handles-context-well" class="level3">
<h3 class="anchored" data-anchor-id="handles-context-well">Handles Context Well</h3>
<p>The conversation aspect works better than I expected. You can refine questions or explore related topics without starting over.</p>
</section>
<section id="shows-different-viewpoints" class="level3">
<h3 class="anchored" data-anchor-id="shows-different-viewpoints">Shows Different Viewpoints</h3>
<p>When sources disagree, it usually mentions the different perspectives rather than just presenting one view.</p>
<p>It’s a useful tool for the initial stages of research when you need to understand a topic quickly or gather information from multiple sources. Not a replacement for thorough research, but it does help speed up the information gathering process.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>research</category>
  <category>productivity</category>
  <category>technology</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/003.html</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-research.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Do’s and Don’ts: Safe and Effective Usage</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/002.html</link>
  <description><![CDATA[ 





<section id="essential-ai-guidelines" class="level2">
<h2 class="anchored" data-anchor-id="essential-ai-guidelines">Essential AI Guidelines</h2>
<p>AI tools are powerful but require careful usage. These guidelines help you maximize benefits while avoiding common pitfalls.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-safety-guidelines.jpg" class="img-fluid" alt="AI Safety Guidelines"> <em>Key principles for responsible AI usage</em></p>
</section>
<section id="what-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-to-do">What TO Do</h2>
<section id="verify-important-information" class="level3">
<h3 class="anchored" data-anchor-id="verify-important-information">✅ Verify Important Information</h3>
<p>Always fact-check AI outputs for: - Medical or health advice - Financial decisions - Legal matters - Technical specifications - Current events or recent data</p>
<p>Cross-reference with authoritative sources before acting on AI recommendations.</p>
</section>
<section id="be-specific-in-your-prompts" class="level3">
<h3 class="anchored" data-anchor-id="be-specific-in-your-prompts">✅ Be Specific in Your Prompts</h3>
<p><strong>Instead of:</strong> “Help me with my presentation” <strong>Use:</strong> “Create an outline for a 10-minute sales presentation to executives about Q3 revenue growth”</p>
<p>Specific prompts produce more useful results.</p>
</section>
<section id="use-ai-for-brainstorming-and-drafts" class="level3">
<h3 class="anchored" data-anchor-id="use-ai-for-brainstorming-and-drafts">✅ Use AI for Brainstorming and Drafts</h3>
<p>AI excels at: - Generating initial ideas - Creating first drafts - Organizing thoughts - Providing alternative perspectives</p>
<p>Use AI output as a starting point, not a final product.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-workflow-diagram.jpg" class="img-fluid" alt="AI Workflow Best Practices"> <em>Proper workflow: AI assists, human validates and refines</em></p>
</section>
<section id="maintain-human-judgment" class="level3">
<h3 class="anchored" data-anchor-id="maintain-human-judgment">✅ Maintain Human Judgment</h3>
<p>Keep human oversight for: - Final decision-making - Quality assessment - Ethical considerations - Context that AI might miss</p>
</section>
<section id="ask-follow-up-questions" class="level3">
<h3 class="anchored" data-anchor-id="ask-follow-up-questions">✅ Ask Follow-Up Questions</h3>
<p>Improve results by asking: - “Can you explain this differently?” - “What are potential problems with this approach?” - “Give me three alternatives” - “Make this more specific”</p>
</section>
</section>
<section id="what-not-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-not-to-do">What NOT to Do</h2>
<section id="dont-share-sensitive-information" class="level3">
<h3 class="anchored" data-anchor-id="dont-share-sensitive-information">❌ Don’t Share Sensitive Information</h3>
<p>Never input: - Passwords or login credentials - Social Security numbers - Credit card details - Proprietary business information - Personal addresses or phone numbers - Private family details</p>
<p>Assume all AI conversations could be stored or accessed by others.</p>
</section>
<section id="dont-rely-on-ai-for-critical-decisions" class="level3">
<h3 class="anchored" data-anchor-id="dont-rely-on-ai-for-critical-decisions">❌ Don’t Rely on AI for Critical Decisions</h3>
<p>Avoid using AI alone for: - Medical diagnoses or treatment - Legal advice or document preparation - Financial investment decisions - Safety-critical calculations - Emergency situations</p>
<p>Consult qualified professionals for these matters.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-security-risks.jpg" class="img-fluid" alt="AI Limitations Warning"> <em>Areas where AI should not be the primary decision maker</em></p>
</section>
<section id="dont-assume-ai-is-always-accurate" class="level3">
<h3 class="anchored" data-anchor-id="dont-assume-ai-is-always-accurate">❌ Don’t Assume AI is Always Accurate</h3>
<p>AI can produce: - Outdated information - Factual errors - Biased responses - Plausible-sounding but incorrect details</p>
<p>Especially problematic for recent events or specialized technical information.</p>
</section>
<section id="dont-use-ai-for-harmful-purposes" class="level3">
<h3 class="anchored" data-anchor-id="dont-use-ai-for-harmful-purposes">❌ Don’t Use AI for Harmful Purposes</h3>
<p>Avoid requesting: - Misleading or false content - Harassment or threatening messages - Plagiarism or academic dishonesty - Illegal activity guidance - Discriminatory content</p>
</section>
<section id="dont-ignore-context-limitations" class="level3">
<h3 class="anchored" data-anchor-id="dont-ignore-context-limitations">❌ Don’t Ignore Context Limitations</h3>
<p>AI doesn’t understand: - Your full personal situation - Unspoken cultural context - Real-time environmental factors - Emotional nuances</p>
<p>Provide necessary context explicitly in your prompts.</p>
</section>
</section>
<section id="implementation-strategy" class="level2">
<h2 class="anchored" data-anchor-id="implementation-strategy">Implementation Strategy</h2>
<section id="start-small" class="level3">
<h3 class="anchored" data-anchor-id="start-small">Start Small</h3>
<p>Begin with low-stakes tasks like email drafting or meal planning before using AI for important projects.</p>
</section>
<section id="build-verification-habits" class="level3">
<h3 class="anchored" data-anchor-id="build-verification-habits">Build Verification Habits</h3>
<p>Develop a routine of checking AI outputs against reliable sources.</p>
</section>
<section id="keep-learning" class="level3">
<h3 class="anchored" data-anchor-id="keep-learning">Keep Learning</h3>
<p>AI capabilities and limitations evolve. Stay informed about updates and new research.</p>
</section>
<section id="document-what-works" class="level3">
<h3 class="anchored" data-anchor-id="document-what-works">Document What Works</h3>
<p>Save effective prompts and note which applications produce reliable results for your needs.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-usage-tips.jpg" class="img-fluid" alt="AI Usage Tips"> <em>Use these tips and tricks!</em></p>
<p>Following these guidelines ensures you gain AI’s benefits while avoiding common mistakes that can lead to poor decisions or security issues.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>safety</category>
  <category>guidelines</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/002.html</guid>
  <pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-safety-guidelines.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>5 Ways to Use AI Every Day</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/001.html</link>
  <description><![CDATA[ 





<section id="using-ai-for-daily-tasks" class="level2">
<h2 class="anchored" data-anchor-id="using-ai-for-daily-tasks">Using AI for Daily Tasks</h2>
<p>AI tools like ChatGPT and Claude can handle routine tasks that usually take significant time. Here are five practical applications that work reliably.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-daily-tasks.jpg" class="img-fluid" alt="AI Daily Tasks"> <em>Common tasks where AI provides immediate value</em></p>
</section>
<section id="meal-planning-and-recipes" class="level2">
<h2 class="anchored" data-anchor-id="meal-planning-and-recipes">1. Meal Planning and Recipes</h2>
<p>Turn ingredients into meal plans instantly.</p>
<p><strong>Basic ingredient prompt:</strong></p>
<pre><code>I have chicken breast, rice, and broccoli. 
Give me 3 different 30-minute meals with instructions.</code></pre>
<p><strong>Weekly planning prompt:</strong></p>
<pre><code>Create a 5-day meal plan for 2 people, $60 budget. 
Include grocery list organized by store section.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-meal-planning.jpg" class="img-fluid" alt="AI Meal Planning"> <em>AI generates recipes based on available ingredients and dietary preferences</em></p>
<p>AI handles dietary restrictions, cooking time constraints, and budget limitations effectively. The grocery lists are particularly useful since they organize items by store layout.</p>
</section>
<section id="email-and-message-writing" class="level2">
<h2 class="anchored" data-anchor-id="email-and-message-writing">2. Email and Message Writing</h2>
<p>Improve clarity and tone in professional communication.</p>
<p><strong>Email revision prompt:</strong></p>
<pre><code>Make this email more professional and concise:
[paste your draft]</code></pre>
<p><strong>Difficult conversation prompt:</strong></p>
<pre><code>Help me write a polite but firm email declining this request:
[explain situation]</code></pre>
<p><strong>Meeting follow-up prompt:</strong></p>
<pre><code>Draft a follow-up email summarizing these meeting points:
[list key decisions and action items]</code></pre>
<p>This works for text messages, LinkedIn messages, and any written communication where tone matters.</p>
</section>
<section id="quick-learning-and-research" class="level2">
<h2 class="anchored" data-anchor-id="quick-learning-and-research">3. Quick Learning and Research</h2>
<p>Get structured explanations on unfamiliar topics.</p>
<p><strong>Concept explanation prompt:</strong></p>
<pre><code>Explain [topic] in simple terms, then give me 
5 follow-up questions to test my understanding.</code></pre>
<p><strong>Skill learning prompt:</strong></p>
<pre><code>I need to learn Excel pivot tables for work. 
Give me a step-by-step tutorial focusing on the most practical features.</code></pre>
<p><strong>Research synthesis prompt:</strong></p>
<pre><code>Compare the top 3 options for [product/service] 
with pros and cons for each.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-learning-interface.jpg" class="img-fluid" alt="AI Learning Assistant"> <em>AI breaks down complex topics into manageable learning steps</em></p>
<p>AI excels at creating structured learning paths and synthesizing information from multiple angles.</p>
</section>
<section id="travel-and-event-planning" class="level2">
<h2 class="anchored" data-anchor-id="travel-and-event-planning">4. Travel and Event Planning</h2>
<p>Generate detailed itineraries and logistics.</p>
<p><strong>Trip planning prompt:</strong></p>
<pre><code>Plan a 3-day weekend trip to [destination] for [number] people 
with a $[amount] budget. Include transportation, accommodation, 
and daily activities.</code></pre>
<p><strong>Local exploration prompt:</strong></p>
<pre><code>What are 5 lesser-known attractions in [city] 
that locals recommend?</code></pre>
<p><strong>Event planning prompt:</strong></p>
<pre><code>Create a timeline and checklist for planning a 
[birthday party/dinner party/work event] for [number] people.</code></pre>
<p>AI handles multiple constraints simultaneously - budget, time, preferences, and logistics.</p>
</section>
<section id="home-management-and-finances" class="level2">
<h2 class="anchored" data-anchor-id="home-management-and-finances">5. Home Management and Finances</h2>
<p>Get practical advice for household and money management.</p>
<p><strong>Home maintenance prompt:</strong></p>
<pre><code>My [appliance/fixture] is [problem description]. 
Walk me through troubleshooting steps from simple to complex.</code></pre>
<p><strong>Budget planning prompt:</strong></p>
<pre><code>I make $[amount] monthly, fixed expenses are $[amount]. 
Create a realistic budget with savings and discretionary spending.</code></pre>
<p><strong>Purchase decision prompt:</strong></p>
<pre><code>I'm considering buying [item] for [purpose]. 
What factors should I evaluate? Create a decision framework.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-budget-planning.jpg" class="img-fluid" alt="AI Home Finance"> <em>AI provides structured approaches to financial and household decisions</em></p>
<p>These applications work because AI can process multiple variables and provide step-by-step guidance for complex decisions.</p>
</section>
<section id="implementation-notes" class="level2">
<h2 class="anchored" data-anchor-id="implementation-notes">Implementation Notes</h2>
<p><strong>Prompt specificity matters.</strong> Include relevant details like budget, timeframe, number of people, and specific constraints.</p>
<p><strong>Follow-up questions improve results.</strong> Ask for alternatives, simplifications, or additional detail as needed.</p>
<p><strong>Save effective prompts.</strong> Keep a note file with prompts that work well for repeated use.</p>
<p>These five applications cover the most common daily planning and decision-making tasks. Each provides immediate time savings and often produces better results than manual research.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>productivity</category>
  <category>tutorial</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/001.html</guid>
  <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-daily-tasks.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How AI Systems Work: A Technical Overview</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/000.html</link>
  <description><![CDATA[ 





<section id="understanding-modern-ai-systems" class="level2">
<h2 class="anchored" data-anchor-id="understanding-modern-ai-systems">Understanding Modern AI Systems</h2>
<p>Artificial intelligence systems like ChatGPT and Claude can engage in conversations, solve problems, and generate content. However, their underlying mechanisms are fundamentally different from human cognition.</p>
<p>These systems operate as sophisticated pattern recognition and prediction engines. While they can produce human-like responses, they function by identifying statistical patterns in data rather than developing conceptual understanding.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" class="img-fluid" alt="Neural Network vs Brain"> <em>AI neural networks are inspired by brain structure but operate through different mechanisms</em></p>
<blockquote class="blockquote">
<p><strong>Key Question:</strong> How do statistical pattern-matching systems produce seemingly intelligent behavior?</p>
</blockquote>
</section>
<section id="core-architecture-prediction-systems" class="level2">
<h2 class="anchored" data-anchor-id="core-architecture-prediction-systems">Core Architecture: Prediction Systems</h2>
<p>Modern AI systems solve a fundamental prediction problem: given an input, determine the most statistically likely output. This applies whether predicting the next word in text, pixels in an image, or moves in a game.</p>
<section id="transformer-architecture" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h3>
<p>Current large language models like GPT-4 and Claude use transformer architecture:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified transformer process</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> transformer_prediction(input_text):</span>
<span id="cb1-3">    tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize(input_text)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert text to processable units</span></span>
<span id="cb1-4">    embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_to_vectors(tokens)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Map to numerical representations</span></span>
<span id="cb1-5">    </span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> neural_layers:</span>
<span id="cb1-7">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention_mechanism(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identify relationships</span></span>
<span id="cb1-8">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Process patterns</span></span>
<span id="cb1-9">    </span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predict_next_token(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Generate most likely continuation</span></span></code></pre></div></div>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/transformer-architecture.jpg" class="img-fluid" alt="Transformer Architecture"> <em>Transformer architecture with attention and processing layers</em></p>
<p>Modern systems operate at significant scale. GPT-3 contains 175 billion parameters trained on 45TB of text data. GPT-4 is estimated to exceed 1 trillion parameters with correspondingly higher computational requirements.</p>
</section>
<section id="attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanisms">Attention Mechanisms</h3>
<p>The attention mechanism enables models to focus selectively on different input elements when making predictions.</p>
<p><strong>Example:</strong> “The cat sat on the mat because it was comfortable.”</p>
<p>When determining what “it” refers to, the attention mechanism assigns weights: - “cat” (high attention - 0.7) - “mat” (medium attention - 0.2) - “sat” (low attention - 0.1)</p>
<p>This mechanism allows processing of relationships between distant elements in sequences.</p>
</section>
</section>
<section id="training-process" class="level2">
<h2 class="anchored" data-anchor-id="training-process">Training Process</h2>
<p>AI system development occurs through structured training phases that determine capabilities and limitations.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-training-process.jpg" class="img-fluid" alt="AI Training Process"> <em>Complete pipeline from data collection to deployed system</em></p>
<section id="phase-1-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-pre-training">Phase 1: Pre-training</h3>
<p>Systems learn from large text datasets through next-token prediction:</p>
<ol type="1">
<li><strong>Data Collection</strong>: Process text from websites, books, and articles</li>
<li><strong>Tokenization</strong>: Convert text to discrete units for processing</li>
<li><strong>Prediction Training</strong>: Train models to predict missing tokens in sequences</li>
<li><strong>Parameter Adjustment</strong>: Iteratively modify billions of parameters based on prediction accuracy</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified training loop</span></span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(many_epochs):</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> text_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> massive_dataset:</span>
<span id="cb2-4">        input_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Input sequence</span></span>
<span id="cb2-5">        target_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Target prediction</span></span>
<span id="cb2-6">        </span>
<span id="cb2-7">        prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(input_tokens)</span>
<span id="cb2-8">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calculate_error(prediction, target_token)</span>
<span id="cb2-9">        </span>
<span id="cb2-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Update model parameters</span></span>
<span id="cb2-11">        model.update_weights(loss.gradient())</span></code></pre></div></div>
</section>
<section id="phase-2-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-fine-tuning">Phase 2: Fine-tuning</h3>
<p>Pre-trained models are refined for specific applications:</p>
<ul>
<li><strong>Supervised Fine-tuning</strong>: Training on curated question-answer pairs</li>
<li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: Using human preferences to shape responses</li>
<li><strong>Safety Training</strong>: Implementing guidelines to avoid harmful outputs</li>
</ul>
<p>This process transforms basic text predictors into conversational assistants.</p>
</section>
</section>
<section id="emergent-capabilities" class="level2">
<h2 class="anchored" data-anchor-id="emergent-capabilities">Emergent Capabilities</h2>
<p>Large-scale training produces capabilities that weren’t explicitly programmed. These emergent abilities arise from complex interactions between simple components.</p>
<section id="examples-of-emergence" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-emergence">Examples of Emergence</h3>
<p>As model size increases, new capabilities develop:</p>
<p><strong>Chain-of-Thought Reasoning</strong>: Models develop step-by-step problem-solving approaches without explicit training on this strategy.</p>
<p><strong>In-Context Learning</strong>: Systems can perform new tasks based on examples in prompts without additional training.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example of in-context learning</span></span>
<span id="cb3-2">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Translate English to French:</span></span>
<span id="cb3-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Hello → Bonjour</span></span>
<span id="cb3-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Goodbye → Au revoir</span></span>
<span id="cb3-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Thank you → Merci</span></span>
<span id="cb3-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Good morning → </span></span>
<span id="cb3-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Model produces "Bonjour" despite no explicit</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># training on this translation task</span></span></code></pre></div></div>
<p><strong>Cross-Lingual Transfer</strong>: Models trained primarily on English can operate effectively in other languages.</p>
<p>The mechanisms underlying emergence remain an active research area, with theories including phase transitions, increased representational capacity, and pattern recognition thresholds.</p>
</section>
</section>
<section id="system-limitations" class="level2">
<h2 class="anchored" data-anchor-id="system-limitations">System Limitations</h2>
<p>Current AI systems have several fundamental constraints that stem from their statistical architecture:</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-limitations-chart.jpg" class="img-fluid" alt="AI Limitations Chart"> <em>Overview of AI system capabilities and constraints</em></p>
<section id="pattern-matching-vs.-understanding" class="level3">
<h3 class="anchored" data-anchor-id="pattern-matching-vs.-understanding">Pattern Matching vs.&nbsp;Understanding</h3>
<p>AI systems process statistical relationships in symbols rather than developing conceptual understanding. They can accurately reproduce information without grasping underlying mechanisms.</p>
<p><strong>Example</strong>: A system can state that “water boils at 100°C” without understanding molecular behavior, phase transitions, or the physical processes involved.</p>
</section>
<section id="output-generation-under-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="output-generation-under-uncertainty">Output Generation Under Uncertainty</h3>
<p>Systems are designed to produce responses even when lacking relevant information, leading to plausible but potentially inaccurate outputs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified response generation</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_response(query):</span>
<span id="cb4-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> confidence_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> threshold:</span>
<span id="cb4-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> retrieve_known_information(query)</span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-6">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># System still generates output</span></span>
<span id="cb4-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> generate_plausible_response(query)</span></code></pre></div></div>
</section>
<section id="training-data-constraints" class="level3">
<h3 class="anchored" data-anchor-id="training-data-constraints">Training Data Constraints</h3>
<p>Models are limited by their training data cutoff and cannot incorporate new information without retraining.</p>
</section>
<section id="memory-limitations" class="level3">
<h3 class="anchored" data-anchor-id="memory-limitations">Memory Limitations</h3>
<p>Current systems can process limited context windows (typically 4,000-128,000 tokens) with no persistent memory between conversations.</p>
</section>
<section id="correlation-vs.-causation" class="level3">
<h3 class="anchored" data-anchor-id="correlation-vs.-causation">Correlation vs.&nbsp;Causation</h3>
<p>Systems excel at identifying statistical correlations but have limited understanding of causal relationships. They may recognize that umbrella sales correlate with rain without understanding the causal mechanism.</p>
</section>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical Implications</h2>
<p>Understanding AI architecture has important implications for effective use:</p>
<section id="system-strengths" class="level3">
<h3 class="anchored" data-anchor-id="system-strengths">System Strengths</h3>
<ul>
<li>Pattern recognition and text generation capabilities</li>
<li>Rapid processing of large information sets</li>
<li>Accessible interface for complex tasks</li>
<li>Consistent performance within training domains</li>
</ul>
</section>
<section id="appropriate-applications" class="level3">
<h3 class="anchored" data-anchor-id="appropriate-applications">Appropriate Applications</h3>
<ul>
<li>Content drafting and editing</li>
<li>Information synthesis from known sources</li>
<li>Code generation and debugging assistance</li>
<li>Language translation</li>
<li>Creative brainstorming</li>
</ul>
</section>
<section id="best-practices" class="level3">
<h3 class="anchored" data-anchor-id="best-practices">Best Practices</h3>
<ol type="1">
<li><strong>Verify Important Information</strong>: Fact-check outputs for critical applications</li>
<li><strong>Understand Limitations</strong>: Recognize domain constraints and knowledge cutoffs</li>
<li><strong>Maintain Human Oversight</strong>: Keep humans involved in important decisions</li>
<li><strong>Recognize Bias</strong>: Be aware that systems reflect training data biases</li>
<li><strong>Use as Tools</strong>: Apply systems as analytical instruments rather than authoritative sources</li>
</ol>
</section>
</section>
<section id="future-development" class="level2">
<h2 class="anchored" data-anchor-id="future-development">Future Development</h2>
<p>Current AI systems represent early implementations of statistical learning approaches. Several research directions may expand capabilities:</p>
<section id="near-term-developments-1-3-years" class="level3">
<h3 class="anchored" data-anchor-id="near-term-developments-1-3-years">Near-Term Developments (1-3 years)</h3>
<ul>
<li>Multimodal systems integrating text, images, audio, and video</li>
<li>Extended context windows for longer conversations and documents</li>
<li>Improved reasoning and problem-solving capabilities</li>
<li>Specialized models optimized for specific domains</li>
</ul>
</section>
<section id="long-term-research-areas-10-years" class="level3">
<h3 class="anchored" data-anchor-id="long-term-research-areas-10-years">Long-Term Research Areas (10+ years)</h3>
<ul>
<li>Artificial General Intelligence research</li>
<li>Questions about machine consciousness and subjective experience</li>
<li>AI alignment and safety considerations</li>
</ul>
</section>
<section id="active-research-questions" class="level3">
<h3 class="anchored" data-anchor-id="active-research-questions">Active Research Questions</h3>
<p>Researchers are investigating fundamental questions:</p>
<ul>
<li><strong>Scaling Laws</strong>: Relationships between model size, data, and capabilities</li>
<li><strong>Emergence Mechanisms</strong>: Why new abilities appear at certain scales</li>
<li><strong>Alignment</strong>: Ensuring AI systems operate according to intended objectives</li>
<li><strong>Interpretability</strong>: Understanding internal model representations and processes</li>
</ul>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Modern AI systems represent sophisticated statistical engines that process patterns in data to generate human-like outputs. While they demonstrate impressive capabilities, they operate through pattern matching rather than conceptual understanding.</p>
<section id="key-technical-points" class="level3">
<h3 class="anchored" data-anchor-id="key-technical-points">Key Technical Points</h3>
<p>Current AI systems achieve intelligent-seeming behavior through statistical pattern processing rather than understanding. This approach has several implications:</p>
<ul>
<li><strong>For Users</strong>: Understanding system strengths and limitations enables more effective application</li>
<li><strong>For Organizations</strong>: Appropriate use requires human oversight and verification processes</li>
<li><strong>For Society</strong>: New frameworks are needed for evaluating machine capabilities and limitations</li>
</ul>
<p>Statistical pattern matching at scale produces sophisticated behavior, suggesting that aspects of intelligence may be more computational than previously understood.</p>
<p>As these systems become more prevalent, understanding their statistical nature becomes increasingly important for effective implementation. The technology continues evolving rapidly, with ongoing research addressing questions about scaling, emergence, and alignment with human objectives.</p>
<p>The field represents significant progress in automated pattern recognition and generation, with continued development likely to expand capabilities while maintaining the fundamental statistical architecture.</p>
<hr>
<p><strong>Understanding the technical foundations of AI systems helps inform appropriate use and realistic expectations about current capabilities.</strong></p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>deep-dive</category>
  <category>technology</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/000.html</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
