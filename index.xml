<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>What Can You Do With AI?</title>
<link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/</link>
<atom:link href="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Let&#39;s Find Out</description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Wed, 15 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>AI Meditation Apps: Is the Personalization Real?</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/016_/016.html</link>
  <description><![CDATA[ 





<section id="stress-testing-ai-meditation" class="level2">
<h2 class="anchored" data-anchor-id="stress-testing-ai-meditation">Stress Testing AI Meditation</h2>
<p>School and recruiting have been picking up intensity—midterms, case prep, networking events, finance interviews. I’ve been consistently stressed and not sleeping well. A friend suggested trying Calm, the meditation app that claims to use AI for personalized relaxation.</p>
<p>I’m skeptical of wellness apps in general, but I figured this was a good test case: if AI personalization works anywhere, it should work when I’m actually stressed and need it. So I signed up for a week to see if the “AI-powered personalization” is real or just marketing.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/016_/calm-app-interface.jpg" alt="Calm App Interface" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Calm’s interface showing meditation recommendations</em></p>
</section>
<section id="what-these-apps-claim-to-personalize" class="level2">
<h2 class="anchored" data-anchor-id="what-these-apps-claim-to-personalize">What These Apps Claim to Personalize</h2>
<section id="the-ai-features" class="level3">
<h3 class="anchored" data-anchor-id="the-ai-features">The AI Features</h3>
<p>According to the app and marketing materials: - <strong>Personalized daily recommendations</strong> based on your stress patterns - <strong>Adaptive session length</strong> that adjusts to your schedule - <strong>Content matching</strong> that learns which meditation styles work best for you - <strong>Check-in analysis</strong> that tracks your emotional state over time - <strong>Smart reminders</strong> that send notifications when you’re most likely to need meditation</p>
</section>
<section id="the-setup-process" class="level3">
<h3 class="anchored" data-anchor-id="the-setup-process">The Setup Process</h3>
<p>When I first opened the app, it asked: - Why I’m using Calm (options: stress, sleep, focus, anxiety, etc.) - My experience level with meditation (beginner, intermediate, experienced) - What time of day I prefer to meditate - Whether I want daily reminders</p>
<p>Then it presented a “personalized” home screen with recommended meditations.</p>
<p>Right away, I’m wondering: is this AI learning, or just a decision tree based on my initial answers?</p>
</section>
</section>
<section id="my-week-of-testing" class="level2">
<h2 class="anchored" data-anchor-id="my-week-of-testing">My Week of Testing</h2>
<section id="day-1-2-initial-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="day-1-2-initial-recommendations">Day 1-2: Initial Recommendations</h3>
<p>The app recommended: - “Managing Stress” - 10 minute guided meditation - “Calming Anxiety” - 15 minute session - “Sleep Stories” for nighttime</p>
<p>These matched what I selected during setup (stress management). But they’re also probably what everyone who selects “stress” gets recommended. Nothing personalized yet.</p>
<p>I tried the 10-minute stress management session. It was fine—standard guided meditation with breathing exercises and body scan. At the end, it asked “How do you feel?” with options: Calm, Relaxed, Same, Restless.</p>
<p>I selected “Relaxed.” Let’s see if the app learns from this.</p>
</section>
<section id="day-3-4-different-stress-levels" class="level3">
<h3 class="anchored" data-anchor-id="day-3-4-different-stress-levels">Day 3-4: Different Stress Levels</h3>
<p>On Day 3, I had two interviews back-to-back. Before the first one, I did a quick 5-minute “Focus” meditation the app recommended.</p>
<p>After interviews, I was exhausted and stressed. Opened the app again that evening. It recommended… the same 10-minute stress management session from Day 1.</p>
<p>Wait—shouldn’t it notice I used the app twice in one day, during high-stress moments, and adjust its recommendations? Maybe suggest something longer or more intensive?</p>
<p>Instead, the recommendations looked identical to Day 1.</p>
</section>
<section id="day-5-7-checking-for-patterns" class="level3">
<h3 class="anchored" data-anchor-id="day-5-7-checking-for-patterns">Day 5-7: Checking for Patterns</h3>
<p>I deliberately tried different meditation types: - Morning: 7-minute “Daily Calm” meditation - Afternoon: 10-minute breathing exercise - Night: 20-minute sleep story</p>
<p>Each time, I marked how I felt afterward. Sometimes “Calm,” sometimes “Same,” once “Restless” (I was too stressed to focus).</p>
<p>By Day 7, my recommendations were… still basically the same as Day 1. A few new options appeared in the “Recommended for You” section, but they seemed randomly rotated rather than based on what I’d actually used or found helpful.</p>
</section>
</section>
<section id="is-this-actually-ai-personalization" class="level2">
<h2 class="anchored" data-anchor-id="is-this-actually-ai-personalization">Is This Actually AI Personalization?</h2>
<section id="what-would-real-personalization-look-like" class="level3">
<h3 class="anchored" data-anchor-id="what-would-real-personalization-look-like">What Would Real Personalization Look Like?</h3>
<p>If Calm were truly using AI to personalize my experience, I’d expect:</p>
<p><strong>Usage pattern recognition:</strong> Notice that I meditate more frequently during high-stress periods (like Day 3 with interviews) and suggest more intensive practices during those times.</p>
<p><strong>Effectiveness tracking:</strong> If I consistently rate 7-minute sessions as “Relaxing” but 20-minute sessions as “Restless” (because I can’t focus that long when stressed), it should recommend shorter sessions.</p>
<p><strong>Time-of-day optimization:</strong> Learn that I actually use the app most in the evening, not morning, despite saying “morning” in my initial setup.</p>
<p><strong>Content adaptation:</strong> If I skip certain meditation styles repeatedly, stop recommending them.</p>
</section>
<section id="what-i-actually-observed" class="level3">
<h3 class="anchored" data-anchor-id="what-i-actually-observed">What I Actually Observed</h3>
<p>The recommendations seemed based on: - My initial survey answers (static profile) - General content rotation (everyone sees different things each day) - Basic category matching (I said “stress,” so I get stress-related content)</p>
<p>This isn’t machine learning—it’s a preference quiz with some randomization.</p>
</section>
<section id="the-check-in-feature" class="level3">
<h3 class="anchored" data-anchor-id="the-check-in-feature">The “Check-In” Feature</h3>
<p>Calm asks “How do you feel?” after each session. This data could theoretically train a model to learn what works for me.</p>
<p>But after a week of providing this feedback, I saw no evidence it changed anything. The app didn’t: - Suggest more of what I rated highly - Avoid what I rated poorly - Adjust session length based on my responses - Change reminder timing based on when I actually use the app</p>
<p>According to <a href="https://www.sciencedirect.com/science/article/pii/S2949916X24000525">research on adaptive meditation apps</a>, truly personalized digital interventions should show measurable adaptation over time. I didn’t see this.</p>
</section>
</section>
<section id="ai-meditation-vs.-regular-guided-meditation" class="level2">
<h2 class="anchored" data-anchor-id="ai-meditation-vs.-regular-guided-meditation">AI Meditation vs.&nbsp;Regular Guided Meditation</h2>
<section id="what-makes-meditation-effective" class="level3">
<h3 class="anchored" data-anchor-id="what-makes-meditation-effective">What Makes Meditation Effective</h3>
<p>Research on meditation effectiveness shows it depends on: - Consistency of practice - Finding techniques that match your needs - Appropriate session length for your focus capacity - Quality of instruction</p>
<p>None of these inherently require AI. A good meditation teacher, a simple YouTube video, or even free apps without “AI” can provide these.</p>
</section>
<section id="does-ai-add-value" class="level3">
<h3 class="anchored" data-anchor-id="does-ai-add-value">Does AI Add Value?</h3>
<p>In theory, AI personalization could help by: - Learning which techniques work for your specific stress patterns - Adapting difficulty as you build meditation capacity - Identifying optimal times for practice based on your schedule</p>
<p>But Calm doesn’t seem to do this. The “AI” label appears to be marketing rather than meaningful personalization.</p>
</section>
<section id="the-free-alternative" class="level3">
<h3 class="anchored" data-anchor-id="the-free-alternative">The Free Alternative</h3>
<p>I compared Calm to free guided meditations on YouTube and Spotify. Many are: - Same quality instruction - Same variety of styles and lengths - No subscription fee - Equally effective</p>
<p>The main advantage of these apps is convenience (everything in one app) and production quality (nice UI, good voice actors). But these aren’t AI features—they’re just app design.</p>
<p>You could manually build a YouTube playlist of meditations that work for you, and that “personalization” would be more effective than Calm’s alleged AI.</p>
</section>
</section>
<section id="the-ai-ification-of-wellness-apps" class="level2">
<h2 class="anchored" data-anchor-id="the-ai-ification-of-wellness-apps">The AI-ification of Wellness Apps</h2>
<section id="why-everything-is-ai-powered-now" class="level3">
<h3 class="anchored" data-anchor-id="why-everything-is-ai-powered-now">Why Everything Is “AI-Powered” Now</h3>
<p>Calm isn’t alone. Nearly every wellness app now claims AI: - “AI-powered” workout plans - “AI-driven” nutrition coaching - “AI-enhanced” sleep tracking - “AI-personalized” meditation</p>
<p>The pattern is clear: adding “AI” to your app: - Justifies higher subscription prices (Calm is $70/year) - Makes the app sound more sophisticated - Appeals to tech-forward users - Differentiates from free alternatives</p>
</section>
<section id="whats-probably-happening" class="level3">
<h3 class="anchored" data-anchor-id="whats-probably-happening">What’s Probably Happening</h3>
<p>Most “AI personalization” in wellness apps is likely: - Simple if-then logic based on user inputs - Random content rotation to create variety - Basic categorization (stress → stress content) - A/B testing to optimize engagement (not personalization)</p>
<p>These are useful features, but they’re not AI in any meaningful sense.</p>
</section>
<section id="the-premium-price-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-premium-price-problem">The Premium Price Problem</h3>
<p>Calm charges $70/year partly because of its AI personalization promise. But if the personalization isn’t real, you’re paying a premium for: - A nice UI - Good production quality - Convenient access to meditation content</p>
<p>These are fine features, but they don’t require AI or justify the AI marketing.</p>
</section>
</section>
<section id="did-it-actually-help-with-stress" class="level2">
<h2 class="anchored" data-anchor-id="did-it-actually-help-with-stress">Did It Actually Help With Stress?</h2>
<section id="the-honest-assessment" class="level3">
<h3 class="anchored" data-anchor-id="the-honest-assessment">The Honest Assessment</h3>
<p>Yes, using Calm helped me feel less stressed during recruiting season. But I’m pretty sure this had nothing to do with AI:</p>
<p><strong>Meditation works:</strong> Taking 10 minutes to breathe and focus helps reduce stress. This is true whether you use a paid app, YouTube, or just sit quietly.</p>
<p><strong>Guided instruction helps:</strong> Having someone guide you through meditation is easier than doing it alone, especially as a beginner. But good instruction doesn’t require AI.</p>
<p><strong>Convenience matters:</strong> Having all meditations in one app makes it more likely I’ll actually do it. But this is about app design, not AI.</p>
<p><strong>Reminder system:</strong> The daily notifications helped build a habit. Again, not AI—just push notifications.</p>
</section>
<section id="would-free-alternatives-work-as-well" class="level3">
<h3 class="anchored" data-anchor-id="would-free-alternatives-work-as-well">Would Free Alternatives Work as Well?</h3>
<p>Probably yes. The benefit came from: 1. Deciding to prioritize meditation during a stressful period 2. Having accessible guided content 3. Following through consistently</p>
<p>None of this required Calm specifically or its “AI” features.</p>
</section>
<section id="the-placebo-question" class="level3">
<h3 class="anchored" data-anchor-id="the-placebo-question">The Placebo Question</h3>
<p>There’s a possibility that believing the app is “personalized” makes it more effective through placebo effect. If you think the AI is customizing meditations for your specific needs, you might engage more seriously.</p>
<p>But this raises an ethical question: should companies charge premium prices for placebo effects based on false AI claims?</p>
</section>
</section>
<section id="should-you-use-ai-meditation-apps" class="level2">
<h2 class="anchored" data-anchor-id="should-you-use-ai-meditation-apps">Should You Use AI Meditation Apps?</h2>
<section id="use-them-if" class="level3">
<h3 class="anchored" data-anchor-id="use-them-if">Use Them If:</h3>
<ul>
<li>You want convenient access to guided meditation</li>
<li>You find the UI and production quality motivating</li>
<li>You’re willing to pay for convenience</li>
<li>You need variety in meditation styles</li>
</ul>
<p>But don’t pay extra for “AI personalization” that probably isn’t real.</p>
</section>
<section id="skip-them-if" class="level3">
<h3 class="anchored" data-anchor-id="skip-them-if">Skip Them If:</h3>
<ul>
<li>You’re comfortable finding free meditations on YouTube/Spotify</li>
<li>You don’t need an app to motivate practice</li>
<li>You’re skeptical of premium subscription prices</li>
</ul>
</section>
<section id="better-approaches" class="level3">
<h3 class="anchored" data-anchor-id="better-approaches">Better Approaches:</h3>
<p><strong>Try free options first:</strong> Use YouTube or free apps (Insight Timer, UCLA Mindful) to see if guided meditation helps you.</p>
<p><strong>Manual personalization:</strong> Pay attention to which meditation styles, lengths, and times of day work best for you. This is better personalization than any AI.</p>
<p><strong>Focus on consistency over features:</strong> The best meditation app is the one you’ll actually use. Fancy AI features don’t matter if you don’t build a habit.</p>
<p><strong>Question the AI label:</strong> When any wellness app claims AI, ask: “What is the AI actually doing that couldn’t be done with basic programming?”</p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Calm is a well-designed meditation app with good content. The guided meditations helped me manage stress during a busy recruiting and school period.</p>
<p>But the “AI personalization” appears to be marketing rather than meaningful technology. After a week of use and deliberate testing, I saw no evidence that the app was learning my preferences or adapting recommendations based on my behavior.</p>
<p>The recommendations seemed to come from: - My initial survey answers (static profile) - Content rotation (showing variety) - Basic category matching</p>
<p>This isn’t AI—it’s a preference quiz with some randomization.</p>
<p>For people who find Calm helpful and are willing to pay, that’s fine. The app has value. But it’s worth being clear about what you’re paying for: convenient access to quality meditation content, not AI-powered personalization.</p>
<p>As wellness apps increasingly add “AI” to their marketing, we should be skeptical. Real AI personalization would show measurable adaptation over time. Most apps, including Calm, don’t demonstrate this.</p>
<p>The meditation helped with my recruiting stress. The AI claims? Those just stressed me out more.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>health</category>
  <category>fitness</category>
  <category>skeptical</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/016_/016.html</guid>
  <pubDate>Wed, 15 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/016_/meditation-app-interface.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using Claude to Synthesize Dense Class Notes</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/015_/015.html</link>
  <description><![CDATA[ 





<section id="drowning-in-dense-class-notes" class="level2">
<h2 class="anchored" data-anchor-id="drowning-in-dense-class-notes">Drowning in Dense Class Notes</h2>
<p>I’m taking Human Disease and Evolution this semester, and it’s incredibly note-intensive. Every lecture covers evolutionary biology, epidemiology, pathogen adaptation, and historical case studies—all dense material that builds on previous content. By midterms, I had hundreds of pages of notes with no clear way to study them efficiently.</p>
<p>I decided to test whether Claude could help synthesize these notes and work as a study partner. Not just summarizing, but actually helping me understand connections and prepare for exams.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/015_/disease-evolution-notes.jpg" alt="Disease Evolution Course Notes" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Dense notes from Human Disease and Evolution lectures</em></p>
</section>
<section id="how-im-using-claude" class="level2">
<h2 class="anchored" data-anchor-id="how-im-using-claude">How I’m Using Claude</h2>
<section id="initial-upload-and-organization" class="level3">
<h3 class="anchored" data-anchor-id="initial-upload-and-organization">Initial Upload and Organization</h3>
<p>I created a Claude project specifically for the course and uploaded: - Lecture notes from the first 8 weeks - Reading summaries - Professor’s study guide questions - Previous exam questions (from the syllabus)</p>
<p>Then I asked:</p>
<blockquote class="blockquote">
<p>“I’ve uploaded notes from my Human Disease and Evolution course. Help me organize the main themes across lectures and identify connections between topics.”</p>
</blockquote>
<p>Claude produced a thematic breakdown: - Evolutionary pressures on pathogens - Host-pathogen coevolution - Historical transitions (hunter-gatherer → agricultural → urban) and disease emergence - Trade-offs in immune response - Case studies (malaria, tuberculosis, influenza)</p>
<p>This was immediately useful. My notes were chronologically organized by lecture, but the exam tests conceptual understanding across topics.</p>
</section>
<section id="synthesizing-across-lectures" class="level3">
<h3 class="anchored" data-anchor-id="synthesizing-across-lectures">Synthesizing Across Lectures</h3>
<p>My next prompt:</p>
<blockquote class="blockquote">
<p>“We covered malaria in weeks 2, 4, and 7 from different angles. Synthesize all the malaria content into one coherent explanation covering: evolutionary biology, geographic distribution, historical impact, and current challenges.”</p>
</blockquote>
<p>Claude pulled information from multiple lecture sets and created a unified overview. This saved me from manually cross-referencing my notes across different weeks.</p>
</section>
<section id="generating-practice-questions" class="level3">
<h3 class="anchored" data-anchor-id="generating-practice-questions">Generating Practice Questions</h3>
<p>For active recall practice:</p>
<blockquote class="blockquote">
<p>“Based on the uploaded notes, generate 10 exam-style questions that test conceptual understanding, not just memorization. Include questions that require applying concepts to new scenarios.”</p>
</blockquote>
<p>The questions were decent: - “How would you expect pathogen virulence to evolve in a population with high vs.&nbsp;low host density?” - “Explain why tuberculosis resurged in urban populations during industrialization using evolutionary principles.”</p>
<p>These forced me to think beyond memorizing facts.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/015_/pathogen-evolution-diagram.jpg" alt="Pathogen Evolution" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Evolutionary dynamics between hosts and pathogens</em></p>
</section>
</section>
<section id="what-actually-helped" class="level2">
<h2 class="anchored" data-anchor-id="what-actually-helped">What Actually Helped</h2>
<section id="finding-connections-i-missed" class="level3">
<h3 class="anchored" data-anchor-id="finding-connections-i-missed">Finding Connections I Missed</h3>
<p>Claude identified patterns across lectures that I hadn’t noticed while taking notes in real-time. For example, the concept of “virulence-transmission trade-offs” appeared in multiple contexts (influenza, cholera, malaria), but I’d written it down separately each time without seeing the unifying principle.</p>
</section>
<section id="clarifying-confusing-concepts" class="level3">
<h3 class="anchored" data-anchor-id="clarifying-confusing-concepts">Clarifying Confusing Concepts</h3>
<p>When I asked:</p>
<blockquote class="blockquote">
<p>“I don’t fully understand the relationship between R0 (basic reproduction number) and herd immunity threshold. Explain using the influenza example from my notes.”</p>
</blockquote>
<p>Claude broke it down using the specific examples from my lectures, not generic textbook explanations. This was more helpful than Googling because it referenced the actual case studies we’d covered.</p>
</section>
<section id="creating-study-guides" class="level3">
<h3 class="anchored" data-anchor-id="creating-study-guides">Creating Study Guides</h3>
<p>Instead of re-reading hundreds of pages, I asked:</p>
<blockquote class="blockquote">
<p>“Create a two-page study guide covering the five most important concepts from weeks 1-8, with specific examples from the notes.”</p>
</blockquote>
<p>The condensed version helped me review quickly before office hours and identify gaps in my understanding.</p>
</section>
</section>
<section id="significant-limitations" class="level2">
<h2 class="anchored" data-anchor-id="significant-limitations">Significant Limitations</h2>
<section id="cant-replace-actually-studying" class="level3">
<h3 class="anchored" data-anchor-id="cant-replace-actually-studying">Can’t Replace Actually Studying</h3>
<p>The biggest misconception would be thinking Claude can study for you. It can’t.</p>
<p>When I tried asking Claude to quiz me on material, I noticed I could “cheat” by asking for hints or looking back at its previous responses. The lack of real accountability meant I wasn’t forcing myself to actually retrieve information from memory.</p>
<p>According to <a href="https://www.cmu.edu/news/stories/archives/2021/october/active-learning.html">research on active learning</a>, the cognitive struggle of retrieval is what builds long-term retention. AI tools that make information too easy to access can undermine this.</p>
</section>
<section id="factual-accuracy-issues" class="level3">
<h3 class="anchored" data-anchor-id="factual-accuracy-issues">Factual Accuracy Issues</h3>
<p>Claude occasionally made small errors when synthesizing across notes: - Mixing up dates for historical disease outbreaks - Slightly mischaracterizing which regions certain diseases are endemic to - Conflating similar but distinct evolutionary concepts</p>
<p>These weren’t obvious hallucinations—they were plausible-sounding errors that I only caught because I’d attended the lectures. If I’d been using Claude to learn new material rather than organize existing notes, I might not have noticed.</p>
</section>
<section id="cant-capture-lecture-nuance" class="level3">
<h3 class="anchored" data-anchor-id="cant-capture-lecture-nuance">Can’t Capture Lecture Nuance</h3>
<p>My professor emphasizes certain concepts or says “this will be on the exam.” Those verbal cues don’t make it into my typed notes, so Claude can’t prioritize what’s actually important vs.&nbsp;what’s just background information.</p>
<p>It treats all information in my notes equally, when some concepts are more central than others.</p>
</section>
<section id="dependence-risk" class="level3">
<h3 class="anchored" data-anchor-id="dependence-risk">Dependence Risk</h3>
<p>There’s a real risk of becoming dependent on AI for synthesis and losing the ability to do it myself. The mental work of organizing information and finding connections is part of learning—offloading that entirely to AI might make me better at using AI but worse at actual thinking.</p>
<p><a href="https://www.mdpi.com/2075-4698/15/1/6">Research on cognitive offloading to technology</a> suggests that over-reliance on external tools can reduce our own cognitive abilities over time.</p>
</section>
</section>
<section id="is-claude-actually-a-good-study-partner" class="level2">
<h2 class="anchored" data-anchor-id="is-claude-actually-a-good-study-partner">Is Claude Actually a Good Study Partner?</h2>
<section id="what-makes-a-good-study-partner" class="level3">
<h3 class="anchored" data-anchor-id="what-makes-a-good-study-partner">What Makes a Good Study Partner</h3>
<p>Real study partners: - Challenge your understanding by asking probing questions - Notice when you’re bullshitting and call you out - Have their own understanding that you can compare against - Hold you accountable for actually studying</p>
<p>Claude does some of this (asking questions, providing alternative explanations) but fails at others (accountability, challenging weak understanding).</p>
</section>
<section id="where-it-actually-works" class="level3">
<h3 class="anchored" data-anchor-id="where-it-actually-works">Where It Actually Works</h3>
<p>Claude is better described as a “note organization assistant” than a “study partner.” It’s useful for: - Synthesizing large amounts of text quickly - Finding patterns across multiple documents - Generating initial practice questions - Creating structured study guides</p>
<p>These are valuable, but they’re the preliminary work before actual studying begins.</p>
</section>
<section id="the-accountability-gap" class="level3">
<h3 class="anchored" data-anchor-id="the-accountability-gap">The Accountability Gap</h3>
<p>The biggest difference between Claude and a human study partner: I can’t fool Claude, but I also can’t fool myself into thinking Claude-assisted studying counts as real studying.</p>
<p>When I study with friends, the social pressure and explanation requirement force actual learning. With Claude, I can go through the motions of “studying” while not really engaging deeply with the material.</p>
</section>
</section>
<section id="how-im-actually-using-it" class="level2">
<h2 class="anchored" data-anchor-id="how-im-actually-using-it">How I’m Actually Using It</h2>
<section id="early-stage-organization-only" class="level3">
<h3 class="anchored" data-anchor-id="early-stage-organization-only">Early-Stage Organization Only</h3>
<p>I use Claude at the beginning of my study process: 1. Upload notes after several lectures 2. Get thematic organization and connections 3. Create initial study guide 4. Generate practice questions</p>
<p>Then I study using traditional methods: spaced repetition flashcards, practice problems without AI help, and study groups with actual humans.</p>
</section>
<section id="verification-against-lecture-materials" class="level3">
<h3 class="anchored" data-anchor-id="verification-against-lecture-materials">Verification Against Lecture Materials</h3>
<p>Anything Claude synthesizes, I verify against: - Original lecture slides - Textbook readings - Professor’s posted materials</p>
<p>I don’t trust Claude’s synthesis as the final word—it’s a starting point that I refine.</p>
</section>
<section id="using-it-for-weak-areas" class="level3">
<h3 class="anchored" data-anchor-id="using-it-for-weak-areas">Using It for Weak Areas</h3>
<p>When I identify concepts I don’t understand well, I ask Claude for additional explanations or examples. But then I test my understanding by explaining it to a friend without AI help.</p>
</section>
<section id="real-study-sessions-stay-human" class="level3">
<h3 class="anchored" data-anchor-id="real-study-sessions-stay-human">Real Study Sessions Stay Human</h3>
<p>My actual exam prep involves: - Study groups with classmates - Office hours with TA and professor - Practice exams under time pressure - Teaching concepts to others</p>
<p>Claude handles the logistical grunt work of organizing notes, but learning happens through human interaction and cognitive struggle.</p>
</section>
</section>
<section id="final-assessment" class="level2">
<h2 class="anchored" data-anchor-id="final-assessment">Final Assessment</h2>
<p>Using Claude to synthesize dense class notes is genuinely useful for note-intensive courses. It saves time on organization and helps identify connections across material.</p>
<p>But calling it a “study partner” overstates what it can do. It’s more like a smart organizational tool that can handle the tedious work of cross-referencing and initial synthesis.</p>
<p>The real learning—deep understanding, retention, application—still requires traditional study methods and human interaction. Claude can make the prep work more efficient, but it can’t replace the cognitive work of actually studying.</p>
<p>For Human Disease and Evolution specifically, Claude helped me organize a semester’s worth of dense notes into a manageable study plan. But I’m not bringing Claude into the exam room—I need to make sure the knowledge is in my head, not just in my AI assistant’s context window.</p>
<p>The tool is helpful. But over-relying on it would be a mistake that probably shows up when exam day arrives.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>daily-life</category>
  <category>professional</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/015_/015.html</guid>
  <pubDate>Mon, 13 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/015_/disease-evolution-notes.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Superagency Review #1: History Is Written by the Winners</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/014_/014.html</link>
  <description><![CDATA[ 





<section id="reading-superagency-a-gloomers-perspective" class="level2">
<h2 class="anchored" data-anchor-id="reading-superagency-a-gloomers-perspective">Reading Superagency: A Gloomer’s Perspective</h2>
<p>I’m reading Reid Hoffman and Greg Beato’s <a href="https://www.superagency.ai/"><em>Superagency: What Could Possibly Go Right with Our AI Future</em></a>—a book that argues for optimism about generative AI and its potential to enhance human agency. As someone who identifies as a gloomer about AI, I’m approaching this with skepticism but genuine curiosity.</p>
<p>This is part 1 of 5 reviews covering the entire book. Pages 1-47: The introduction and framing.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/014_/superagency-book-cover.jpg" alt="Superagency Book" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Superagency by Reid Hoffman and Greg Beato</em></p>
</section>
<section id="the-historical-framing-art-luddites-and-printing-presses" class="level2">
<h2 class="anchored" data-anchor-id="the-historical-framing-art-luddites-and-printing-presses">The Historical Framing: Art, Luddites, and Printing Presses</h2>
<p>The introduction opens with an interesting historical analogy. Hoffman and Beato walk through past technological disruptions:</p>
<p><strong>Socrates and Art:</strong> Socrates opposed art because it failed to capture the conversational, evolutionary, iterative process of discussion-based learning. Written text was static; dialogue was dynamic.</p>
<p><strong>The Luddites:</strong> Workers who resisted industrial machinery that threatened their livelihoods and ways of life.</p>
<p><strong>The Printing Press:</strong> A technology that faced backlash from those who feared the democratization of knowledge.</p>
<p>The pattern is clear: history is full of examples of technological backlash. The book frames AI as the latest in this line—another “infrastructural change event” that will transform human capacity and agency.</p>
<p>I’ll admit, the framing is effective. The core question the book poses is compelling: “Can we continue to maintain control of our lives, and successfully plot our own destinies?” in the age of AI.</p>
</section>
<section id="history-is-written-by-the-winners" class="level2">
<h2 class="anchored" data-anchor-id="history-is-written-by-the-winners">History Is Written by the Winners</h2>
<p>Here’s where my skepticism kicks in.</p>
<p>Yes, all these technological disruptions—art, steam power, the printing press—were consequential and enabled human agency in important ways:</p>
<p><strong>Art</strong> captured feelings, periods, and provided first-hand historical resources.</p>
<p><strong>Steam power</strong> and the Industrial Revolution brought unimaginable productivity and capital accumulation, leading to the consolidation of modern cities.</p>
<p><strong>The printing press</strong> enabled communication and knowledge dissemination like never before.</p>
<p>We’ve all benefited from these major strides, one way or another. But the more closely we examine the nuances of these pivotal moments, the more we realize there’s much more to the story.</p>
<p>The problem is that we often focus on the positives—maybe as a sign of human delusion, or a craving to minimize problems. Important details get left behind. History is written by the winners, after all.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/014_/industrial-revolution-workers.jpg" alt="Historical Technology Impact" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>The Industrial Revolution: productivity gains alongside worker displacement</em></p>
</section>
<section id="what-the-winners-history-leaves-out" class="level2">
<h2 class="anchored" data-anchor-id="what-the-winners-history-leaves-out">What the Winners’ History Leaves Out</h2>
<section id="the-industrial-revolutions-dark-side" class="level3">
<h3 class="anchored" data-anchor-id="the-industrial-revolutions-dark-side">The Industrial Revolution’s Dark Side</h3>
<p>The steam engine and industrialization brought unprecedented productivity. But they also brought: - Brutal working conditions in factories - Child labor - Destruction of artisan crafts and traditional ways of life - Environmental degradation that we’re still dealing with - Massive wealth inequality and the creation of exploitative labor systems</p>
<p>According to <a href="https://www.britannica.com/event/Industrial-Revolution">historical analysis of the Industrial Revolution</a>, while productivity soared, real wages for workers stagnated or declined for decades. The “winners” who wrote that history were the factory owners and industrialists, not the displaced weavers or child laborers.</p>
</section>
<section id="the-printing-press-and-information-chaos" class="level3">
<h3 class="anchored" data-anchor-id="the-printing-press-and-information-chaos">The Printing Press and Information Chaos</h3>
<p>The printing press democratized knowledge—but it also: - Enabled mass propaganda - Facilitated religious wars and sectarian violence - Created information overload that people weren’t equipped to handle - Disrupted existing knowledge gatekeepers (sometimes for good, sometimes not)</p>
<p>The Reformation, enabled by printing technology, led to centuries of religious conflict. That’s not in the optimistic framing.</p>
</section>
<section id="the-luddites-were-right-about-some-things" class="level3">
<h3 class="anchored" data-anchor-id="the-luddites-were-right-about-some-things">The Luddites Were Right About Some Things</h3>
<p>We use “Luddite” as an insult for technophobes. But the original Luddites weren’t irrationally afraid of technology—they were skilled workers watching their livelihoods and communities be destroyed by machines that enriched factory owners.</p>
<p>Their concerns about who benefits from technological change were valid. The technology did increase productivity, but the gains went to capital owners, not workers. The Luddites lost that fight, so history remembers them as backwards resistors rather than people with legitimate concerns about economic justice.</p>
</section>
</section>
<section id="the-agency-question-for-whom" class="level2">
<h2 class="anchored" data-anchor-id="the-agency-question-for-whom">The Agency Question: For Whom?</h2>
<p>Hoffman and Beato frame AI as an “agency enabler”—technology that will enhance our capacity to “do what we ought to do” and “plot our own destinies.”</p>
<p>But here’s the critical question the introduction doesn’t adequately address: <strong>Whose agency gets enabled?</strong></p>
<section id="technology-doesnt-distribute-benefits-evenly" class="level3">
<h3 class="anchored" data-anchor-id="technology-doesnt-distribute-benefits-evenly">Technology Doesn’t Distribute Benefits Evenly</h3>
<p>Past technological revolutions increased <em>aggregate</em> human capacity and agency. But the distribution was wildly unequal: - Factory owners gained agency; workers often lost autonomy - Landowners who adopted new agricultural tech thrived; tenant farmers were displaced - Early adopters of communication technology gained power; those without access fell further behind</p>
<p>When Hoffman talks about AI enabling “our” agency, who is “we”? Tech founders and investors? Knowledge workers with access to cutting-edge tools? Or also the truck drivers, customer service workers, and radiologists whose jobs might be automated?</p>
</section>
<section id="have-we-forgotten-our-ways-of-life" class="level3">
<h3 class="anchored" data-anchor-id="have-we-forgotten-our-ways-of-life">Have We Forgotten Our Ways of Life?</h3>
<p>Each technological revolution didn’t just change what people could do—it changed how people lived. Industrialization pulled people from rural communities into urban factories. The printing press disrupted oral traditions and face-to-face knowledge transmission.</p>
<p>Some of these changes were positive. Some represented the loss of valuable ways of life that we can never recover.</p>
<p>The introduction’s optimistic framing acknowledges these past disruptions but seems confident that AI will be different—that we can get the benefits without the costs. History suggests otherwise.</p>
</section>
</section>
<section id="what-im-looking-for-in-the-rest-of-the-book" class="level2">
<h2 class="anchored" data-anchor-id="what-im-looking-for-in-the-rest-of-the-book">What I’m Looking For in the Rest of the Book</h2>
<p>The introduction sets up an optimistic case for AI as an agency enabler, using historical technological disruptions as precedent. The framing is well-done and intellectually honest about past backlash to new technologies.</p>
<p>But it seems to brush past the real costs of those past revolutions. The “winners” narrative focuses on aggregate gains while downplaying distributional concerns and the destruction of existing ways of life.</p>
<p>As I continue reading, I’m looking for:</p>
<p><strong>Distributional analysis:</strong> Who specifically gains agency from AI, and who loses it?</p>
<p><strong>Cost acknowledgment:</strong> What ways of life, skills, or social structures might we lose, and are those losses acceptable?</p>
<p><strong>Power dynamics:</strong> How do we ensure AI benefits are distributed more equitably than past technological revolutions?</p>
<p><strong>Concrete mechanisms:</strong> Not just “AI will enable agency,” but <em>how</em> and <em>for whom</em> specifically?</p>
<p>The book asks: “What could possibly go right with our AI future?” But the introduction’s own historical examples suggest we should also ask: “What will go wrong, and for whom?”</p>
<p>History is written by the winners. I’m reading this book to see if Hoffman and Beato can imagine an AI future where there are fewer losers than in past technological revolutions—or if they’re just writing the winners’ narrative before the story has finished.</p>
</section>
<section id="up-next" class="level2">
<h2 class="anchored" data-anchor-id="up-next">Up Next</h2>
<p>Review #2 will cover pages 48-94, where I expect the book to start making its positive case for AI more explicitly. I’ll be watching to see if it addresses the distributional concerns raised by its own historical framing.</p>
<p>Stay tuned.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>book-review</category>
  <category>skeptical</category>
  <category>research</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/014_/014.html</guid>
  <pubDate>Sat, 11 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/014_/superagency-book-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Testing AI as a Tutor: Does Prompt Structure Actually Matter?</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/013_/013.html</link>
  <description><![CDATA[ 





<section id="learning-to-cook-with-ai-an-experiment" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-cook-with-ai-an-experiment">Learning to Cook with AI: An Experiment</h2>
<p>I’m Peruvian but never learned to cook Lomo Saltado properly. Living in a college dorm with limited equipment, I decided to use Claude as a tutor to learn. But instead of just asking for help, I ran an experiment: two separate tutoring sessions using different prompting approaches to see if prompt engineering actually makes a difference for learning.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/013_/lomo-saltado-dish.jpg" alt="Lomo Saltado Dish" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>A finished plate of Lomo Saltado - the goal of the tutoring sessions</em></p>
</section>
<section id="the-experiment-setup" class="level2">
<h2 class="anchored" data-anchor-id="the-experiment-setup">The Experiment Setup</h2>
<section id="session-1-vanilla-claude" class="level3">
<h3 class="anchored" data-anchor-id="session-1-vanilla-claude">Session 1: Vanilla Claude</h3>
<p>I started a fresh conversation with no special prompting. Just asked:</p>
<blockquote class="blockquote">
<p>“I want to learn how to cook Lomo Saltado in my college dorm. Can you teach me?”</p>
</blockquote>
<p>Claude immediately provided a comprehensive ingredient list, equipment needs, and step-by-step instructions. Efficient and informative.</p>
</section>
<section id="session-2-structured-tutoring-prompt" class="level3">
<h3 class="anchored" data-anchor-id="session-2-structured-tutoring-prompt">Session 2: Structured Tutoring Prompt</h3>
<p>For the second session, I used the <a href="https://www.oneusefulthing.org/p/how-to-use-ai-to-teach">Mollick structured tutoring prompt</a>, which instructs the AI to: - Introduce itself by name - Ask questions one at a time before teaching - Gather information about the student’s prior knowledge - Use leading questions rather than just providing answers - Provide emotional support and encouragement</p>
<p>The prompt started with:</p>
<blockquote class="blockquote">
<p>“You are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor who is happy to help them with any questions…”</p>
</blockquote>
<p>Same question about learning Lomo Saltado, but this time Claude introduced itself as “Alex” and asked about my cooking experience before giving any instructions.</p>
</section>
</section>
<section id="how-the-sessions-differed" class="level2">
<h2 class="anchored" data-anchor-id="how-the-sessions-differed">How The Sessions Differed</h2>
<section id="session-1-information-first" class="level3">
<h3 class="anchored" data-anchor-id="session-1-information-first">Session 1: Information First</h3>
<p>Claude immediately delivered: - Complete ingredient list with substitutions - All equipment needed - Step-by-step cooking instructions - Do’s and don’ts - Storage and freezing tips</p>
<p>It was like reading a very thorough cookbook. Efficient, but I started feeling overwhelmed around the “do’s and don’ts” section because it kept adding more information without checking if I was following along.</p>
<p>When I asked for a “cookbook style summary” later, it became clear I’d experienced information overload.</p>
</section>
<section id="session-2-questions-first" class="level3">
<h3 class="anchored" data-anchor-id="session-2-questions-first">Session 2: Questions First</h3>
<p>Claude asked: - What’s my cooking experience level? - What equipment do I have access to? - What do I already know about Lomo Saltado?</p>
<p>When I mentioned I’m Peruvian and grew up eating this dish, the whole approach changed. Claude acknowledged my cultural connection, didn’t waste time explaining what Lomo Saltado is, and focused on translating my taste memory into cooking technique.</p>
<p>The information came in smaller chunks, each followed by a question to check understanding. It felt more like a conversation than a lecture.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/013_/claude-tutoring-interface.jpg" alt="Claude Tutoring Interface" style="max-width: 450px; max-height: 300px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Comparing the two Claude tutoring session interfaces</em></p>
</section>
</section>
<section id="when-things-went-wrong" class="level2">
<h2 class="anchored" data-anchor-id="when-things-went-wrong">When Things Went Wrong</h2>
<p>I intentionally messed up the recipe in both sessions to see how each tutor would handle mistakes.</p>
<section id="session-1-response" class="level3">
<h3 class="anchored" data-anchor-id="session-1-response">Session 1 Response</h3>
<p>When I said I overcooked the meat, added too much soy sauce, and the tomatoes were mushy, Claude gave practical solutions: - Slice overcooked meat thinner - Balance soy sauce with vinegar and sugar - Remove mushy tomatoes and add fresh ones</p>
<p>Organized by problem, very solution-focused. It worked, but felt clinical.</p>
<p>At one point I got frustrated and said “I do NOT want to try again tomorrow. I want to DO IT NOW.” The tutor went into emergency rescue mode, which helped but didn’t address the emotional crisis.</p>
</section>
<section id="session-2-response" class="level3">
<h3 class="anchored" data-anchor-id="session-2-response">Session 2 Response</h3>
<p>When I introduced the same problems, Claude started with:</p>
<blockquote class="blockquote">
<p>“Don’t panic! This is totally normal when learning, and yes, we can salvage this!”</p>
</blockquote>
<p>Then provided the same practical solutions, but added:</p>
<blockquote class="blockquote">
<p>“Remember: Even ‘mistakes’ can taste good, and every Peruvian abuela had to start somewhere!”</p>
</blockquote>
<p>This was culturally resonant and emotionally supportive. The solutions were similar, but the tone acknowledged that learning involves struggle, not just technical problem-solving.</p>
<p>I didn’t have the same emotional breakdown in Session 2. The earlier relationship-building (validation, personalization) seemed to prevent it.</p>
</section>
</section>
<section id="the-final-cookbook-recipes" class="level2">
<h2 class="anchored" data-anchor-id="the-final-cookbook-recipes">The Final Cookbook Recipes</h2>
<p>Both sessions ended with me asking for a clean cookbook-style recipe.</p>
<section id="session-1-recipe" class="level3">
<h3 class="anchored" data-anchor-id="session-1-recipe">Session 1 Recipe</h3>
<ul>
<li>Heavy use of emoji headers (🥩, 🍅, ⏰)</li>
<li>Multiple subsections: Equipment, Ingredients, Shortcuts, Steps, Money-Saving Tips, Dorm Hacks, Storage</li>
<li>Comprehensive resource document</li>
<li>Generic college student advice</li>
</ul>
</section>
<section id="session-2-recipe" class="level3">
<h3 class="anchored" data-anchor-id="session-2-recipe">Session 2 Recipe</h3>
<ul>
<li>Cleaner visual hierarchy</li>
<li>Sections: What You’ll Need → Game Plan → Key Success Tips → Storage</li>
<li>More scannable layout</li>
<li>Tailored to: college student + new cook + Peruvian heritage + meal prep constraints</li>
</ul>
<p>Both recipes are accurate and functional. Session 1 is more comprehensive; Session 2 is more personalized to my specific situation.</p>
</section>
</section>
<section id="what-actually-made-a-difference" class="level2">
<h2 class="anchored" data-anchor-id="what-actually-made-a-difference">What Actually Made a Difference</h2>
<section id="the-structured-prompt-won" class="level3">
<h3 class="anchored" data-anchor-id="the-structured-prompt-won">The Structured Prompt Won</h3>
<p>Session 2 was the better tutoring experience because:</p>
<p><strong>Better sequencing:</strong> Asking questions before teaching meant the information was calibrated to what I actually needed, not everything the AI knows about Lomo Saltado.</p>
<p><strong>Personalization:</strong> Learning I was Peruvian changed how Claude explained things. It connected to my existing knowledge (“You know what it should taste like”) rather than starting from zero.</p>
<p><strong>Emotional support:</strong> Normalizing mistakes and providing encouragement made me more likely to persist through problems. This matters for skill-building where failure is inevitable.</p>
<p><strong>Active learning:</strong> Session 2 ended each explanation with a question, forcing me to think rather than passively receive information.</p>
</section>
<section id="where-session-1-was-better" class="level3">
<h3 class="anchored" data-anchor-id="where-session-1-was-better">Where Session 1 Was Better</h3>
<p><strong>Speed:</strong> If you just want information fast, the vanilla approach delivers immediately without the question-gathering phase.</p>
<p><strong>Comprehensiveness:</strong> The final recipe from Session 1 had more detail on meal prep, freezing, and storage tips.</p>
<p><strong>No preamble:</strong> Some people find the “getting to know you” phase of structured tutoring annoying if they just want answers.</p>
</section>
</section>
<section id="does-this-matter-beyond-cooking" class="level2">
<h2 class="anchored" data-anchor-id="does-this-matter-beyond-cooking">Does This Matter Beyond Cooking?</h2>
<section id="the-pedagogy-research" class="level3">
<h3 class="anchored" data-anchor-id="the-pedagogy-research">The Pedagogy Research</h3>
<p>The structured prompt approach aligns with established learning science. According to <a href="https://ies.ed.gov/learn/blog/high-quality-tutoring-evidence-based-strategy-tackle-learning-loss">research on effective tutoring</a>, good tutoring involves: - Assessing prior knowledge before teaching - Providing scaffolded support - Using questions to promote active learning - Giving emotional encouragement alongside technical help</p>
<p>These aren’t AI-specific insights—they’re how human tutoring works best too.</p>
</section>
<section id="the-limitations" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations">The Limitations</h3>
<p>Even with perfect prompting, AI tutoring has constraints:</p>
<p><strong>Can’t taste your food:</strong> Claude couldn’t tell me if my Lomo Saltado actually tasted right. I had to rely on my own judgment.</p>
<p><strong>Can’t see your technique:</strong> When I said the meat was overcooked, Claude couldn’t see whether I was cutting it correctly or using too high heat. The advice was generic.</p>
<p><strong>Can’t adapt in real-time:</strong> A human tutor watching me cook would catch mistakes as they happen. AI only responds to what I describe.</p>
<p><strong>Generic cultural knowledge:</strong> Session 2’s cultural references (“Peruvian abuela”) were sweet but generic. It didn’t know my actual family’s cooking style or regional variations.</p>
</section>
<section id="when-structured-prompting-matters-most" class="level3">
<h3 class="anchored" data-anchor-id="when-structured-prompting-matters-most">When Structured Prompting Matters Most</h3>
<p>The prompt engineering made the biggest difference when: - Learning something new (not just looking up facts) - Dealing with frustration or mistakes - Needing personalized guidance - Building skills through practice</p>
<p>For quick information retrieval, vanilla Claude is probably fine. For actual learning, structure helps.</p>
</section>
</section>
<section id="what-i-learned" class="level2">
<h2 class="anchored" data-anchor-id="what-i-learned">What I Learned</h2>
<section id="about-ai-tutoring" class="level3">
<h3 class="anchored" data-anchor-id="about-ai-tutoring">About AI Tutoring</h3>
<p>Prompt engineering isn’t just theoretical optimization—it genuinely changed my learning experience. The structured prompt created better dialogue, personalization, and emotional support.</p>
<p>But even good prompting has limits. AI tutoring works best for knowledge-based learning where you can verify results yourself. For physical skills like cooking, you still need to trust your own judgment about the outcome.</p>
</section>
<section id="about-learning-lomo-saltado" class="level3">
<h3 class="anchored" data-anchor-id="about-learning-lomo-saltado">About Learning Lomo Saltado</h3>
<p>I actually cooked it. Both sessions gave me functional recipes, but Session 2’s emotional support made me more willing to try despite knowing I’d probably mess up.</p>
<p>The AI was right about one thing: even mistakes taste pretty good, and you learn by doing.</p>
</section>
<section id="about-prompt-design" class="level3">
<h3 class="anchored" data-anchor-id="about-prompt-design">About Prompt Design</h3>
<p>Good tutoring prompts should include: - Instructions to gather context before teaching - One-question-at-a-time pacing - Emotional scaffolding for when things go wrong - Requirements to keep students actively thinking</p>
<p>These aren’t complex AI techniques—they’re just principles of good teaching, codified for an AI to follow.</p>
<p>If you’re using AI to learn something practical, take five minutes to set up a proper tutoring prompt. The difference is real.</p>


</section>
</section>

 ]]></description>
  <category>research</category>
  <category>professional</category>
  <category>LLM</category>
  <category>daily-life</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/013_/013.html</guid>
  <pubDate>Thu, 09 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/013_/lomo-saltado-dish.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Switching from Consulting to Finance Prep with AI</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/012_/012.html</link>
  <description><![CDATA[ 





<section id="pivoting-interview-prep-consulting-to-finance" class="level2">
<h2 class="anchored" data-anchor-id="pivoting-interview-prep-consulting-to-finance">Pivoting Interview Prep: Consulting to Finance</h2>
<p>I have an upcoming interview for a Santander CIB Investment Banking Analyst Program, specifically for their DCM (Debt Capital Markets) division. This is different from the consulting interviews I’ve been preparing for—finance interviews require understanding specific firms, technical knowledge about debt markets, and different types of behavioral questions.</p>
<p>Since I’ve been using AI for consulting prep, I decided to test whether Claude and Perplexity could help me pivot to finance interview preparation.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/012_/claude-finance-research.jpg" alt="Claude Research Interface" style="max-width: 600px; max-height: 350px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Claude conducting research on Santander’s DCM business and interview approach</em></p>
</section>
<section id="the-prompts-i-used" class="level2">
<h2 class="anchored" data-anchor-id="the-prompts-i-used">The Prompts I Used</h2>
<section id="initial-research-request-to-claude" class="level3">
<h3 class="anchored" data-anchor-id="initial-research-request-to-claude">Initial Research Request to Claude</h3>
<p>My first prompt:</p>
<blockquote class="blockquote">
<p>“First and foremost, I am more so interested in understanding DCM, and potential BEHAVIORAL questions that might arise.</p>
<p>Why DCM? Why DCM LatAm? Why Santander DCM?</p>
<p>Use the web to thoroughly search and understand more about the firm, how the firm talks about itself, and what common behavioral questions do they engage with for their IB process.”</p>
</blockquote>
<p>Claude responded:</p>
<blockquote class="blockquote">
<p>“I’ll conduct thorough research on Santander CIB, their DCM business, and their interview process. Let me search for information about how they present themselves, their Latin America focus, and common interview questions.”</p>
</blockquote>
<p>It then ran searches for: - “Santander Corporate Investment Banking DCM” (10 results) - Started searching the web for additional information</p>
</section>
<section id="perplexity-deep-dive" class="level3">
<h3 class="anchored" data-anchor-id="perplexity-deep-dive">Perplexity Deep Dive</h3>
<p>I also used Perplexity’s research feature with multiple targeted searches:</p>
<ul>
<li>“Santander CIB debt capital markets”</li>
<li>“Santander investment banking culture values”</li>
<li>“Santander CIB Latin America strategy”</li>
</ul>
<p>Perplexity found 20 sources including: - Debt Capital Markets | Santander Corporate &amp; Investment Banking - From personalization to social values: what is shaking up investment [banking] - Macro &amp; Strategy Research - The past, present and future of debt capital markets - [PDF] Culture Report 2019 - Banco Santander - Santander US Capital Markets</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/012_/perplexity-research-results.jpg" alt="Perplexity Research" style="max-width: 600px; max-height: 350px; object-fit: contain; display: block; margin: 20px auto;"></p>
<p><em>Perplexity gathering and reviewing sources on Santander DCM</em></p>
</section>
</section>
<section id="what-actually-helped" class="level2">
<h2 class="anchored" data-anchor-id="what-actually-helped">What Actually Helped</h2>
<section id="firm-specific-research" class="level3">
<h3 class="anchored" data-anchor-id="firm-specific-research">Firm-Specific Research</h3>
<p>Both tools pulled relevant information about Santander’s positioning: - Their focus on Latin America markets and how that differentiates them - Recent deals and market activity in DCM - Corporate values and culture statements - Strategic priorities in debt capital markets</p>
<p>This gave me talking points for “Why Santander” that were specific rather than generic.</p>
</section>
<section id="understanding-dcm-vs.-other-ib-divisions" class="level3">
<h3 class="anchored" data-anchor-id="understanding-dcm-vs.-other-ib-divisions">Understanding DCM vs.&nbsp;Other IB Divisions</h3>
<p>The AI helped clarify: - What DCM analysts actually do day-to-day - How DCM differs from M&amp;A or equity capital markets - Why someone might choose DCM over other finance paths - Technical concepts I needed to understand (bond pricing, credit ratings, syndication)</p>
</section>
<section id="behavioral-question-preparation" class="level3">
<h3 class="anchored" data-anchor-id="behavioral-question-preparation">Behavioral Question Preparation</h3>
<p>The tools suggested finance-specific behavioral questions that differ from consulting: - “Walk me through a DCF” (technical behavioral) - “Why debt capital markets vs.&nbsp;other finance roles?” - “How do you handle working with difficult clients on tight deadlines?” - “Tell me about a time you analyzed complex financial information”</p>
<p>These are different from consulting’s case-focused approach.</p>
</section>
<section id="identifying-knowledge-gaps" class="level3">
<h3 class="anchored" data-anchor-id="identifying-knowledge-gaps">Identifying Knowledge Gaps</h3>
<p>The research helped me realize what I didn’t know: - Specific debt instruments Santander specializes in - Recent Latin American market trends affecting DCM - Technical terminology I needed to learn - The difference between investment-grade and high-yield debt origination</p>
</section>
</section>
<section id="significant-limitations" class="level2">
<h2 class="anchored" data-anchor-id="significant-limitations">Significant Limitations</h2>
<section id="surface-level-firm-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="surface-level-firm-knowledge">Surface-Level Firm Knowledge</h3>
<p>The biggest problem: AI gives you the information that’s publicly available on corporate websites and general industry sources. It can’t tell you: - What Santander’s DCM team culture is actually like - Specific deals they’re working on that aren’t public - Internal priorities that aren’t in press releases - What actually impresses their interviewers vs.&nbsp;what sounds good on paper</p>
</section>
<section id="generic-behavioral-answers" class="level3">
<h3 class="anchored" data-anchor-id="generic-behavioral-answers">Generic Behavioral Answers</h3>
<p>When I asked Claude to help refine my answers to “Why DCM?”, it produced polished but generic responses: - “I’m drawn to the analytical rigor of debt markets…” - “The complexity of structuring transactions appeals to me…” - “I want to work at the intersection of capital markets and corporate strategy…”</p>
<p>These sound like everyone else’s AI-polished answers. The interviewers at Santander have probably heard variations of these from dozens of candidates using similar tools.</p>
</section>
<section id="cant-teach-technical-skills" class="level3">
<h3 class="anchored" data-anchor-id="cant-teach-technical-skills">Can’t Teach Technical Skills</h3>
<p>AI can explain what a DCF is, but it can’t help you build the muscle memory of actually modeling one under time pressure. It can’t simulate the experience of being asked to walk through a valuation on a whiteboard while someone watches.</p>
<p><a href="https://mergersandinquisitions.com/investment-banking-interview-questions-and-answers/">Finance interview prep resources</a> emphasize that technical skills require hands-on practice with actual models, not just conceptual understanding.</p>
</section>
<section id="outdated-information" class="level3">
<h3 class="anchored" data-anchor-id="outdated-information">Outdated Information</h3>
<p>Some of the sources Perplexity pulled were from 2019 (the Culture Report). The finance industry moves quickly—strategies, priorities, and team structures change. Relying on AI-found sources without verifying recency is risky.</p>
</section>
<section id="misses-networking-insights" class="level3">
<h3 class="anchored" data-anchor-id="misses-networking-insights">Misses Networking Insights</h3>
<p>The most valuable interview prep for finance comes from talking to people who work at the firm or in similar roles. AI can’t replace: - Coffee chats with current Santander analysts - Alumni connections who know the interview process - Understanding what specific interviewers care about - Getting feedback on your story from people in the industry</p>
</section>
</section>
<section id="consulting-prep-vs.-finance-prep-with-ai" class="level2">
<h2 class="anchored" data-anchor-id="consulting-prep-vs.-finance-prep-with-ai">Consulting Prep vs.&nbsp;Finance Prep with AI</h2>
<section id="what-transfers" class="level3">
<h3 class="anchored" data-anchor-id="what-transfers">What Transfers</h3>
<p>Some aspects of using AI for consulting prep apply to finance: - Organizing practice materials and tracking progress - Getting quick explanations of concepts - Structuring answers using frameworks - Identifying areas where you need more preparation</p>
</section>
<section id="whats-different" class="level3">
<h3 class="anchored" data-anchor-id="whats-different">What’s Different</h3>
<p>Finance interviews require different AI usage:</p>
<p><strong>Technical Knowledge:</strong> Consulting is more about frameworks and problem-solving approach. Finance requires specific technical knowledge (financial modeling, valuation methods, market mechanics) that you need to practice hands-on.</p>
<p><strong>Firm Research:</strong> Consulting firms are more about “fit” and problem-solving ability. Finance firms want to know you understand their specific business lines, recent deals, and market positioning.</p>
<p><strong>Answer Style:</strong> Consulting wants structured, MECE thinking demonstrated verbally. Finance wants technical competence demonstrated through modeling and concise, confident answers about markets.</p>
<p><strong>Preparation Balance:</strong> For consulting, AI can handle maybe 40% of prep (frameworks, structure, case types). For finance, it’s more like 20%—you need more hands-on technical practice and human networking.</p>
</section>
</section>
<section id="my-actual-approach" class="level2">
<h2 class="anchored" data-anchor-id="my-actual-approach">My Actual Approach</h2>
<section id="using-ai-for-initial-research-phase" class="level3">
<h3 class="anchored" data-anchor-id="using-ai-for-initial-research-phase">Using AI for Initial Research Phase</h3>
<p>I’m using Claude and Perplexity to: - Get baseline understanding of Santander’s DCM business - Compile a list of recent deals and market trends - Understand basic DCM technical concepts - Draft initial answers to behavioral questions</p>
<p>But I treat this as the starting point, not the final product.</p>
</section>
<section id="following-up-with-human-sources" class="level3">
<h3 class="anchored" data-anchor-id="following-up-with-human-sources">Following Up with Human Sources</h3>
<p>After AI research, I’m: - Reaching out to Santander employees on LinkedIn - Talking to friends in investment banking about DCM - Getting my behavioral answers reviewed by people in finance - Asking specific questions about Santander’s Latin America focus</p>
</section>
<section id="technical-skill-building" class="level3">
<h3 class="anchored" data-anchor-id="technical-skill-building">Technical Skill Building</h3>
<p>For the technical components, AI isn’t much help. I’m: - Working through actual DCF models - Practicing explaining valuation methods out loud - Reviewing recent debt deals and their structures - Doing mock technical questions with finance friends</p>
</section>
<section id="verifying-everything" class="level3">
<h3 class="anchored" data-anchor-id="verifying-everything">Verifying Everything</h3>
<p>Any specific fact or claim from AI research, I verify: - Check Santander’s official investor relations materials - Look up recent news about their DCM business - Confirm market trends with multiple sources - Cross-reference technical concepts with textbooks</p>
<p>The AI can point me toward information, but I don’t trust it without verification.</p>
</section>
</section>
<section id="final-assessment" class="level2">
<h2 class="anchored" data-anchor-id="final-assessment">Final Assessment</h2>
<p>Using AI to pivot from consulting to finance interview prep is helpful but limited. It’s good for: - Quick baseline research on the firm and role - Understanding how finance interviews differ from consulting - Getting initial structure for behavioral answers - Identifying what you don’t know</p>
<p>But it’s insufficient for: - Developing real technical skills - Understanding firm-specific culture and priorities - Creating distinctive, memorable answers - Getting the insider knowledge that networking provides</p>
<p>Finance interviews, even more than consulting, require human connections and hands-on technical practice. AI can accelerate the research phase, but it can’t replace the core preparation activities that actually matter.</p>
<p>The risk is thinking that comprehensive AI research is enough. It’s not. The candidates who get offers at places like Santander DCM are the ones who combine efficient research with deep technical preparation and strong industry networks—things AI can’t provide.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>professional</category>
  <category>claude</category>
  <category>perplexity</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/012_/012.html</guid>
  <pubDate>Sun, 05 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/012_/claude-finance-research.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Running a Virtual Focus Group with ChatGPT</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/011.html</link>
  <description><![CDATA[ 





<section id="testing-products-with-ai-generated-focus-groups" class="level2">
<h2 class="anchored" data-anchor-id="testing-products-with-ai-generated-focus-groups">Testing Products with AI-Generated Focus Groups</h2>
<p>For the Cuzco Crunch project, Eury and I needed to test two product concepts before committing to one. Instead of recruiting actual participants for a focus group, we decided to try something experimental: using ChatGPT to simulate a focus group with 10 diverse personas.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/cuzco-crunch-package.jpg" alt="Cuzco Crunch Product" style="max-width: 450px; max-height: 300px; object-fit: cover; object-position: center; display: block; margin: 20px auto;"></p>
<p><em>Cuzco Crunch: Golden plantain slices with Peruvian sal de Maras</em></p>
</section>
<section id="the-experimental-design" class="level2">
<h2 class="anchored" data-anchor-id="the-experimental-design">The Experimental Design</h2>
<section id="two-products-to-test" class="level3">
<h3 class="anchored" data-anchor-id="two-products-to-test">Two Products to Test</h3>
<p><strong>Cuzco Crunch (Product A):</strong> Positioned as premium - golden, ultra-crispy plantain slices with Peruvian sal de Maras for a mineral-salt finish. Natural sweetness from plantain, meant to be versatile. Price: $7 for 1.5 oz.</p>
<p><strong>Plantain Lite (Product B):</strong> Positioned as everyday - lighter snack with delicate crunch and simple seasoning. Emphasizes convenience and portability. Price: $5 for 1.5 oz.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/plantain-lite-package.jpg" alt="Plantain Lite Product" style="max-width: 500px; max-height: 400px; object-fit: cover; object-position: center; display: block; margin: 20px auto;"></p>
<p><em>Plantain Lite: A lighter, everyday plantain chip option</em></p>
</section>
<section id="creating-10-diverse-personas" class="level3">
<h3 class="anchored" data-anchor-id="creating-10-diverse-personas">Creating 10 Diverse Personas</h3>
<p>We built personas across multiple demographic dimensions: - <strong>Gender:</strong> Male/Female - <strong>Age ranges:</strong> 18-25, 26-33, 34-41, 42-50 - <strong>Health conditions:</strong> Hypertension, obesity, heart disease, asthma, food allergies, digestive issues, or none - <strong>Ethnicity:</strong> Latino or Not Latino - <strong>Location:</strong> Urban, suburban, or rural</p>
<p>Examples included: - Young urban health-conscious Latina (18-25, no health conditions) - Middle-aged suburban Latino managing hypertension (42-50) - Young urban non-Latino with food allergies (26-33, gluten/dairy/nuts) - Middle-aged rural non-Latino with obesity (34-41) - Middle-aged rural Latina with multiple conditions (42-50, hypertension + obesity)</p>
<p>The goal was to represent our potential target segments and see how different audiences responded to each product.</p>
</section>
</section>
<section id="the-calibrated-survey" class="level2">
<h2 class="anchored" data-anchor-id="the-calibrated-survey">The Calibrated Survey</h2>
<section id="starting-with-a-benchmark" class="level3">
<h3 class="anchored" data-anchor-id="starting-with-a-benchmark">Starting with a Benchmark</h3>
<p>We calibrated responses by having participants rate Lay’s Classic Potato Chips first as a reference point. This gave us a common baseline to compare against: - 1 = Unacceptable, wouldn’t eat even if free - 3 = Acceptable/Average, meets basic expectations - 5 = Excellent, exceeds expectations</p>
</section>
<section id="comprehensive-rating-categories" class="level3">
<h3 class="anchored" data-anchor-id="comprehensive-rating-categories">Comprehensive Rating Categories</h3>
<p>The survey covered:</p>
<p><strong>A. Overall Satisfaction</strong> (quality, purchase intent, recommendation likelihood)</p>
<p><strong>B. Taste &amp; Flavor</strong> (overall taste, flavor intensity, saltiness, naturalness, aftertaste)</p>
<p><strong>C. Texture &amp; Physical Quality</strong> (crunchiness, thickness, consistency, oiliness, freshness)</p>
<p><strong>D. Visual Appeal</strong> (appearance, color, uniformity, packaging appeal, information clarity)</p>
<p><strong>E. Value &amp; Competitive Positioning</strong> (value for money, price sensitivity, preference vs potato chips and competitors)</p>
<p><strong>F. Product Attributes</strong> (uniqueness, healthiness, suitability for guests, meeting expectations)</p>
<p><strong>G. Usage Context</strong> (purchase frequency, consumption occasions)</p>
<p>Each persona rated 33 quantitative questions plus provided qualitative feedback on likes, improvements, and how they’d describe the product.</p>
</section>
</section>
<section id="what-we-learned-from-the-exercise" class="level2">
<h2 class="anchored" data-anchor-id="what-we-learned-from-the-exercise">What We Learned from the Exercise</h2>
<section id="ai-can-generate-plausible-responses" class="level3">
<h3 class="anchored" data-anchor-id="ai-can-generate-plausible-responses">AI Can Generate Plausible Responses</h3>
<p>ChatGPT was surprisingly good at maintaining consistent personas. The middle-aged rural Latina with hypertension and obesity consistently flagged sodium concerns and price sensitivity across multiple questions. The young urban health-conscious Latina responded positively to premium positioning and cultural connection.</p>
<p>The personas felt internally coherent - their ratings for saltiness, healthiness, and value aligned with their demographic profiles and stated health concerns.</p>
</section>
<section id="but-its-still-simulated-data" class="level3">
<h3 class="anchored" data-anchor-id="but-its-still-simulated-data">But It’s Still Simulated Data</h3>
<p>The fundamental limitation: these aren’t real taste preferences. The AI is generating statistically plausible responses based on stereotypical associations between demographics and preferences.</p>
<p>For example, it “knows” that someone with hypertension should care about sodium, so it rates accordingly. But it can’t actually tell us if our specific salt level tastes good or if the Peruvian sal de Maras provides a noticeably different experience.</p>
</section>
<section id="useful-for-initial-direction" class="level3">
<h3 class="anchored" data-anchor-id="useful-for-initial-direction">Useful for Initial Direction</h3>
<p>Where this exercise helped: - Identifying which demographic segments might prefer premium vs everyday positioning - Spotting potential concerns (price sensitivity in rural markets, sodium levels for health-conscious segments) - Practicing survey design before using it with real participants - Understanding how different personas might prioritize different product attributes</p>
</section>
</section>
<section id="serious-limitations" class="level2">
<h2 class="anchored" data-anchor-id="serious-limitations">Serious Limitations</h2>
<section id="no-actual-sensory-experience" class="level3">
<h3 class="anchored" data-anchor-id="no-actual-sensory-experience">No Actual Sensory Experience</h3>
<p>The biggest problem: ChatGPT hasn’t tasted anything. It can’t tell us if our plantain chips are actually crunchy, if the salt level is genuinely balanced, or if the flavor profile works.</p>
<p>All taste-related responses are based on generic associations (“premium plantain chips should be crunchier,” “simple seasoning means less salty”). These might not match reality.</p>
</section>
<section id="reinforces-stereotypes" class="level3">
<h3 class="anchored" data-anchor-id="reinforces-stereotypes">Reinforces Stereotypes</h3>
<p>The AI generates responses based on demographic patterns it learned from training data. This means it might reproduce stereotypical assumptions rather than capturing actual individual preferences.</p>
<p>A real 42-year-old Latino with hypertension might not care about sodium as much as the persona suggests, or might have completely different taste preferences than “typical” for that demographic.</p>
</section>
<section id="cant-capture-real-market-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="cant-capture-real-market-dynamics">Can’t Capture Real Market Dynamics</h3>
<p>Things the simulation can’t tell us: - Whether people would actually notice our product on shelves - If the packaging design triggers emotional responses - Whether word-of-mouth would happen organically - If there are unexpected use cases we haven’t considered - How brand perception builds over time</p>
</section>
<section id="the-validation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-validation-problem">The Validation Problem</h3>
<p>We built in validation checks (questions 31-33) to catch inconsistent rating patterns. But when the AI is generating all responses, it’s just validating its own internal consistency, not actual human behavior.</p>
<p>According to <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12234702/">research on AI-generated synthetic data</a>, using LLM outputs as substitutes for human research data can introduce systematic biases that aren’t immediately obvious.</p>
</section>
</section>
<section id="how-were-actually-using-this" class="level2">
<h2 class="anchored" data-anchor-id="how-were-actually-using-this">How We’re Actually Using This</h2>
<section id="initial-hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="initial-hypothesis-testing">Initial Hypothesis Testing</h3>
<p>The AI focus group helped us form hypotheses about product positioning: - Cuzco Crunch might appeal more to urban health-conscious consumers willing to pay premium - Plantain Lite could work better for price-sensitive families and convenience-focused shoppers - Both products might face sodium concerns from health-conscious segments</p>
<p>But these are just hypotheses that need real validation.</p>
</section>
<section id="survey-design-practice" class="level3">
<h3 class="anchored" data-anchor-id="survey-design-practice">Survey Design Practice</h3>
<p>Building the comprehensive survey instrument was valuable. We learned: - Which questions provide useful differentiation - How to structure rating scales with proper calibration - What validation checks to include - Which demographic factors might matter most</p>
<p>The survey itself is now ready to use with actual participants.</p>
</section>
<section id="next-steps-with-real-people" class="level3">
<h3 class="anchored" data-anchor-id="next-steps-with-real-people">Next Steps with Real People</h3>
<p>We’re not making product decisions based on AI responses. The plan is: 1. Use the survey with real taste testers 2. Compare actual results to AI predictions 3. Identify where AI assumptions were wrong 4. Make product decisions based on real preferences</p>
<p>The AI exercise was a dress rehearsal, not the actual performance.</p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Running a virtual focus group with AI personas was an interesting experiment in using LLMs for product development. It’s useful for: - Rapid hypothesis generation - Testing survey instruments - Exploring how different demographic segments might respond - Practicing market research methodologies</p>
<p>But it’s dangerous if you: - Trust the responses as actual market data - Skip real human testing because “we already did AI testing” - Make product decisions based on simulated preferences - Assume the AI understands nuanced taste experiences</p>
<p>For Cuzco Crunch, this exercise helped us structure our research approach and form initial hypotheses. But we’re clear that actual product validation requires real people tasting real chips. The AI can simulate responses, but it can’t simulate whether our plantain chips actually taste good.</p>


</section>

 ]]></description>
  <category>branding</category>
  <category>LLM</category>
  <category>entrepreneurship</category>
  <category>creative</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/011.html</guid>
  <pubDate>Fri, 03 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/011_/cuzco-crunch-package.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using LLMs for Interview Prep</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/010.html</link>
  <description><![CDATA[ 





<section id="building-an-mbb-interview-assistant" class="level2">
<h2 class="anchored" data-anchor-id="building-an-mbb-interview-assistant">Building an MBB Interview Assistant</h2>
<p>With consulting recruiting coming up, I built a custom project in Claude to help with case interview prep. The idea was to have a dedicated space where I could practice cases, track my progress, and get feedback—all in one place.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/interview-preparation.jpg" class="img-fluid" alt="Interview Preparation"> <em>Generic stock photo of a professional interview setting</em></p>
</section>
<section id="how-the-assistant-works" class="level2">
<h2 class="anchored" data-anchor-id="how-the-assistant-works">How the Assistant Works</h2>
<section id="the-setup" class="level3">
<h3 class="anchored" data-anchor-id="the-setup">The Setup</h3>
<p>I created a Claude project called “MBB Interview Assistant” with custom instructions:</p>
<blockquote class="blockquote">
<p>“You will be hearing inputs from MBB interviews. Your role as my highly-paid MBB tutor is to come up with frameworks. Answer questions quickly and brainstorm potential questions, and then… develop a MECE, MBB approach (case structure).”</p>
</blockquote>
<p>The project has become a repository of my case practice—I can see all my past cases listed: UK leisure club market analysis, UK media company revenue challenges, Science magazine revenue decline, Residential cable company alarm services, Kids Place Daycare Capacity Strategy.</p>
</section>
<section id="using-the-voice-recording-feature" class="level3">
<h3 class="anchored" data-anchor-id="using-the-voice-recording-feature">Using the Voice Recording Feature</h3>
<p>The most useful feature has been Claude’s voice recording capability. I can speak through a case out loud, and it transcribes everything accurately and then provides feedback. This simulates the verbal nature of actual case interviews better than typing.</p>
<p>For example, when I was working through the UK leisure club case, I could talk through my framework structure verbally, and Claude would follow along and point out gaps in my logic or areas I hadn’t considered.</p>
</section>
</section>
<section id="what-actually-works" class="level2">
<h2 class="anchored" data-anchor-id="what-actually-works">What Actually Works</h2>
<section id="case-tracking" class="level3">
<h3 class="anchored" data-anchor-id="case-tracking">Case Tracking</h3>
<p>Having all my practice cases in one place is genuinely helpful. I can see which types of cases I’ve worked on and which I need more practice with. The project view shows each case with timestamps, so I can track my prep progress over time.</p>
</section>
<section id="framework-development" class="level3">
<h3 class="anchored" data-anchor-id="framework-development">Framework Development</h3>
<p>Claude is good at helping me structure MECE (Mutually Exclusive, Collectively Exhaustive) frameworks. When I’m stuck on how to break down a problem, it can suggest logical buckets to organize my thinking.</p>
<p>For the daycare capacity strategy case, it helped me think through supply-side factors (staffing, facilities, regulations) and demand-side factors (demographics, competition, pricing) in a structured way.</p>
</section>
<section id="accurate-transcription" class="level3">
<h3 class="anchored" data-anchor-id="accurate-transcription">Accurate Transcription</h3>
<p>The voice recording feature transcribes my verbal case practice accurately. I can review what I actually said rather than what I thought I said, which helps identify verbal tics or unclear explanations.</p>
</section>
<section id="quick-feedback" class="level3">
<h3 class="anchored" data-anchor-id="quick-feedback">Quick Feedback</h3>
<p>It provides immediate feedback on structural issues—missing elements in my framework, calculations I didn’t account for, or assumptions I should have stated explicitly.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/claude-interview-project.jpg" class="img-fluid" alt="Claude Project Interface"> <em>My MBB Interview Assistant project showing case history and files</em></p>
</section>
</section>
<section id="major-limitations-ive-found" class="level2">
<h2 class="anchored" data-anchor-id="major-limitations-ive-found">Major Limitations I’ve Found</h2>
<section id="hallucinations-and-made-up-information" class="level3">
<h3 class="anchored" data-anchor-id="hallucinations-and-made-up-information">Hallucinations and Made-Up Information</h3>
<p>This is the biggest problem. Claude sometimes makes up data points or industry facts that sound plausible but aren’t real. In the media company revenue case, it cited specific market statistics that I later couldn’t verify anywhere.</p>
<p>For interview prep, this is dangerous because you might memorize false information and confidently state it in a real interview. I’ve learned to verify anything that sounds like a specific fact or statistic.</p>
</section>
<section id="going-off-topic" class="level3">
<h3 class="anchored" data-anchor-id="going-off-topic">Going Off Topic</h3>
<p>Sometimes Claude diverges from the specific case question to discuss tangentially related concepts. When working on the cable company alarm services case, it started explaining general IoT trends instead of focusing on the specific competitive positioning question I was asking about.</p>
</section>
<section id="scope-creep" class="level3">
<h3 class="anchored" data-anchor-id="scope-creep">Scope Creep</h3>
<p>The AI occasionally suggests frameworks or analyses that are way too broad for a 20-minute case interview. It might recommend conducting customer surveys or building complex financial models—things that wouldn’t be feasible in an actual interview setting.</p>
</section>
<section id="weird-or-broken-links" class="level3">
<h3 class="anchored" data-anchor-id="weird-or-broken-links">Weird or Broken Links</h3>
<p>When Claude tries to reference sources or suggest additional reading, the links are often fabricated or don’t lead anywhere useful. I’ve clicked on several suggested resources only to find they don’t exist.</p>
</section>
<section id="cant-simulate-real-pressure" class="level3">
<h3 class="anchored" data-anchor-id="cant-simulate-real-pressure">Can’t Simulate Real Pressure</h3>
<p>The fundamental limitation remains: practicing with an AI in my room doesn’t replicate the pressure of a real interviewer. There’s no one judging my confidence, no awkward silences to manage, no reading of facial expressions.</p>
<p><a href="https://www.mckinsey.com/careers/interviewing">Research on case interview preparation</a> emphasizes that the interpersonal dynamics and time pressure are critical elements that can only be practiced with real people.</p>
</section>
</section>
<section id="my-actual-approach" class="level2">
<h2 class="anchored" data-anchor-id="my-actual-approach">My Actual Approach</h2>
<section id="early-stage-framework-practice" class="level3">
<h3 class="anchored" data-anchor-id="early-stage-framework-practice">Early-Stage Framework Practice</h3>
<p>I use the Claude project at the beginning of my prep for each case type. It helps me understand what a good framework looks like and gives me immediate feedback on structural issues.</p>
</section>
<section id="verbal-practice-tool" class="level3">
<h3 class="anchored" data-anchor-id="verbal-practice-tool">Verbal Practice Tool</h3>
<p>The voice recording feature is useful for getting comfortable speaking through cases out loud. I can practice articulating my thinking without the pressure of another person listening.</p>
</section>
<section id="progress-tracking" class="level3">
<h3 class="anchored" data-anchor-id="progress-tracking">Progress Tracking</h3>
<p>The project acts as a log of all the cases I’ve practiced. I can see patterns in which types of cases I struggle with and where I need more work.</p>
</section>
<section id="always-verify-facts" class="level3">
<h3 class="anchored" data-anchor-id="always-verify-facts">Always Verify Facts</h3>
<p>I never trust specific data points or statistics from Claude without verification. If it mentions a market size or industry trend, I look it up independently.</p>
</section>
<section id="real-humans-are-essential" class="level3">
<h3 class="anchored" data-anchor-id="real-humans-are-essential">Real Humans Are Essential</h3>
<p>After using Claude for initial structure practice, I do the same cases with actual people—friends doing consulting recruiting, career services, or case partners. The AI gets me to baseline competence faster, but real practice is what actually prepares you for the interview dynamic.</p>
</section>
</section>
<section id="final-take" class="level2">
<h2 class="anchored" data-anchor-id="final-take">Final Take</h2>
<p>The MBB Interview Assistant project is a useful supplementary tool but comes with significant limitations. It’s good for: - Organizing and tracking case practice - Getting quick structural feedback - Practicing verbal articulation with voice recordings - Understanding framework basics</p>
<p>But it’s actively harmful if you: - Trust its specific facts without verification - Rely on it as your primary practice method - Follow its suggestions when they’re too broad or impractical - Think it can replace practicing with real people</p>
<p>The hallucination issue is serious enough that I treat everything Claude says as a suggestion to verify rather than information to memorize. For actual interview prep, the AI is a starting point, not a replacement for traditional practice methods.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>entrepreneurship</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/010.html</guid>
  <pubDate>Wed, 01 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/010_/interview-preparation.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Experimenting with Vibe Creation Using LLMs</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/009.html</link>
  <description><![CDATA[ 





<section id="playing-with-atmospheric-prompting" class="level2">
<h2 class="anchored" data-anchor-id="playing-with-atmospheric-prompting">Playing with Atmospheric Prompting</h2>
<p>I’ve been experimenting with what I’m calling “vibe creation” in Claude—essentially asking the LLM to generate or interpret atmospheric descriptions for different contexts. The goal was to see how well these models understand and reproduce subjective, mood-based content rather than factual information.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-prompting.jpg" class="img-fluid" alt="Vibe Prompting Interface"> <em>Testing different prompts to generate atmospheric descriptions</em></p>
</section>
<section id="the-prompts-i-tested" class="level2">
<h2 class="anchored" data-anchor-id="the-prompts-i-tested">The Prompts I Tested</h2>
<section id="starting-simple" class="level3">
<h3 class="anchored" data-anchor-id="starting-simple">Starting Simple</h3>
<p>My first prompt was straightforward:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of a rainy coffee shop at 3pm on a Tuesday”</p>
</blockquote>
<p>Claude generated a description that hit the expected notes—muted conversations, steam from coffee cups, the rhythmic sound of rain. It was accurate but generic, the kind of description you’d find in any piece of atmospheric writing.</p>
</section>
<section id="adding-specificity" class="level3">
<h3 class="anchored" data-anchor-id="adding-specificity">Adding Specificity</h3>
<p>I tried making the prompt more specific:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of a college library during finals week at 2am. Include sounds, lighting, and the feeling of collective stress”</p>
</blockquote>
<p>This produced better results. The model captured details like the fluorescent lighting, the sound of highlighters on paper, and the “quiet panic” of students. It understood the assignment but still felt somewhat detached—like someone describing a scene they’d read about rather than experienced.</p>
</section>
<section id="testing-negative-space" class="level3">
<h3 class="anchored" data-anchor-id="testing-negative-space">Testing Negative Space</h3>
<p>Here’s where it got interesting. I asked:</p>
<blockquote class="blockquote">
<p>“Describe the vibe of an empty office building at night. Focus on what’s NOT there rather than what is”</p>
</blockquote>
<p>Claude struggled more with this. It could describe absence—no people, no noise—but couldn’t quite capture the uncanny feeling of a space designed for activity sitting dormant. The descriptions remained surface-level. The output focused on the lack of meetings, chatter, talk, noise in the office. But it did not capture the liminal-space-like, vast, expansive (almost scary) vibe of an empty office in the way the show Severance, for instance, can.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-comparison.jpg" class="img-fluid" alt="Vibe Comparison Results"> <em>Comparing outputs from different prompting approaches</em></p>
</section>
</section>
<section id="what-worked-and-what-didnt" class="level2">
<h2 class="anchored" data-anchor-id="what-worked-and-what-didnt">What Worked and What Didn’t</h2>
<section id="strengths" class="level3">
<h3 class="anchored" data-anchor-id="strengths">Strengths</h3>
<p>The LLM is good at: - Assembling sensory details that conventionally fit a scene - Understanding cultural contexts (“finals week” immediately triggered appropriate stress markers) - Maintaining consistency within a single atmospheric description - Responding to structural prompts (“focus on sounds” vs “focus on lighting”)</p>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Where it falls short: - Generating truly novel atmospheric combinations - Capturing subtle emotional undertones that aren’t explicitly named - Moving beyond stereotypical associations (coffee shop = cozy, library = studious) - Understanding the relationship between physical space and psychological state in non-obvious ways</p>
<p>The model seems to work from a database of common associations rather than synthesizing new atmospheric understanding.</p>
</section>
</section>
<section id="why-this-matters-from-a-skeptical-angle" class="level2">
<h2 class="anchored" data-anchor-id="why-this-matters-from-a-skeptical-angle">Why This Matters (From a Skeptical Angle)</h2>
<section id="the-homogenization-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-homogenization-problem">The Homogenization Problem</h3>
<p>If everyone starts using LLMs to generate atmospheric descriptions for creative projects, writing, or marketing, we risk creating a feedback loop of averaged-out vibes. The model can only reproduce what it’s seen in training data, which means we get the most statistically likely description of any given mood or setting.</p>
<p>After all, LLMs tend to regress toward mean responses when generating creative content, potentially flattening the diversity of human expression.</p>
</section>
<section id="lost-nuance" class="level3">
<h3 class="anchored" data-anchor-id="lost-nuance">Lost Nuance</h3>
<p>Real atmospheric writing comes from personal observation and specific experience. When I asked Claude to describe my college library during finals, it gave me a generic college library experience. It couldn’t know about the specific smell of old heating systems mixed with energy drinks, or the way light reflects off that one weird pillar in the corner.</p>
</section>
<section id="useful-but-limited-tool" class="level3">
<h3 class="anchored" data-anchor-id="useful-but-limited-tool">Useful But Limited Tool</h3>
<p>This isn’t to say vibe creation with LLMs is useless. It’s good for: - Quick brainstorming when you’re stuck - Getting a baseline description to then personalize - Understanding how certain settings are conventionally described</p>
<p>But treating it as a replacement for actual observation and personal experience would result in flatter, less distinctive creative work.</p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Playing with vibe creation revealed both the capabilities and constraints of current LLMs. They’re sophisticated pattern-matching systems that can assemble convincing atmospheric descriptions from training data, but they lack the experiential knowledge that makes truly evocative writing memorable.</p>
<p>The exercise reinforced my skepticism about over-relying on these tools for creative work. They’re useful for scaffolding and inspiration, but the most interesting atmospheric details still need to come from human observation and experience.</p>
<p>For anyone trying similar experiments: push the model toward specificity and unconventional combinations. The more generic your prompt, the more generic your output.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>LLM</category>
  <category>creative</category>
  <category>claude</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/009.html</guid>
  <pubDate>Mon, 29 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/009_/vibe-prompting.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Photo Organization: Finding Pictures I Forgot I Had</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /008.html</link>
  <description><![CDATA[ 





<section id="searching-through-10000-photos" class="level2">
<h2 class="anchored" data-anchor-id="searching-through-10000-photos">Searching Through 10,000 Photos</h2>
<p>I realized I had over 10,000 photos in Google Photos and no way to find specific ones. Today I tried using the AI search features to locate pictures from last year’s vacation, and it actually worked better than expected.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-search.jpg" class="img-fluid" alt="AI Photo Search"> <em>Google Photos AI recognizing objects, people, and locations</em></p>
</section>
<section id="what-the-ai-can-find" class="level2">
<h2 class="anchored" data-anchor-id="what-the-ai-can-find">What the AI Can Find</h2>
<section id="object-recognition" class="level3">
<h3 class="anchored" data-anchor-id="object-recognition">Object Recognition</h3>
<p>I searched for “pizza” and it found every photo with pizza in it, even ones where pizza was just sitting on a table in the background. Same thing worked for “dog,” “beach,” and “birthday cake.” The AI recognizes way more objects than I expected.</p>
</section>
<section id="face-grouping" class="level3">
<h3 class="anchored" data-anchor-id="face-grouping">Face Grouping</h3>
<p>Google Photos automatically groups photos by faces, so I can find all pictures of specific people without having to manually tag them. It even recognizes people as they age or in different lighting.</p>
</section>
<section id="location-and-time" class="level3">
<h3 class="anchored" data-anchor-id="location-and-time">Location and Time</h3>
<p>If you have location data turned on, you can search by place names. I found all my photos from “San Francisco” or “coffee shop” pretty easily. Time-based searches work too - “photos from summer 2024” pulled up the right time period.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-albums.jpg" class="img-fluid" alt="Auto-Generated Albums"> <em>Automatically created albums based on events and trips</em></p>
</section>
</section>
<section id="automatic-albums" class="level2">
<h2 class="anchored" data-anchor-id="automatic-albums">Automatic Albums</h2>
<section id="trip-detection" class="level3">
<h3 class="anchored" data-anchor-id="trip-detection">Trip Detection</h3>
<p>The AI creates albums for trips automatically based on location and date patterns. It figured out my weekend in Portland and grouped all those photos together without me doing anything.</p>
</section>
<section id="event-recognition" class="level3">
<h3 class="anchored" data-anchor-id="event-recognition">Event Recognition</h3>
<p>It also makes albums for things like “Birthday party” or “Graduation” based on the types of photos and timing. Sometimes it gets it wrong - labeled a regular dinner as a “celebration” - but it’s right more often than not.</p>
</section>
<section id="people-albums" class="level3">
<h3 class="anchored" data-anchor-id="people-albums">People Albums</h3>
<p>Creates collections of photos featuring specific people, which is useful for family pictures or when you want to share photos of someone with them.</p>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="privacy-concerns" class="level3">
<h3 class="anchored" data-anchor-id="privacy-concerns">Privacy Concerns</h3>
<p>All this convenience requires uploading your photos to Google’s servers where they analyze everything. If that bothers you, you can’t really use these features.</p>
</section>
<section id="sometimes-too-smart" class="level3">
<h3 class="anchored" data-anchor-id="sometimes-too-smart">Sometimes Too Smart</h3>
<p>The AI occasionally creates albums for things that weren’t actually events, or groups random photos together based on superficial similarities.</p>
</section>
<section id="search-isnt-perfect" class="level3">
<h3 class="anchored" data-anchor-id="search-isnt-perfect">Search Isn’t Perfect</h3>
<p>Complex searches don’t always work. “Photos of John at the beach” might miss some or include wrong results. Simple, single-concept searches work much better.</p>
<p>It’s genuinely useful for finding old photos you’d never locate otherwise. The search functionality makes having thousands of photos actually manageable instead of just overwhelming.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>organization</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /008.html</guid>
  <pubDate>Sat, 27 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/008_ /ai-photo-search.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Email Management That Actually Works</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/007.html</link>
  <description><![CDATA[ 





<section id="letting-ai-handle-my-inbox" class="level2">
<h2 class="anchored" data-anchor-id="letting-ai-handle-my-inbox">Letting AI Handle My Inbox</h2>
<p>I set up SaneBox to manage my email and it’s been running for about a week now. Instead of manually sorting through everything, the AI decides what’s important and what can wait.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-inbox-sorting.jpg" class="img-fluid" alt="Smart Inbox Sorting"> <em>AI automatically categorizing emails by priority level</em></p>
</section>
<section id="how-it-sorts-things-out" class="level2">
<h2 class="anchored" data-anchor-id="how-it-sorts-things-out">How It Sorts Things Out</h2>
<section id="priority-filtering" class="level3">
<h3 class="anchored" data-anchor-id="priority-filtering">Priority Filtering</h3>
<p>SaneBox learns from which emails I actually open and respond to. Work emails from my team get flagged as important, while newsletters and promotional stuff gets moved to a separate folder. It’s not perfect but catches most of the obvious stuff.</p>
</section>
<section id="smart-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="smart-scheduling">Smart Scheduling</h3>
<p>Gmail’s AI also suggests response times and can schedule emails to send later. I’ve been using it to send emails during business hours even when I’m working late, so I don’t look like I’m always online.</p>
</section>
<section id="quick-replies" class="level3">
<h3 class="anchored" data-anchor-id="quick-replies">Quick Replies</h3>
<p>The suggested responses are actually useful for simple emails. When someone sends a meeting request, it’ll suggest “Looks good” or “Let me check my calendar” which saves typing the same responses over and over.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-email-responses.jpg" class="img-fluid" alt="Email Response Suggestions"> <em>AI-generated reply suggestions for common email types</em></p>
</section>
</section>
<section id="what-ive-noticed" class="level2">
<h2 class="anchored" data-anchor-id="what-ive-noticed">What I’ve Noticed</h2>
<section id="less-time-sorting" class="level3">
<h3 class="anchored" data-anchor-id="less-time-sorting">Less Time Sorting</h3>
<p>The main benefit is spending less time deciding what to read first. Important emails show up in the main inbox, everything else gets organized automatically.</p>
</section>
<section id="fewer-missed-messages" class="level3">
<h3 class="anchored" data-anchor-id="fewer-missed-messages">Fewer Missed Messages</h3>
<p>Before, urgent emails would get buried under promotional stuff. Now they’re separated, so I’m less likely to miss something important.</p>
</section>
<section id="still-need-to-check" class="level3">
<h3 class="anchored" data-anchor-id="still-need-to-check">Still Need to Check</h3>
<p>The AI sometimes gets it wrong - puts important emails in the low-priority folder or flags spam as urgent. You still need to glance through everything, but it’s more organized.</p>
<p>It’s a decent time-saver for people who get a lot of email. Not life-changing, but removes some of the daily friction of inbox management. The filtering works better than I expected, though you need to train it for a few days before it gets your priorities right.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>productivity</category>
  <category>organization</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/007.html</guid>
  <pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/007_/smart-inbox-sorting.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Building a Brand with AI: Cuzco Crunch</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/006.html</link>
  <description><![CDATA[ 





<section id="creating-a-brand-with-ai" class="level2">
<h2 class="anchored" data-anchor-id="creating-a-brand-with-ai">Creating a Brand with AI</h2>
<p>Eury and I decided to try building a snack brand called Cuzco Crunch, mostly to see how much of the process we could handle using AI tools. We are just experimenting with what’s possible when you use GPT for everything from naming to packaging concepts.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch.jpg" class="img-fluid" alt="Cuzco Crunch Logo"> <em>Initial logo concepts generated through AI prompting</em></p>
</section>
<section id="what-were-using-ai-for" class="level2">
<h2 class="anchored" data-anchor-id="what-were-using-ai-for">What We’re Using AI For</h2>
<section id="brand-development" class="level3">
<h3 class="anchored" data-anchor-id="brand-development">Brand Development</h3>
<p>The name “Cuzco Crunch” came from asking GPT to suggest names that sounded distinctive but not too weird. We wanted something that hinted at South American flavors without being too on-the-nose. It generated about 30 options and this one felt right.</p>
</section>
<section id="packaging-design-direction" class="level3">
<h3 class="anchored" data-anchor-id="packaging-design-direction">Packaging Design Direction</h3>
<p>We’re using AI to create mood boards and design concepts. Instead of hiring a designer upfront, we’re generating different packaging styles to see what resonates. The AI helps us think through color schemes, typography, and overall brand personality.</p>
</section>
<section id="marketing-copy" class="level3">
<h3 class="anchored" data-anchor-id="marketing-copy">Marketing Copy</h3>
<p>Product descriptions, social media posts, and even this blog post started with AI-generated drafts that we then edited. It’s faster than staring at a blank page, and sometimes the AI suggests angles we wouldn’t have thought of.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch-packaging.jpg" class="img-fluid" alt="Packaging Concepts"> <em>Various packaging design concepts and color schemes</em></p>
</section>
</section>
<section id="whats-working-and-what-isnt" class="level2">
<h2 class="anchored" data-anchor-id="whats-working-and-what-isnt">What’s Working and What Isn’t</h2>
<section id="the-good-stuff" class="level3">
<h3 class="anchored" data-anchor-id="the-good-stuff">The Good Stuff</h3>
<p>Speed is the biggest advantage. We can iterate on ideas quickly without waiting for external feedback or spending money on design consultations. The AI is also good at generating variations - give it one concept and it’ll produce 10 different takes on it.</p>
</section>
<section id="the-limitations" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations">The Limitations</h3>
<p>Everything needs human editing. The AI-generated copy often sounds too generic or tries too hard to be clever. Design concepts look decent but lack the subtle details that make professional packaging stand out on shelves.</p>
<p>Also, the AI doesn’t understand practical constraints. It’ll suggest elaborate packaging designs that would be expensive to produce or flavors that might not actually taste good together.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch-mockups.jpg" class="img-fluid" alt="Package Mockups"> <em>3D mockups and shelf visualization concepts</em></p>
</section>
</section>
<section id="the-reality-check" class="level2">
<h2 class="anchored" data-anchor-id="the-reality-check">The Reality Check</h2>
<section id="still-need-real-skills" class="level3">
<h3 class="anchored" data-anchor-id="still-need-real-skills">Still Need Real Skills</h3>
<p>AI handles the brainstorming and initial concepts, but you still need to understand branding, know your target market, and make business decisions. It’s a tool, not a replacement for actual knowledge about building a brand.</p>
</section>
<section id="cost-vs.-quality-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="cost-vs.-quality-trade-offs">Cost vs.&nbsp;Quality Trade-offs</h3>
<p>We’re probably saving money in the early stages, but there’s a point where you need professional designers and marketers. The question is finding that sweet spot where AI gets you far enough to make informed decisions about what’s worth investing in.</p>
</section>
<section id="learning-experience" class="level3">
<h3 class="anchored" data-anchor-id="learning-experience">Learning Experience</h3>
<p>The most valuable part has been understanding how much work goes into brand development. Using AI to handle the repetitive parts lets us focus on strategy and decision-making instead of getting stuck on execution details.</p>
<p>We’re treating this as an experiment rather than a serious business venture. If Cuzco Crunch turns into something real, great. If not, we’ve learned a lot about using AI for creative projects and brand development.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>branding</category>
  <category>entrepreneurship</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/006.html</guid>
  <pubDate>Tue, 23 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/006_Cuzco_Crunch/cuzco-crunch.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Fitness Coaching at Home</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/005.html</link>
  <description><![CDATA[ 





<section id="working-out-with-ai-guidance" class="level2">
<h2 class="anchored" data-anchor-id="working-out-with-ai-guidance">Working Out with AI Guidance</h2>
<p>I tried the Freeletics app today instead of going to the gym. It uses your phone’s camera to watch your form and give feedback in real time, which felt like having a trainer watching you.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/ai-form-analysis.jpg" class="img-fluid" alt="AI Form Analysis"> <em>Smartphone analyzing exercise form with real-time feedback</em></p>
</section>
<section id="how-it-works" class="level2">
<h2 class="anchored" data-anchor-id="how-it-works">How It Works</h2>
<section id="real-time-form-correction" class="level3">
<h3 class="anchored" data-anchor-id="real-time-form-correction">Real-Time Form Correction</h3>
<p>You prop your phone up and the camera tracks your movements. When I was doing squats, it noticed I wasn’t going low enough and told me to “squat deeper” right as I was doing the exercise. It also caught that I was leaning too far forward.</p>
</section>
<section id="counts-reps-automatically" class="level3">
<h3 class="anchored" data-anchor-id="counts-reps-automatically">Counts Reps Automatically</h3>
<p>The app counts your repetitions so you don’t have to keep track. It only counts reps with proper form, so if your push-up doesn’t go low enough, it won’t count it.</p>
</section>
<section id="adjusts-based-on-performance" class="level3">
<h3 class="anchored" data-anchor-id="adjusts-based-on-performance">Adjusts Based on Performance</h3>
<p>When I started struggling with burpees halfway through, the app suggested switching to a modified version. It seemed to pick up on the fact that my form was getting sloppy and offered an easier variation.</p>
</section>
</section>
<section id="what-works-well" class="level2">
<h2 class="anchored" data-anchor-id="what-works-well">What Works Well</h2>
<section id="form-feedback" class="level3">
<h3 class="anchored" data-anchor-id="form-feedback">Form Feedback</h3>
<p>The corrections are actually helpful. It’s like having someone watch you who knows what proper form looks like. Caught several things I didn’t realize I was doing wrong.</p>
</section>
<section id="no-equipment-needed" class="level3">
<h3 class="anchored" data-anchor-id="no-equipment-needed">No Equipment Needed</h3>
<p>All bodyweight exercises, so you can do it anywhere. The app designs workouts based on what space and time you have available.</p>
</section>
<section id="adapts-to-your-level" class="level3">
<h3 class="anchored" data-anchor-id="adapts-to-your-level">Adapts to Your Level</h3>
<p>Starts with easier exercises and gradually increases difficulty based on how you perform. If you’re struggling, it scales back. If you’re crushing it, it adds more challenging moves.</p>
</section>
<section id="tracks-progress-over-time" class="level3">
<h3 class="anchored" data-anchor-id="tracks-progress-over-time">Tracks Progress Over Time</h3>
<p>Keeps track of how many reps you can do and how your form improves. You can see whether you’re getting stronger or if certain exercises are still difficult.</p>
</section>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<section id="camera-angle-matters" class="level3">
<h3 class="anchored" data-anchor-id="camera-angle-matters">Camera Angle Matters</h3>
<p>You need to position your phone correctly for it to see your whole body. Took a few tries to get the angle right.</p>
</section>
<section id="lighting-requirements" class="level3">
<h3 class="anchored" data-anchor-id="lighting-requirements">Lighting Requirements</h3>
<p>Works better in good lighting. Had trouble tracking movements when the room was too dim.</p>
</section>
<section id="limited-exercise-types" class="level3">
<h3 class="anchored" data-anchor-id="limited-exercise-types">Limited Exercise Types</h3>
<p>Focuses mainly on bodyweight movements. If you want to lift weights or use equipment, you’ll need a different approach.</p>
</section>
<section id="not-always-perfect" class="level3">
<h3 class="anchored" data-anchor-id="not-always-perfect">Not Always Perfect</h3>
<p>Sometimes misses form issues or gives feedback that doesn’t quite match what you’re doing. Still better than working out with no guidance, but not as precise as a human trainer.</p>
<p>It’s a solid option for home workouts when you want some structure and feedback. The form correction feature is genuinely useful, especially for exercises you’re not familiar with.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>fitness</category>
  <category>health</category>
  <category>daily-life</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/005.html</guid>
  <pubDate>Fri, 19 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/005_AI_Fitness_Coaching_at_Home/ai-form-analysis.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI-Assisted Grocery Shopping and Meal Prep</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/004.html</link>
  <description><![CDATA[ 





<section id="smarter-grocery-shopping" class="level2">
<h2 class="anchored" data-anchor-id="smarter-grocery-shopping">Smarter Grocery Shopping</h2>
<p>I tried out the AnyList grocery app today and it’s different from just writing things down on paper. Instead of my usual scattered notes and forgotten items, the app organizes everything and suggests what I might need.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/smart-grocery-list.jpg" class="img-fluid" alt="Smart Grocery List"> <em>AI-generated grocery list organized by store layout</em></p>
</section>
<section id="what-it-actually-does" class="level2">
<h2 class="anchored" data-anchor-id="what-it-actually-does">What It Actually Does</h2>
<section id="tracks-what-you-need" class="level3">
<h3 class="anchored" data-anchor-id="tracks-what-you-need">Tracks What You Need</h3>
<p>The app learns from your purchase history and can predict when you’re running low on regular items. It suggested I needed milk and bread this week before I even thought about it. Not perfect, but surprisingly accurate for staples I buy regularly.</p>
</section>
<section id="organizes-by-store-layout" class="level3">
<h3 class="anchored" data-anchor-id="organizes-by-store-layout">Organizes by Store Layout</h3>
<p>Instead of jumping around the store, the list is organized by sections - produce, dairy, meat, etc. Some apps even know the specific layout of stores you frequent, so your list follows the actual aisles.</p>
</section>
<section id="meal-planning-integration" class="level3">
<h3 class="anchored" data-anchor-id="meal-planning-integration">Meal Planning Integration</h3>
<p>You can add meals for the week and it automatically generates ingredient lists. When I planned tacos for Tuesday, it added ground beef, tortillas, and lettuce to my list. It also checked what I already had at home based on recent purchases.</p>
</section>
</section>
<section id="practical-benefits" class="level2">
<h2 class="anchored" data-anchor-id="practical-benefits">Practical Benefits</h2>
<section id="less-time-in-store" class="level3">
<h3 class="anchored" data-anchor-id="less-time-in-store">Less Time in Store</h3>
<p>Following an organized list means less wandering around looking for items. I’ve noticed my shopping trips are about 15-20 minutes shorter.</p>
</section>
<section id="fewer-forgotten-items" class="level3">
<h3 class="anchored" data-anchor-id="fewer-forgotten-items">Fewer Forgotten Items</h3>
<p>The app remembers things I consistently buy but often forget to write down, like batteries or cleaning supplies.</p>
</section>
<section id="better-meal-planning" class="level3">
<h3 class="anchored" data-anchor-id="better-meal-planning">Better Meal Planning</h3>
<p>When you can see ingredient costs upfront, it’s easier to plan meals within budget. The app sometimes suggests cheaper alternatives for expensive items.</p>
</section>
<section id="automatic-coupons" class="level3">
<h3 class="anchored" data-anchor-id="automatic-coupons">Automatic Coupons</h3>
<p>Many apps integrate with store loyalty programs and apply relevant coupons automatically. I’ve saved money without having to hunt for deals.</p>
</section>
</section>
<section id="what-doesnt-work-perfectly" class="level2">
<h2 class="anchored" data-anchor-id="what-doesnt-work-perfectly">What Doesn’t Work Perfectly</h2>
<section id="learning-period" class="level3">
<h3 class="anchored" data-anchor-id="learning-period">Learning Period</h3>
<p>It takes a few weeks for the app to understand your shopping patterns. Early suggestions were often wrong.</p>
</section>
<section id="store-specific-features" class="level3">
<h3 class="anchored" data-anchor-id="store-specific-features">Store-Specific Features</h3>
<p>Advanced features like aisle mapping only work with certain grocery chains. Smaller or independent stores usually don’t have this integration.</p>
</section>
<section id="fresh-produce-timing" class="level3">
<h3 class="anchored" data-anchor-id="fresh-produce-timing">Fresh Produce Timing</h3>
<p>The AI isn’t great at predicting when you need fresh items since it depends on how quickly you use them.</p>
<p>Overall, it’s a useful tool that makes grocery shopping more organized. The meal planning aspect is particularly helpful for busy weeks when you need to think ahead. Not essential, but definitely convenient once you get used to it.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>daily-life</category>
  <category>productivity</category>
  <category>health</category>
  <category>daily-life</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/004.html</guid>
  <pubDate>Thu, 18 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/004_AI-Assisted_Grocery_Shopping_and_Meal_Prep/smart-fridge-inventory.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Research with Perplexity AI</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/003.html</link>
  <description><![CDATA[ 





<section id="using-ai-for-research" class="level2">
<h2 class="anchored" data-anchor-id="using-ai-for-research">Using AI for Research</h2>
<p>I’ve been trying out Perplexity AI for research tasks lately. It’s different from regular Google searches because it synthesizes information from multiple sources and gives you one coherent answer with citations.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-research.jpg" class="img-fluid" alt="Perplexity AI Interface"> <em>Perplexity AI interface showing search results with sources</em></p>
</section>
<section id="how-it-works" class="level2">
<h2 class="anchored" data-anchor-id="how-it-works">How It Works</h2>
<section id="getting-information-from-multiple-sources" class="level3">
<h3 class="anchored" data-anchor-id="getting-information-from-multiple-sources">Getting Information from Multiple Sources</h3>
<p>Instead of opening multiple tabs and reading through different articles, Perplexity pulls information from various sources and combines it into a single response. When I was researching market trends for a work project, I got a summary that included key points from several industry reports, all with proper citations.</p>
</section>
<section id="follow-up-questions" class="level3">
<h3 class="anchored" data-anchor-id="follow-up-questions">Follow-up Questions</h3>
<p>You can ask follow-up questions that build on your previous search. After asking about “sustainable packaging trends 2025,” I could continue with: - “What are the cost implications?” - “Which companies are leading this transition?” - “How does this compare to 2024 predictions?”</p>
<p>The AI remembers the context, so you don’t need to re-explain what you’re looking for.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-citations.jpg" class="img-fluid" alt="Perplexity Search Results"> <em>Live citations and source integration in action</em></p>
</section>
</section>
<section id="when-its-useful" class="level2">
<h2 class="anchored" data-anchor-id="when-its-useful">When It’s Useful</h2>
<section id="academic-work" class="level3">
<h3 class="anchored" data-anchor-id="academic-work">Academic Work</h3>
<p>Good for getting an overview of research topics and finding relevant studies. Helps identify what’s already been studied and where there might be gaps.</p>
</section>
<section id="business-research" class="level3">
<h3 class="anchored" data-anchor-id="business-research">Business Research</h3>
<p>Useful for market analysis and industry trends. You can ask specific questions and get answers that draw from recent reports and news sources.</p>
</section>
<section id="learning-new-topics" class="level3">
<h3 class="anchored" data-anchor-id="learning-new-topics">Learning New Topics</h3>
<p>When you’re unfamiliar with a subject, it can explain concepts and provide background without having to piece together information from multiple websites.</p>
</section>
<section id="fact-checking" class="level3">
<h3 class="anchored" data-anchor-id="fact-checking">Fact-Checking</h3>
<p>Since it shows sources for each piece of information, it’s easier to verify claims and see where data comes from.</p>
</section>
</section>
<section id="what-ive-found" class="level2">
<h2 class="anchored" data-anchor-id="what-ive-found">What I’ve Found</h2>
<section id="saves-time" class="level3">
<h3 class="anchored" data-anchor-id="saves-time">Saves Time</h3>
<p>The main benefit is speed. You get the key information faster since it’s already compiled from multiple sources. You still need to read the original sources for details, but the initial overview is much quicker.</p>
</section>
<section id="clear-sources" class="level3">
<h3 class="anchored" data-anchor-id="clear-sources">Clear Sources</h3>
<p>Every piece of information comes with citations, so you can see exactly where it came from and verify it if needed.</p>
</section>
<section id="handles-context-well" class="level3">
<h3 class="anchored" data-anchor-id="handles-context-well">Handles Context Well</h3>
<p>The conversation aspect works better than I expected. You can refine questions or explore related topics without starting over.</p>
</section>
<section id="shows-different-viewpoints" class="level3">
<h3 class="anchored" data-anchor-id="shows-different-viewpoints">Shows Different Viewpoints</h3>
<p>When sources disagree, it usually mentions the different perspectives rather than just presenting one view.</p>
<p>It’s a useful tool for the initial stages of research when you need to understand a topic quickly or gather information from multiple sources. Not a replacement for thorough research, but it does help speed up the information gathering process.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>research</category>
  <category>productivity</category>
  <category>technology</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/003.html</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/003_Research_with_Perplexity_AI/perplexity-research.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Do’s and Don’ts: Safe and Effective Usage</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/002.html</link>
  <description><![CDATA[ 





<section id="essential-ai-guidelines" class="level2">
<h2 class="anchored" data-anchor-id="essential-ai-guidelines">Essential AI Guidelines</h2>
<p>AI tools are powerful but require careful usage. These guidelines help you maximize benefits while avoiding common pitfalls.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-safety-guidelines.jpg" class="img-fluid" alt="AI Safety Guidelines"> <em>Key principles for responsible AI usage</em></p>
</section>
<section id="what-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-to-do">What TO Do</h2>
<section id="verify-important-information" class="level3">
<h3 class="anchored" data-anchor-id="verify-important-information">✅ Verify Important Information</h3>
<p>Always fact-check AI outputs for: - Medical or health advice - Financial decisions - Legal matters - Technical specifications - Current events or recent data</p>
<p>Cross-reference with authoritative sources before acting on AI recommendations.</p>
</section>
<section id="be-specific-in-your-prompts" class="level3">
<h3 class="anchored" data-anchor-id="be-specific-in-your-prompts">✅ Be Specific in Your Prompts</h3>
<p><strong>Instead of:</strong> “Help me with my presentation” <strong>Use:</strong> “Create an outline for a 10-minute sales presentation to executives about Q3 revenue growth”</p>
<p>Specific prompts produce more useful results.</p>
</section>
<section id="use-ai-for-brainstorming-and-drafts" class="level3">
<h3 class="anchored" data-anchor-id="use-ai-for-brainstorming-and-drafts">✅ Use AI for Brainstorming and Drafts</h3>
<p>AI excels at: - Generating initial ideas - Creating first drafts - Organizing thoughts - Providing alternative perspectives</p>
<p>Use AI output as a starting point, not a final product.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-workflow-diagram.jpg" class="img-fluid" alt="AI Workflow Best Practices"> <em>Proper workflow: AI assists, human validates and refines</em></p>
</section>
<section id="maintain-human-judgment" class="level3">
<h3 class="anchored" data-anchor-id="maintain-human-judgment">✅ Maintain Human Judgment</h3>
<p>Keep human oversight for: - Final decision-making - Quality assessment - Ethical considerations - Context that AI might miss</p>
</section>
<section id="ask-follow-up-questions" class="level3">
<h3 class="anchored" data-anchor-id="ask-follow-up-questions">✅ Ask Follow-Up Questions</h3>
<p>Improve results by asking: - “Can you explain this differently?” - “What are potential problems with this approach?” - “Give me three alternatives” - “Make this more specific”</p>
</section>
</section>
<section id="what-not-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-not-to-do">What NOT to Do</h2>
<section id="dont-share-sensitive-information" class="level3">
<h3 class="anchored" data-anchor-id="dont-share-sensitive-information">❌ Don’t Share Sensitive Information</h3>
<p>Never input: - Passwords or login credentials - Social Security numbers - Credit card details - Proprietary business information - Personal addresses or phone numbers - Private family details</p>
<p>Assume all AI conversations could be stored or accessed by others.</p>
</section>
<section id="dont-rely-on-ai-for-critical-decisions" class="level3">
<h3 class="anchored" data-anchor-id="dont-rely-on-ai-for-critical-decisions">❌ Don’t Rely on AI for Critical Decisions</h3>
<p>Avoid using AI alone for: - Medical diagnoses or treatment - Legal advice or document preparation - Financial investment decisions - Safety-critical calculations - Emergency situations</p>
<p>Consult qualified professionals for these matters.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-security-risks.jpg" class="img-fluid" alt="AI Limitations Warning"> <em>Areas where AI should not be the primary decision maker</em></p>
</section>
<section id="dont-assume-ai-is-always-accurate" class="level3">
<h3 class="anchored" data-anchor-id="dont-assume-ai-is-always-accurate">❌ Don’t Assume AI is Always Accurate</h3>
<p>AI can produce: - Outdated information - Factual errors - Biased responses - Plausible-sounding but incorrect details</p>
<p>Especially problematic for recent events or specialized technical information.</p>
</section>
<section id="dont-use-ai-for-harmful-purposes" class="level3">
<h3 class="anchored" data-anchor-id="dont-use-ai-for-harmful-purposes">❌ Don’t Use AI for Harmful Purposes</h3>
<p>Avoid requesting: - Misleading or false content - Harassment or threatening messages - Plagiarism or academic dishonesty - Illegal activity guidance - Discriminatory content</p>
</section>
<section id="dont-ignore-context-limitations" class="level3">
<h3 class="anchored" data-anchor-id="dont-ignore-context-limitations">❌ Don’t Ignore Context Limitations</h3>
<p>AI doesn’t understand: - Your full personal situation - Unspoken cultural context - Real-time environmental factors - Emotional nuances</p>
<p>Provide necessary context explicitly in your prompts.</p>
</section>
</section>
<section id="implementation-strategy" class="level2">
<h2 class="anchored" data-anchor-id="implementation-strategy">Implementation Strategy</h2>
<section id="start-small" class="level3">
<h3 class="anchored" data-anchor-id="start-small">Start Small</h3>
<p>Begin with low-stakes tasks like email drafting or meal planning before using AI for important projects.</p>
</section>
<section id="build-verification-habits" class="level3">
<h3 class="anchored" data-anchor-id="build-verification-habits">Build Verification Habits</h3>
<p>Develop a routine of checking AI outputs against reliable sources.</p>
</section>
<section id="keep-learning" class="level3">
<h3 class="anchored" data-anchor-id="keep-learning">Keep Learning</h3>
<p>AI capabilities and limitations evolve. Stay informed about updates and new research.</p>
</section>
<section id="document-what-works" class="level3">
<h3 class="anchored" data-anchor-id="document-what-works">Document What Works</h3>
<p>Save effective prompts and note which applications produce reliable results for your needs.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-usage-tips.jpg" class="img-fluid" alt="AI Usage Tips"> <em>Use these tips and tricks!</em></p>
<p>Following these guidelines ensures you gain AI’s benefits while avoiding common mistakes that can lead to poor decisions or security issues.</p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/002.html</guid>
  <pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/002_AI_Dos_And_Donts/ai-safety-guidelines.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>5 Ways to Use AI Every Day</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/001.html</link>
  <description><![CDATA[ 





<section id="using-ai-for-daily-tasks" class="level2">
<h2 class="anchored" data-anchor-id="using-ai-for-daily-tasks">Using AI for Daily Tasks</h2>
<p>AI tools like ChatGPT and Claude can handle routine tasks that usually take significant time. Here are five practical applications that work reliably.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-daily-tasks.jpg" class="img-fluid" alt="AI Daily Tasks"> <em>Common tasks where AI provides immediate value</em></p>
</section>
<section id="meal-planning-and-recipes" class="level2">
<h2 class="anchored" data-anchor-id="meal-planning-and-recipes">1. Meal Planning and Recipes</h2>
<p>Turn ingredients into meal plans instantly.</p>
<p><strong>Basic ingredient prompt:</strong></p>
<pre><code>I have chicken breast, rice, and broccoli. 
Give me 3 different 30-minute meals with instructions.</code></pre>
<p><strong>Weekly planning prompt:</strong></p>
<pre><code>Create a 5-day meal plan for 2 people, $60 budget. 
Include grocery list organized by store section.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-meal-planning.jpg" class="img-fluid" alt="AI Meal Planning"> <em>AI generates recipes based on available ingredients and dietary preferences</em></p>
<p>AI handles dietary restrictions, cooking time constraints, and budget limitations effectively. The grocery lists are particularly useful since they organize items by store layout.</p>
</section>
<section id="email-and-message-writing" class="level2">
<h2 class="anchored" data-anchor-id="email-and-message-writing">2. Email and Message Writing</h2>
<p>Improve clarity and tone in professional communication.</p>
<p><strong>Email revision prompt:</strong></p>
<pre><code>Make this email more professional and concise:
[paste your draft]</code></pre>
<p><strong>Difficult conversation prompt:</strong></p>
<pre><code>Help me write a polite but firm email declining this request:
[explain situation]</code></pre>
<p><strong>Meeting follow-up prompt:</strong></p>
<pre><code>Draft a follow-up email summarizing these meeting points:
[list key decisions and action items]</code></pre>
<p>This works for text messages, LinkedIn messages, and any written communication where tone matters.</p>
</section>
<section id="quick-learning-and-research" class="level2">
<h2 class="anchored" data-anchor-id="quick-learning-and-research">3. Quick Learning and Research</h2>
<p>Get structured explanations on unfamiliar topics.</p>
<p><strong>Concept explanation prompt:</strong></p>
<pre><code>Explain [topic] in simple terms, then give me 
5 follow-up questions to test my understanding.</code></pre>
<p><strong>Skill learning prompt:</strong></p>
<pre><code>I need to learn Excel pivot tables for work. 
Give me a step-by-step tutorial focusing on the most practical features.</code></pre>
<p><strong>Research synthesis prompt:</strong></p>
<pre><code>Compare the top 3 options for [product/service] 
with pros and cons for each.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-learning-interface.jpg" class="img-fluid" alt="AI Learning Assistant"> <em>AI breaks down complex topics into manageable learning steps</em></p>
<p>AI excels at creating structured learning paths and synthesizing information from multiple angles.</p>
</section>
<section id="travel-and-event-planning" class="level2">
<h2 class="anchored" data-anchor-id="travel-and-event-planning">4. Travel and Event Planning</h2>
<p>Generate detailed itineraries and logistics.</p>
<p><strong>Trip planning prompt:</strong></p>
<pre><code>Plan a 3-day weekend trip to [destination] for [number] people 
with a $[amount] budget. Include transportation, accommodation, 
and daily activities.</code></pre>
<p><strong>Local exploration prompt:</strong></p>
<pre><code>What are 5 lesser-known attractions in [city] 
that locals recommend?</code></pre>
<p><strong>Event planning prompt:</strong></p>
<pre><code>Create a timeline and checklist for planning a 
[birthday party/dinner party/work event] for [number] people.</code></pre>
<p>AI handles multiple constraints simultaneously - budget, time, preferences, and logistics.</p>
</section>
<section id="home-management-and-finances" class="level2">
<h2 class="anchored" data-anchor-id="home-management-and-finances">5. Home Management and Finances</h2>
<p>Get practical advice for household and money management.</p>
<p><strong>Home maintenance prompt:</strong></p>
<pre><code>My [appliance/fixture] is [problem description]. 
Walk me through troubleshooting steps from simple to complex.</code></pre>
<p><strong>Budget planning prompt:</strong></p>
<pre><code>I make $[amount] monthly, fixed expenses are $[amount]. 
Create a realistic budget with savings and discretionary spending.</code></pre>
<p><strong>Purchase decision prompt:</strong></p>
<pre><code>I'm considering buying [item] for [purpose]. 
What factors should I evaluate? Create a decision framework.</code></pre>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-budget-planning.jpg" class="img-fluid" alt="AI Home Finance"> <em>AI provides structured approaches to financial and household decisions</em></p>
<p>These applications work because AI can process multiple variables and provide step-by-step guidance for complex decisions.</p>
</section>
<section id="implementation-notes" class="level2">
<h2 class="anchored" data-anchor-id="implementation-notes">Implementation Notes</h2>
<p><strong>Prompt specificity matters.</strong> Include relevant details like budget, timeframe, number of people, and specific constraints.</p>
<p><strong>Follow-up questions improve results.</strong> Ask for alternatives, simplifications, or additional detail as needed.</p>
<p><strong>Save effective prompts.</strong> Keep a note file with prompts that work well for repeated use.</p>
<p>These five applications cover the most common daily planning and decision-making tasks. Each provides immediate time savings and often produces better results than manual research.</p>


</section>

 ]]></description>
  <category>AI</category>
  <category>productivity</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/001.html</guid>
  <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/001_AI_Daily_Guide/ai-daily-tasks.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How AI Systems Work: A Technical Overview</title>
  <dc:creator>Raymond Liu Ao</dc:creator>
  <link>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/000.html</link>
  <description><![CDATA[ 





<section id="understanding-modern-ai-systems" class="level2">
<h2 class="anchored" data-anchor-id="understanding-modern-ai-systems">Understanding Modern AI Systems</h2>
<p>Artificial intelligence systems like ChatGPT and Claude can engage in conversations, solve problems, and generate content. However, their underlying mechanisms are fundamentally different from human cognition.</p>
<p>These systems operate as sophisticated pattern recognition and prediction engines. While they can produce human-like responses, they function by identifying statistical patterns in data rather than developing conceptual understanding.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" class="img-fluid" alt="Neural Network vs Brain"> <em>AI neural networks are inspired by brain structure but operate through different mechanisms</em></p>
<blockquote class="blockquote">
<p><strong>Key Question:</strong> How do statistical pattern-matching systems produce seemingly intelligent behavior?</p>
</blockquote>
</section>
<section id="core-architecture-prediction-systems" class="level2">
<h2 class="anchored" data-anchor-id="core-architecture-prediction-systems">Core Architecture: Prediction Systems</h2>
<p>Modern AI systems solve a fundamental prediction problem: given an input, determine the most statistically likely output. This applies whether predicting the next word in text, pixels in an image, or moves in a game.</p>
<section id="transformer-architecture" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h3>
<p>Current large language models like GPT-4 and Claude use transformer architecture:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified transformer process</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> transformer_prediction(input_text):</span>
<span id="cb1-3">    tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize(input_text)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert text to processable units</span></span>
<span id="cb1-4">    embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_to_vectors(tokens)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Map to numerical representations</span></span>
<span id="cb1-5">    </span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> neural_layers:</span>
<span id="cb1-7">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention_mechanism(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identify relationships</span></span>
<span id="cb1-8">        embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Process patterns</span></span>
<span id="cb1-9">    </span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predict_next_token(embeddings)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Generate most likely continuation</span></span></code></pre></div></div>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/transformer-architecture.jpg" class="img-fluid" alt="Transformer Architecture"> <em>Transformer architecture with attention and processing layers</em></p>
<p>Modern systems operate at significant scale. GPT-3 contains 175 billion parameters trained on 45TB of text data. GPT-4 is estimated to exceed 1 trillion parameters with correspondingly higher computational requirements.</p>
</section>
<section id="attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanisms">Attention Mechanisms</h3>
<p>The attention mechanism enables models to focus selectively on different input elements when making predictions.</p>
<p><strong>Example:</strong> “The cat sat on the mat because it was comfortable.”</p>
<p>When determining what “it” refers to, the attention mechanism assigns weights: - “cat” (high attention - 0.7) - “mat” (medium attention - 0.2) - “sat” (low attention - 0.1)</p>
<p>This mechanism allows processing of relationships between distant elements in sequences.</p>
</section>
</section>
<section id="training-process" class="level2">
<h2 class="anchored" data-anchor-id="training-process">Training Process</h2>
<p>AI system development occurs through structured training phases that determine capabilities and limitations.</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-training-process.jpg" class="img-fluid" alt="AI Training Process"> <em>Complete pipeline from data collection to deployed system</em></p>
<section id="phase-1-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-pre-training">Phase 1: Pre-training</h3>
<p>Systems learn from large text datasets through next-token prediction:</p>
<ol type="1">
<li><strong>Data Collection</strong>: Process text from websites, books, and articles</li>
<li><strong>Tokenization</strong>: Convert text to discrete units for processing</li>
<li><strong>Prediction Training</strong>: Train models to predict missing tokens in sequences</li>
<li><strong>Parameter Adjustment</strong>: Iteratively modify billions of parameters based on prediction accuracy</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified training loop</span></span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(many_epochs):</span>
<span id="cb2-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> text_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> massive_dataset:</span>
<span id="cb2-4">        input_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Input sequence</span></span>
<span id="cb2-5">        target_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_chunk[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Target prediction</span></span>
<span id="cb2-6">        </span>
<span id="cb2-7">        prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(input_tokens)</span>
<span id="cb2-8">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calculate_error(prediction, target_token)</span>
<span id="cb2-9">        </span>
<span id="cb2-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Update model parameters</span></span>
<span id="cb2-11">        model.update_weights(loss.gradient())</span></code></pre></div></div>
</section>
<section id="phase-2-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-fine-tuning">Phase 2: Fine-tuning</h3>
<p>Pre-trained models are refined for specific applications:</p>
<ul>
<li><strong>Supervised Fine-tuning</strong>: Training on curated question-answer pairs</li>
<li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: Using human preferences to shape responses</li>
<li><strong>Safety Training</strong>: Implementing guidelines to avoid harmful outputs</li>
</ul>
<p>This process transforms basic text predictors into conversational assistants.</p>
</section>
</section>
<section id="emergent-capabilities" class="level2">
<h2 class="anchored" data-anchor-id="emergent-capabilities">Emergent Capabilities</h2>
<p>Large-scale training produces capabilities that weren’t explicitly programmed. These emergent abilities arise from complex interactions between simple components.</p>
<section id="examples-of-emergence" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-emergence">Examples of Emergence</h3>
<p>As model size increases, new capabilities develop:</p>
<p><strong>Chain-of-Thought Reasoning</strong>: Models develop step-by-step problem-solving approaches without explicit training on this strategy.</p>
<p><strong>In-Context Learning</strong>: Systems can perform new tasks based on examples in prompts without additional training.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example of in-context learning</span></span>
<span id="cb3-2">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Translate English to French:</span></span>
<span id="cb3-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Hello → Bonjour</span></span>
<span id="cb3-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Goodbye → Au revoir</span></span>
<span id="cb3-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Thank you → Merci</span></span>
<span id="cb3-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Good morning → </span></span>
<span id="cb3-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Model produces "Bonjour" despite no explicit</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># training on this translation task</span></span></code></pre></div></div>
<p><strong>Cross-Lingual Transfer</strong>: Models trained primarily on English can operate effectively in other languages.</p>
<p>The mechanisms underlying emergence remain an active research area, with theories including phase transitions, increased representational capacity, and pattern recognition thresholds.</p>
</section>
</section>
<section id="system-limitations" class="level2">
<h2 class="anchored" data-anchor-id="system-limitations">System Limitations</h2>
<p>Current AI systems have several fundamental constraints that stem from their statistical architecture:</p>
<p><img src="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/ai-limitations-chart.jpg" class="img-fluid" alt="AI Limitations Chart"> <em>Overview of AI system capabilities and constraints</em></p>
<section id="pattern-matching-vs.-understanding" class="level3">
<h3 class="anchored" data-anchor-id="pattern-matching-vs.-understanding">Pattern Matching vs.&nbsp;Understanding</h3>
<p>AI systems process statistical relationships in symbols rather than developing conceptual understanding. They can accurately reproduce information without grasping underlying mechanisms.</p>
<p><strong>Example</strong>: A system can state that “water boils at 100°C” without understanding molecular behavior, phase transitions, or the physical processes involved.</p>
</section>
<section id="output-generation-under-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="output-generation-under-uncertainty">Output Generation Under Uncertainty</h3>
<p>Systems are designed to produce responses even when lacking relevant information, leading to plausible but potentially inaccurate outputs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simplified response generation</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_response(query):</span>
<span id="cb4-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> confidence_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> threshold:</span>
<span id="cb4-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> retrieve_known_information(query)</span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-6">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># System still generates output</span></span>
<span id="cb4-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> generate_plausible_response(query)</span></code></pre></div></div>
</section>
<section id="training-data-constraints" class="level3">
<h3 class="anchored" data-anchor-id="training-data-constraints">Training Data Constraints</h3>
<p>Models are limited by their training data cutoff and cannot incorporate new information without retraining.</p>
</section>
<section id="memory-limitations" class="level3">
<h3 class="anchored" data-anchor-id="memory-limitations">Memory Limitations</h3>
<p>Current systems can process limited context windows (typically 4,000-128,000 tokens) with no persistent memory between conversations.</p>
</section>
<section id="correlation-vs.-causation" class="level3">
<h3 class="anchored" data-anchor-id="correlation-vs.-causation">Correlation vs.&nbsp;Causation</h3>
<p>Systems excel at identifying statistical correlations but have limited understanding of causal relationships. They may recognize that umbrella sales correlate with rain without understanding the causal mechanism.</p>
</section>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical Implications</h2>
<p>Understanding AI architecture has important implications for effective use:</p>
<section id="system-strengths" class="level3">
<h3 class="anchored" data-anchor-id="system-strengths">System Strengths</h3>
<ul>
<li>Pattern recognition and text generation capabilities</li>
<li>Rapid processing of large information sets</li>
<li>Accessible interface for complex tasks</li>
<li>Consistent performance within training domains</li>
</ul>
</section>
<section id="appropriate-applications" class="level3">
<h3 class="anchored" data-anchor-id="appropriate-applications">Appropriate Applications</h3>
<ul>
<li>Content drafting and editing</li>
<li>Information synthesis from known sources</li>
<li>Code generation and debugging assistance</li>
<li>Language translation</li>
<li>Creative brainstorming</li>
</ul>
</section>
<section id="best-practices" class="level3">
<h3 class="anchored" data-anchor-id="best-practices">Best Practices</h3>
<ol type="1">
<li><strong>Verify Important Information</strong>: Fact-check outputs for critical applications</li>
<li><strong>Understand Limitations</strong>: Recognize domain constraints and knowledge cutoffs</li>
<li><strong>Maintain Human Oversight</strong>: Keep humans involved in important decisions</li>
<li><strong>Recognize Bias</strong>: Be aware that systems reflect training data biases</li>
<li><strong>Use as Tools</strong>: Apply systems as analytical instruments rather than authoritative sources</li>
</ol>
</section>
</section>
<section id="future-development" class="level2">
<h2 class="anchored" data-anchor-id="future-development">Future Development</h2>
<p>Current AI systems represent early implementations of statistical learning approaches. Several research directions may expand capabilities:</p>
<section id="near-term-developments-1-3-years" class="level3">
<h3 class="anchored" data-anchor-id="near-term-developments-1-3-years">Near-Term Developments (1-3 years)</h3>
<ul>
<li>Multimodal systems integrating text, images, audio, and video</li>
<li>Extended context windows for longer conversations and documents</li>
<li>Improved reasoning and problem-solving capabilities</li>
<li>Specialized models optimized for specific domains</li>
</ul>
</section>
<section id="long-term-research-areas-10-years" class="level3">
<h3 class="anchored" data-anchor-id="long-term-research-areas-10-years">Long-Term Research Areas (10+ years)</h3>
<ul>
<li>Artificial General Intelligence research</li>
<li>Questions about machine consciousness and subjective experience</li>
<li>AI alignment and safety considerations</li>
</ul>
</section>
<section id="active-research-questions" class="level3">
<h3 class="anchored" data-anchor-id="active-research-questions">Active Research Questions</h3>
<p>Researchers are investigating fundamental questions:</p>
<ul>
<li><strong>Scaling Laws</strong>: Relationships between model size, data, and capabilities</li>
<li><strong>Emergence Mechanisms</strong>: Why new abilities appear at certain scales</li>
<li><strong>Alignment</strong>: Ensuring AI systems operate according to intended objectives</li>
<li><strong>Interpretability</strong>: Understanding internal model representations and processes</li>
</ul>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Modern AI systems represent sophisticated statistical engines that process patterns in data to generate human-like outputs. While they demonstrate impressive capabilities, they operate through pattern matching rather than conceptual understanding.</p>
<section id="key-technical-points" class="level3">
<h3 class="anchored" data-anchor-id="key-technical-points">Key Technical Points</h3>
<p>Current AI systems achieve intelligent-seeming behavior through statistical pattern processing rather than understanding. This approach has several implications:</p>
<ul>
<li><strong>For Users</strong>: Understanding system strengths and limitations enables more effective application</li>
<li><strong>For Organizations</strong>: Appropriate use requires human oversight and verification processes</li>
<li><strong>For Society</strong>: New frameworks are needed for evaluating machine capabilities and limitations</li>
</ul>
<p>Statistical pattern matching at scale produces sophisticated behavior, suggesting that aspects of intelligence may be more computational than previously understood.</p>
<p>As these systems become more prevalent, understanding their statistical nature becomes increasingly important for effective implementation. The technology continues evolving rapidly, with ongoing research addressing questions about scaling, emergence, and alignment with human objectives.</p>
<p>The field represents significant progress in automated pattern recognition and generation, with continued development likely to expand capabilities while maintaining the fundamental statistical architecture.</p>
<hr>
<p><strong>Understanding the technical foundations of AI systems helps inform appropriate use and realistic expectations about current capabilities.</strong></p>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>technology</category>
  <guid>https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/000.html</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
  <media:content url="https://liuaoraymond.github.io/comm4190_F25_Using_LLMs_Blog/posts/000_How_Does_AI_Really_Work/neural-network-brain.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
